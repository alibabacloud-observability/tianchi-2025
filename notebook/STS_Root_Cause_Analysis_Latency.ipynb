{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPL 诊断问题根因-慢\n",
    "\n",
    "该notebook实现了《如何用SPL快速诊断问题根因 -- 慢.md》中描述的逐步流程，用于通过SPL查询诊断端到端高延迟问题。\n",
    "\n",
    "## 分析流程\n",
    "\n",
    "1. 查找高独占时间的span - 使用 trace_exclusive_duration 运算符识别独占持续时间高的span\n",
    "2. 模式分析 - 使用 diff_patterns 发现高延迟span的特征\n",
    "3. CPU指标分析 - 通过CMS实体查询，查询指定service的CPU使用率指标\n",
    "4. 异常检测 - 使用 series_decompose_anomalies 检测CPU使用率异常\n",
    "\n",
    "目标是找出能够解释延迟上升的root cause （例如 recommendation.cpu）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FindRootCauseSpansRT imported successfully\n",
      "✅ TestCMSQuery imported successfully\n",
      "✅ Constants imported successfully\n",
      "✅ All available imports loaded successfully\n",
      "\n",
      "💡 If you see warnings above, please run: pip install -r requirements.txt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 将父目录添加到路径以便导入模块\n",
    "sys.path.append('..')\n",
    "\n",
    "# 导入自定义模块并进行异常处理\n",
    "try:\n",
    "    from find_root_cause_spans_rt import FindRootCauseSpansRT\n",
    "    print(\"✅ FindRootCauseSpansRT imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Warning: Could not import FindRootCauseSpansRT: {e}\")\n",
    "    FindRootCauseSpansRT = None\n",
    "\n",
    "try:\n",
    "    from test_cms_query import TestCMSQuery\n",
    "    print(\"✅ TestCMSQuery imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Warning: Could not import TestCMSQuery: {e}\")\n",
    "    print(\"💡 Please install required dependencies: pip install -r requirements.txt\")\n",
    "    TestCMSQuery = None\n",
    "\n",
    "try:\n",
    "    from utils.constants import HIGH_RT_TRACES, TRACES_FOR_AVG_RT, PERCENT_95\n",
    "    print(\"✅ Constants imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Warning: Could not import constants: {e}\")\n",
    "    # 如果常量无法导入，则设置默认值\n",
    "    HIGH_RT_TRACES = 1000\n",
    "    TRACES_FOR_AVG_RT = 1000\n",
    "    PERCENT_95 = 0.95\n",
    "    print(\"✅ Using default constant values\")\n",
    "\n",
    "# 加载环境变量\n",
    "\n",
    "print(\"✅ All available imports loaded successfully\")\n",
    "print(\"\\n💡 If you see warnings above, please run: pip install -r requirements.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置环境变量\n",
    "\n",
    "设置访问的project和logstore,设置题目中的故障时间段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Analysis Configuration:\n",
      "  SLS Project: proj-xtrace-a46b97cfdc1332238f714864c014a1b-cn-qingdao\n",
      "  Logstore: logstore-tracing\n",
      "  Anomaly Period: 2025-08-28 16:14:30 to 2025-08-28 16:19:30\n",
      "  Normal Period: 2025-08-28 16:04:30 to 2025-08-28 16:14:30\n",
      "  Duration Threshold: 2.0s\n",
      "  CMS Workspace: quanxi-tianchi-test\n"
     ]
    }
   ],
   "source": [
    "# SLS 配置\n",
    "PROJECT_NAME = \"proj-xtrace-a46b97cfdc1332238f714864c014a1b-cn-qingdao\"\n",
    "LOGSTORE_NAME = \"logstore-tracing\"\n",
    "REGION = \"cn-qingdao\"\n",
    "\n",
    "# 分析时间区间\n",
    "# 异常时间段（延迟较高时段）\n",
    "ANOMALY_START_TIME = \"2025-08-28 16:14:30\"\n",
    "ANOMALY_END_TIME = \"2025-08-28 16:19:30\"\n",
    "\n",
    "# 正常时间段（用于对比的基线）\n",
    "NORMAL_START_TIME = \"2025-08-28 16:04:30\"\n",
    "NORMAL_END_TIME = \"2025-08-28 16:14:30\"\n",
    "\n",
    "# 分析参数\n",
    "DURATION_THRESHOLD = 2000000000  # 2000ms（以纳秒为单位）\n",
    "LIMIT_NUM = 1000  # 分析的 trace 数量\n",
    "\n",
    "# CMS 指标配置\n",
    "CMS_WORKSPACE = \"quanxi-tianchi-test\"\n",
    "CMS_ENDPOINT = 'cms.cn-qingdao.aliyuncs.com'\n",
    "\n",
    "print(f\"📊 Analysis Configuration:\")\n",
    "print(f\"  SLS Project: {PROJECT_NAME}\")\n",
    "print(f\"  Logstore: {LOGSTORE_NAME}\")\n",
    "print(f\"  Anomaly Period: {ANOMALY_START_TIME} to {ANOMALY_END_TIME}\")\n",
    "print(f\"  Normal Period: {NORMAL_START_TIME} to {NORMAL_END_TIME}\")\n",
    "print(f\"  Duration Threshold: {DURATION_THRESHOLD/1000000000:.1f}s\")\n",
    "print(f\"  CMS Workspace: {CMS_WORKSPACE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STS创建客户端\n",
    "\n",
    "设置个人账户信息，获取访问权限，STS新建客户端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SLS访问凭证配置正确\n",
      "✅ 成功获取访问权限！\n",
      "✅ SLS客户端已使用临时凭证创建。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from aliyun.log import LogClient \n",
    "from alibabacloud_sts20150401.client import Client as StsClient\n",
    "from alibabacloud_sts20150401 import models as sts_models\n",
    "from alibabacloud_tea_openapi import models as open_api_models\n",
    "from Tea.exceptions import TeaException\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "\n",
    "# ----------请创建一个环境变量文件,并将下面2个参数设置为你之前保存的信息\n",
    "# ----------请在环境变量文件中设置你创建用户时保存的AK, 样例如下:\n",
    "'''\n",
    "ALIBABA_CLOUD_ACCESS_KEY_ID=\"你保存的AccessKey ID\"\n",
    "ALIBABA_CLOUD_ACCESS_KEY_SECRET\"你保存的AccessKey Secret\"\n",
    "'''\n",
    "MAIN_ACCOUNT_ACCESS_KEY_ID = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_ID')\n",
    "MAIN_ACCOUNT_ACCESS_KEY_SECRET = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_SECRET')\n",
    "ALIBABA_CLOUD_ROLE_ARN = os.getenv('ALIBABA_CLOUD_ROLE_ARN','acs:ram::1672753017899339:role/tianchi-user-a')\n",
    "STS_SESSION_NAME = os.getenv('ALIBABA_CLOUD_ROLE_SESSION_NAME', 'my-sls-access') # 自定义会话名称，没有固定要求\n",
    "\n",
    "if MAIN_ACCOUNT_ACCESS_KEY_ID and MAIN_ACCOUNT_ACCESS_KEY_SECRET and ALIBABA_CLOUD_ROLE_ARN:\n",
    "    print(\"✅ SLS访问凭证配置正确\")\n",
    "else:\n",
    "    print(\"❌ 请在环境变量文件中配置ALIBABA_CLOUD_ACCESS_KEY_ID和ALIBABA_CLOUD_ACCESS_KEY_SECRET\")\n",
    "\n",
    "\n",
    "def get_sts_credentials():\n",
    "    \n",
    "    if not all([MAIN_ACCOUNT_ACCESS_KEY_ID, MAIN_ACCOUNT_ACCESS_KEY_SECRET, ALIBABA_CLOUD_ROLE_ARN]):\n",
    "        print(\"❌ 个人账号信息缺失! 请在环境变量文件中配置 ALIBABA_CLOUD_ACCESS_KEY_ID, ALIBABA_CLOUD_ACCESS_KEY_SECRET\")\n",
    "        return None\n",
    "\n",
    "    config = open_api_models.Config(\n",
    "        access_key_id=MAIN_ACCOUNT_ACCESS_KEY_ID, # type: ignore\n",
    "        access_key_secret=MAIN_ACCOUNT_ACCESS_KEY_SECRET, # type: ignore\n",
    "        endpoint=f'sts.{REGION}.aliyuncs.com'\n",
    "    )\n",
    "    sts_client = StsClient(config)\n",
    "    \n",
    "    assume_role_request = sts_models.AssumeRoleRequest(\n",
    "        role_arn=ALIBABA_CLOUD_ROLE_ARN, # type: ignore\n",
    "        role_session_name=STS_SESSION_NAME,\n",
    "        duration_seconds=3600\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = sts_client.assume_role(assume_role_request)\n",
    "        print(\"✅ 成功获取访问权限！\")\n",
    "        return response.body.credentials\n",
    "    except TeaException as e:\n",
    "        print(f\"❌ 获取STS临时凭证失败: {e.message}\")\n",
    "        print(f\"  错误码: {e.code}\")\n",
    "        print(\"  请检查:1. 主账号AK是否正确;2. 目标角色ARN是否正确;3. 目标角色的信任策略是否已配置为信任您的主账号。\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 发生未知错误在获取STS凭证时: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 函数：创建SLS客户端 ---\n",
    "def create_sls_client_with_sts():\n",
    "    \n",
    "    sts_credentials = get_sts_credentials()\n",
    "    \n",
    "    if not sts_credentials:\n",
    "        return None\n",
    "        \n",
    "    sls_endpoint = f\"{REGION}.log.aliyuncs.com\"\n",
    "    \n",
    "    # aliyun-log-python-sdk 使用 securityToken 参数\n",
    "    log_client = LogClient(\n",
    "        endpoint=sls_endpoint,\n",
    "        accessKeyId=sts_credentials.access_key_id,\n",
    "        accessKey=sts_credentials.access_key_secret,\n",
    "        securityToken=sts_credentials.security_token  \n",
    "    )\n",
    "    \n",
    "    print(\"✅ SLS客户端已使用临时凭证创建。\")\n",
    "    return log_client\n",
    "\n",
    "# 创建带有STS凭证的SLS客户端\n",
    "log_client_instance = create_sls_client_with_sts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤1：查找高独占时间的Span\n",
    "\n",
    "使用 trace_exclusive_duration 运算符，在异常时段识别独占时间高的span。\n",
    "\n",
    "独占时间：span内实际消耗的时间，不包括其子span消耗的时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Step 1: Finding high exclusive time spans...\n",
      "============================================================\n",
      "获取独占时间数据...\n",
      "正常时间段查询到的独占时间日志条数: 3000\n",
      "开始计算正常时间段的平均独占时间...\n",
      "收集到 13281 个span的独占时间信息\n",
      "查询span的serviceName和spanName信息...\n",
      "从原始span中采样了 3000 个用于计算平均值\n",
      "查询第 1 批，共 500 个span...\n",
      "查询到 100 条记录\n",
      "查询第 2 批，共 500 个span...\n",
      "查询到 100 条记录\n",
      "查询第 3 批，共 500 个span...\n",
      "查询到 100 条记录\n",
      "查询第 4 批，共 500 个span...\n",
      "查询到 100 条记录\n",
      "查询第 5 批，共 500 个span...\n",
      "查询到 100 条记录\n",
      "查询第 6 批，共 500 个span...\n",
      "查询到 100 条记录\n",
      "组合键 frontend-proxy<sep>router flagservice egress 的平均独占时间: 2105060777.78\n",
      "组合键 frontend-web<sep>resourceFetch 的平均独占时间: 131874850.34\n",
      "组合键 frontend-web<sep>documentLoad 的平均独占时间: 367325072.75\n",
      "组合键 frontend<sep>GET 的平均独占时间: 13495926.74\n",
      "组合键 frontend-web<sep>HTTP GET 的平均独占时间: 72617073.17\n",
      "组合键 frontend<sep>grpc.oteldemo.RecommendationService/ListRecommendations 的平均独占时间: 61212609.08\n",
      "组合键 recommendation<sep>get_product_list 的平均独占时间: 39862021.38\n",
      "组合键 frontend-web<sep>HTTP POST 的平均独占时间: 113213333.33\n",
      "组合键 frontend-web<sep>documentFetch 的平均独占时间: 32211111.33\n",
      "组合键 frontend-proxy<sep>router frontend egress 的平均独占时间: 22540037.34\n",
      "组合键 recommendation<sep>/oteldemo.RecommendationService/ListRecommendations 的平均独占时间: 21925908.81\n",
      "组合键 recommendation<sep>/oteldemo.ProductCatalogService/ListProducts 的平均独占时间: 14892128.67\n",
      "组合键 load-generator<sep>POST 的平均独占时间: 5148330.43\n",
      "组合键 frontend<sep>grpc.oteldemo.CartService/GetCart 的平均独占时间: 7286159.67\n",
      "组合键 frontend<sep>render route (pages) /cart 的平均独占时间: 8715785.00\n",
      "组合键 frontend<sep>GET /api/recommendations 的平均独占时间: 15000000.00\n",
      "组合键 frontend<sep>grpc.oteldemo.ProductCatalogService/GetProduct 的平均独占时间: 3807425.70\n",
      "组合键 frontend<sep>grpc.oteldemo.CurrencyService/Convert 的平均独占时间: 4767074.38\n",
      "组合键 cart<sep>POST /oteldemo.CartService/GetCart 的平均独占时间: 4717800.00\n",
      "组合键 frontend-proxy<sep>router image-provider egress 的平均独占时间: 3769000.00\n",
      "组合键 currency<sep>Currency/Convert 的平均独占时间: 4539705.31\n",
      "组合键 cart<sep>POST 的平均独占时间: 4478640.00\n",
      "组合键 cart<sep>HGET 的平均独占时间: 4631800.00\n",
      "组合键 currency<sep>Currency/GetSupportedCurrencies 的平均独占时间: 3594947.00\n",
      "组合键 frontend<sep>grpc.oteldemo.ProductCatalogService/ListProducts 的平均独占时间: 3847082.25\n",
      "组合键 frontend<sep>grpc.oteldemo.CurrencyService/GetSupportedCurrencies 的平均独占时间: 3188118.17\n",
      "组合键 product-catalog<sep>oteldemo.ProductCatalogService/GetProduct 的平均独占时间: 3092606.44\n",
      "组合键 frontend<sep>render route (pages) / 的平均独占时间: 3115563.62\n",
      "组合键 product-catalog<sep>oteldemo.ProductCatalogService/ListProducts 的平均独占时间: 2931132.00\n",
      "组合键 frontend<sep>GET /cart 的平均独占时间: 2872127.00\n",
      "组合键 cart<sep>POST /oteldemo.CartService/EmptyCart 的平均独占时间: 2568200.00\n",
      "组合键 frontend<sep>GET / 的平均独占时间: 2896553.00\n",
      "共计算了 32 个serviceName<sep>spanName组合的平均独占时间\n",
      "⚙️  Analyzer 已初始化，并启用了基线对比功能\n"
     ]
    }
   ],
   "source": [
    "print(\"🔍 Step 1: Finding high exclusive time spans...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from find_root_cause_spans_rt import FindRootCauseSpansRT\n",
    "\n",
    "# 使用基线对比方式初始化根因分析器\n",
    "finder = FindRootCauseSpansRT(\n",
    "    client=log_client_instance,\n",
    "    project_name=PROJECT_NAME,\n",
    "    logstore_name=LOGSTORE_NAME,\n",
    "    region=REGION,\n",
    "    start_time=ANOMALY_START_TIME,\n",
    "    end_time=ANOMALY_END_TIME,\n",
    "    duration_threshold=DURATION_THRESHOLD,\n",
    "    limit_num=LIMIT_NUM,\n",
    "    normal_start_time=NORMAL_START_TIME,\n",
    "    normal_end_time=NORMAL_END_TIME,\n",
    "    minus_average=True,  # 启用基线值相减，有助于更好地检测异常\n",
    "    only_top1_per_trace=False\n",
    ") # type: ignore\n",
    "\n",
    "print(\"⚙️  Analyzer 已初始化，并启用了基线对比功能\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 最多分析 1000 条持续时间大于 2.0 秒的 trace\n",
      "🎯 正在查找贡献独占时间前95%的span...\n",
      "查询到的日志条数: 2000\n",
      "🔧 处理模式: 处理每个trace中的所有span\n",
      "总共找到 4115 个有效的span独占时间数据\n",
      "成功映射 2119 个span的serviceName和spanName\n",
      "方案1覆盖率: 51.49% (2119/4115)\n",
      "✅ 选择方案1：直接使用span_list中的serviceName和spanName（推荐）\n",
      "🚀 [方案1] 使用span_list中的serviceName和spanName进行调整...\n",
      "🚀 [方案1] 无需额外查询，直接处理 4115 个span\n",
      "开始本地计算调整后的独占时间...\n",
      "完成 4115 个span的时间调整计算\n",
      "总独占时间: 10094340059\n",
      "占前95%独占时间的span数量: 1918\n",
      "这些span的累计独占时间: 9590000000, 占总时间的: 95.00%\n",
      "\n",
      "📊 结果:\n",
      "  共找到 1918 个贡献了独占时间前95%的span\n",
      "\n",
      "🔍 示例span IDs:\n",
      "  1. 84f6544f9e2a9f46\n",
      "  2. 481f04f735a798f9\n",
      "  3. ac75fde1026dbf0b\n",
      "  4. e5acfd096fb6acac\n",
      "  5. 0dc22ae39727061c\n",
      "  ... and 1913 more spans\n",
      "查询到的日志条数: 2000\n",
      "🔧 处理模式: 处理每个trace中的所有span\n",
      "总共找到 4115 个有效的span独占时间数据\n",
      "成功映射 2119 个span的serviceName和spanName\n",
      "方案1覆盖率: 51.49% (2119/4115)\n",
      "✅ 选择方案1：直接使用span_list中的serviceName和spanName（推荐）\n",
      "🚀 [方案1] 使用span_list中的serviceName和spanName进行调整...\n",
      "🚀 [方案1] 无需额外查询，直接处理 4115 个span\n",
      "开始本地计算调整后的独占时间...\n",
      "完成 4115 个span的时间调整计算\n",
      "总独占时间: 10094340059\n",
      "占前95%独占时间的span数量: 1918\n",
      "这些span的累计独占时间: 9590000000, 占总时间的: 95.00%\n",
      "\n",
      "📝 已生成用于进一步分析的查询条件\n"
     ]
    }
   ],
   "source": [
    "print(f\"📈 最多分析 {LIMIT_NUM} 条持续时间大于 {DURATION_THRESHOLD/1000000000:.1f} 秒的 trace\")\n",
    "\n",
    "# 查找贡献了独占时间前95%的span\n",
    "print(\"🎯 正在查找贡献独占时间前95%的span...\")\n",
    "\n",
    "top_95_percent_spans = finder.find_top_95_percent_spans()\n",
    "\n",
    "print(f\"\\n📊 结果:\")\n",
    "print(f\"  共找到 {len(top_95_percent_spans)} 个贡献了独占时间前95%的span\")\n",
    "\n",
    "if top_95_percent_spans:\n",
    "    print(f\"\\n🔍 示例span IDs:\")\n",
    "    for i, span_id in enumerate(top_95_percent_spans[:5]):\n",
    "        print(f\"  {i+1}. {span_id}\")\n",
    "    \n",
    "    if len(top_95_percent_spans) > 5:\n",
    "        print(f\"  ... and {len(top_95_percent_spans) - 5} more spans\")\n",
    "        \n",
    "    # Get the query conditions for these spans\n",
    "    span_conditions, detailed_query = finder.get_top_95_percent_spans_query()\n",
    "    print(f\"\\n📝 已生成用于进一步分析的查询条件\")\n",
    "else:\n",
    "    print(\"⚠️  未找到高独占时间的span\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤2：使用 diff_patterns 进行模式分析\n",
    "\n",
    "使用 diff_patterns 运算符，发现高独占时间span与正常span的区别特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Step 2: Pattern analysis with diff_patterns...\n",
      "============================================================\n",
      "📋 已生成用于模式分析的 diff_patterns 查询语句\n",
      "\n",
      "🚀 正在使用SLS客户端执行 diff_patterns 查询...\n",
      "✅ 模式分析完成：共返回 1 条结果记录\n",
      "\n",
      "📊 模式分析结果:\n",
      "  结果 1: {'ret': '[[\"\\\\\"spanName\\\\\"=\\'router flagservice egress\\'\"],[1869],[245],[0.9744525547445255],[0.09729944400317713],[0.8771531107413484],[1.0],[0.0],null]'}\n",
      "  🔍 Analyzing pattern result: {'ret': '[[\"\\\\\"spanName\\\\\"=\\'router flagservice egress\\'\"],[1869],[245],[0.9744525547445255],[0.09729944400317713],[0.8771531107413484],[1.0],[0.0],null]'}\n",
      "    📊 Extracted patterns: ['\"spanName\"=\\'router flagservice egress\\'']\n",
      "    📊 Pattern counts: [1869]\n",
      "    ℹ️ Found spanName pattern: 'router flagservice egress' (count: 1869)\n",
      "    📊 No serviceName patterns found - attempting service inference from spanName patterns\n",
      "    📊 Service inference from spans: {'ad': 1869}\n",
      "\n",
      "🎯 识别出的service模式:\n",
      "  - ad: 1869 次模式匹配\n",
      "\n",
      "💡 在模式中出现最多的serviceName: ad\n"
     ]
    }
   ],
   "source": [
    "print(\"🔍 Step 2: Pattern analysis with diff_patterns...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if top_95_percent_spans:\n",
    "    # 首先将全部高独占时间的span_id拼接成一个字符串，用于diff_patterns查询条件\n",
    "    span_conditions_for_patterns = \" or \".join([f\"spanId='{span_id}'\" for span_id in top_95_percent_spans[:2000]])  # Limit for query size\n",
    "    \n",
    "    param_str = \"\"\"{\"minimum_support_fraction\": 0.03}\"\"\"\n",
    "    # 核心为 diff_patterns 算法调用，进行模式差异分析\n",
    "    diff_patterns_query = f\"\"\"\n",
    "duration > {DURATION_THRESHOLD} | set session enable_remote_functions=true; set session velox_support_row_constructor_enabled=true; \n",
    "with t0 as (\n",
    "    select spanName, serviceName, cast(duration as double) as duration,\n",
    "           JSON_EXTRACT_SCALAR(resources, '$[\"k8s.pod.ip\"]') AS pod_ip,\n",
    "           JSON_EXTRACT_SCALAR(resources, '$[\"k8s.node.name\"]') AS node_name,\n",
    "           JSON_EXTRACT_SCALAR(resources, '$[\"service.version\"]') AS service_version,  \n",
    "           if(({span_conditions_for_patterns}), 'true', 'false') as anomaly_label, \n",
    "           cast(if((statusCode = 2 or statusCode = 3), 1, 0) as double) as error_count \n",
    "    from log\n",
    "), \n",
    "t1 as (\n",
    "    select array_agg(spanName) as spanName, \n",
    "           array_agg(serviceName) as serviceName, \n",
    "           array_agg(duration) as duration,\n",
    "           array_agg(pod_ip) as pod_ip, \n",
    "           array_agg(node_name) as node_name, \n",
    "           array_agg(service_version) as service_version, \n",
    "           array_agg(anomaly_label) as anomaly_label, \n",
    "           array_agg(error_count) as error_count \n",
    "    from t0\n",
    "),\n",
    "t2 as (\n",
    "    select row(spanName, serviceName, anomaly_label) as table_row \n",
    "    from t1\n",
    "),\n",
    "t3 as (\n",
    "    select diff_patterns(table_row, ARRAY['spanName', 'serviceName', 'anomaly_label'], 'anomaly_label', 'true', 'false', '', '', '{param_str}') as ret \n",
    "    from t2\n",
    ")\n",
    "select * from t3\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"📋 已生成用于模式分析的 diff_patterns 查询语句\")\n",
    "    # print(f\"🎯 正在分析前 {min(50, len(top_95_percent_spans))} 个高独占时间span的模式\")\n",
    "    \n",
    "    # 使用SLS客户端进行查询\n",
    "    print(\"\\n🚀 正在使用SLS客户端执行 diff_patterns 查询...\")\n",
    "    \n",
    "    try:\n",
    "        # 创建用于 diff_patterns 查询的 GetLogsRequest\n",
    "        from aliyun.log import GetLogsRequest\n",
    "        \n",
    "        request = GetLogsRequest(\n",
    "            project=PROJECT_NAME,\n",
    "            logstore=LOGSTORE_NAME,\n",
    "            query=diff_patterns_query.strip(),\n",
    "            fromTime=int(time.mktime(datetime.strptime(ANOMALY_START_TIME, \"%Y-%m-%d %H:%M:%S\").timetuple())),\n",
    "            toTime=int(time.mktime(datetime.strptime(ANOMALY_END_TIME, \"%Y-%m-%d %H:%M:%S\").timetuple())),\n",
    "            line=100  # Limit results\n",
    "        )\n",
    "        \n",
    "        # 使用finder的SLS客户端执行查询\n",
    "        patterns_result = finder.client.get_logs(request)\n",
    "        \n",
    "        if patterns_result and patterns_result.get_logs():\n",
    "            logs = [log_item.get_contents() for log_item in patterns_result.get_logs()]\n",
    "            print(f\"✅ 模式分析完成：共返回 {len(logs)} 条结果记录\")\n",
    "            \n",
    "            # 展示模型分析结果\n",
    "            # 在结果中会展示同一service在正常样本和异常样本中出现的次数对比\n",
    "            print(\"\\n📊 模式分析结果:\")\n",
    "            for i, log_entry in enumerate(logs[:3]):  # Show first 3 results\n",
    "                print(f\"  结果 {i+1}: {log_entry}\")\n",
    "                \n",
    "            # Extract service patterns from diff_patterns results\n",
    "            service_patterns = {}\n",
    "            span_patterns = []  # Store span patterns for service inference\n",
    "            \n",
    "            for log_entry in logs:\n",
    "                if hasattr(log_entry, 'get_contents'):\n",
    "                    contents = log_entry.get_contents()\n",
    "                else:\n",
    "                    contents = log_entry\n",
    "                print(f\"  🔍 Analyzing pattern result: {contents}\")\n",
    "                \n",
    "                # Parse structured result from diff_patterns query\n",
    "                if 'ret' in contents:\n",
    "                    ret_value = contents['ret']\n",
    "                    \n",
    "                    if isinstance(ret_value, str):\n",
    "                        try:\n",
    "                            # Replace 'null' with 'None' for Python parsing\n",
    "                            data_str = ret_value.replace('null', 'None')\n",
    "                            result = eval(data_str)\n",
    "                            \n",
    "                            if len(result) >= 2 and isinstance(result[0], list) and isinstance(result[1], list):\n",
    "                                patterns = result[0]  # Pattern names\n",
    "                                counts = result[1]    # Pattern counts\n",
    "                                \n",
    "                                print(f\"    📊 Extracted patterns: {patterns}\")\n",
    "                                print(f\"    📊 Pattern counts: {counts}\")\n",
    "                                \n",
    "                                for i, pattern in enumerate(patterns):\n",
    "                                    if i < len(counts):\n",
    "                                        count = counts[i]\n",
    "                                        \n",
    "                                        # Parse serviceName patterns from diff_patterns results\n",
    "                                        if 'serviceName' in pattern and '=' in pattern:\n",
    "                                            # Handle complex patterns like \"serviceName\"='cart' AND \"spanName\"='POST'\n",
    "                                            import re\n",
    "                                            match = re.search(r'\"serviceName\"=\\'([^\\']+)\\'', pattern)\n",
    "                                            if match:\n",
    "                                                service_part = match.group(1)\n",
    "                                                service_patterns[service_part] = service_patterns.get(service_part, 0) + count\n",
    "                                                print(f\"    ✅ Found serviceName pattern: '{service_part}' (count: {count})\")\n",
    "                                            else:\n",
    "                                                print(f\"    ⚠️ Could not parse serviceName from: '{pattern}'\")\n",
    "                                        \n",
    "                                        # Log spanName patterns for service inference\n",
    "                                        elif 'spanName' in pattern:\n",
    "                                            span_name = pattern.split('=')[1].strip('\\'\"') if '=' in pattern else pattern\n",
    "                                            print(f\"    ℹ️ Found spanName pattern: '{span_name}' (count: {count})\")\n",
    "                                            span_patterns.append((span_name, count))\n",
    "                                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"    ⚠️ Error parsing ret field: {e}\")\n",
    "                            import traceback\n",
    "                            traceback.print_exc()\n",
    "                \n",
    "            # If no serviceName patterns found, try to infer from spanName patterns\n",
    "            if not service_patterns and span_patterns:\n",
    "                print(f\"    📊 No serviceName patterns found - attempting service inference from spanName patterns\")\n",
    "                \n",
    "                service_candidates = {}\n",
    "                for span_name, count in span_patterns:\n",
    "                    # Map spanName patterns to likely services\n",
    "                    if 'CartService' in span_name or 'cart' in span_name.lower():\n",
    "                        service_candidates['cart'] = service_candidates.get('cart', 0) + count\n",
    "                    elif 'ProductCatalogService' in span_name or 'product' in span_name.lower():\n",
    "                        service_candidates['product-catalog'] = service_candidates.get('product-catalog', 0) + count  \n",
    "                    elif 'PaymentService' in span_name or 'payment' in span_name.lower():\n",
    "                        service_candidates['payment'] = service_candidates.get('payment', 0) + count\n",
    "                    elif 'CheckoutService' in span_name or 'checkout' in span_name.lower():\n",
    "                        service_candidates['checkout'] = service_candidates.get('checkout', 0) + count\n",
    "                    elif 'RecommendationService' in span_name or 'recommendation' in span_name.lower():\n",
    "                        service_candidates['recommendation'] = service_candidates.get('recommendation', 0) + count\n",
    "                    elif 'CurrencyService' in span_name or 'currency' in span_name.lower():\n",
    "                        service_candidates['currency'] = service_candidates.get('currency', 0) + count\n",
    "                    elif 'frontend' in span_name.lower():\n",
    "                        service_candidates['frontend'] = service_candidates.get('frontend', 0) + count\n",
    "                    elif 'flagservice' in span_name.lower() or 'ad' in span_name.lower():\n",
    "                        service_candidates['ad'] = service_candidates.get('ad', 0) + count\n",
    "                    else:\n",
    "                        print(f\"    ❓ Cannot infer service from span: '{span_name}'\")\n",
    "                \n",
    "                if service_candidates:\n",
    "                    print(f\"    📊 Service inference from spans: {dict(service_candidates)}\")\n",
    "                    service_patterns = service_candidates\n",
    "                else:\n",
    "                    print(f\"    ❌ Cannot determine target service from available span patterns\")\n",
    "                    \n",
    "            # Store span patterns globally for additional analysis if needed\n",
    "            globals()['span_patterns'] = span_patterns\n",
    "                    \n",
    "            if service_patterns:\n",
    "                print(f\"\\n🎯 识别出的service模式:\")\n",
    "                for service, count in sorted(service_patterns.items(), key=lambda x: x[1], reverse=True):\n",
    "                    print(f\"  - {service}: {count} 次模式匹配\")\n",
    "                    \n",
    "                # 根据最常见模式更新TARGET_SERVICE\n",
    "                most_common_service = max(service_patterns.items(), key=lambda x: x[1])[0]\n",
    "                print(f\"\\n💡 在模式中出现最多的serviceName: {most_common_service}\")\n",
    "                \n",
    "                # 存储出现最频繁的service name，供后续步骤使用\n",
    "                globals()['TARGET_SERVICE_FROM_PATTERNS'] = most_common_service\n",
    "                \n",
    "        else:\n",
    "            print(\"⚠️  未返回任何模式分析结果\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 执行 diff_patterns 查询时出错: {e}\")\n",
    "        print(\"💡 建议在SLS控制台手动执行\")\n",
    "    \n",
    "        # 打印查询语句用于SLS控制台手动执行（如有需要）\n",
    "        print(\"\\n📝 SLS控制台手动执行用查询语句（如有需要）：\")\n",
    "        print(\"=\"*50)\n",
    "        print(diff_patterns_query)\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(\"\\n💡 预期结果：该查询应该能识别哪个 serviceName 拥有最多高独占时间的span\")\n",
    "        print(\"   例如：分析结果可能显示 serviceName='recommendation' 的span占比最高\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  无法进行模式分析 - 未找到高独占时间的span\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤3：CPU指标分析\n",
    "\n",
    "根据模式分析结果（例如，如果 recommendation 服务显示独占时间高），使用CMS实体查询，对该service的CPU指标进行查询。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Step 3: CPU Metrics Analysis...\n",
      "============================================================\n",
      "✅ 成功获取临时访问凭证！\n",
      "✅ 已通过导入的 TestCMSQuery 初始化CMS客户端\n",
      "🔧 CMS客户端已初始化\n",
      "🔧 workspace: quanxi-tianchi-test\n",
      "🔧 Endpoint: cms.cn-qingdao.aliyuncs.com\n",
      "🎯 Using service from diff_patterns: ad\n",
      "🎯 正在分析目标service:ad的CPU指标\n"
     ]
    }
   ],
   "source": [
    "print(\"🔍 Step 3: CPU Metrics Analysis...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 初始化CMS测试客户端，用于指标查询\n",
    "# 如果存在导入问题通过直接创建类修复(except)\n",
    "try:\n",
    "    if TestCMSQuery is not None:\n",
    "        cms_tester = TestCMSQuery()\n",
    "        cms_tester.setUp()\n",
    "        print(f\"✅ 已通过导入的 TestCMSQuery 初始化CMS客户端\")\n",
    "    else:\n",
    "        raise ImportError(\"TestCMSQuery is None\")\n",
    "except:\n",
    "    print(\"⚠️  TestCMSQuery import failed, creating CMS client directly...\")\n",
    "    \n",
    "    import os\n",
    "    from alibabacloud_cms20240330.client import Client as Cms20240330Client\n",
    "    from alibabacloud_tea_openapi import models as open_api_models\n",
    "    from alibabacloud_cms20240330 import models as cms_20240330_models\n",
    "    from alibabacloud_tea_util import models as util_models\n",
    "    \n",
    "    \n",
    "    class DirectCMSClient:\n",
    "        def __init__(self):\n",
    "            self.access_key_id = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_ID')\n",
    "            self.access_key_secret = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_SECRET')\n",
    "            self.workspace = CMS_WORKSPACE\n",
    "            self.endpoint = CMS_ENDPOINT\n",
    "            \n",
    "            if not self.access_key_id or not self.access_key_secret:\n",
    "                raise ValueError(\"请设置环境变量 ALIBABA_CLOUD_ACCESS_KEY_ID 和 ALIBABA_CLOUD_ACCESS_KEY_SECRET\")\n",
    "            \n",
    "            config = open_api_models.Config(\n",
    "                access_key_id=self.access_key_id,\n",
    "                access_key_secret=self.access_key_secret,\n",
    "            )\n",
    "            config.endpoint = self.endpoint\n",
    "            self.cms_client = Cms20240330Client(config)\n",
    "        \n",
    "        def _execute_spl_query(self, query: str, from_time: int = None, to_time: int = None):\n",
    "            \"\"\"执行SPL查询\"\"\"\n",
    "            if from_time is None:\n",
    "                from_time = int(time.time()) - 60 * 60 * 1\n",
    "            if to_time is None:\n",
    "                to_time = int(time.time())\n",
    "            \n",
    "            try:\n",
    "                headers = cms_20240330_models.GetEntityStoreDataHeaders()\n",
    "                request = cms_20240330_models.GetEntityStoreDataRequest(\n",
    "                    query=query,\n",
    "                    from_=from_time,\n",
    "                    to=to_time\n",
    "                )\n",
    "                runtime = util_models.RuntimeOptions()\n",
    "                response = self.cms_client.get_entity_store_data_with_options(\n",
    "                    self.workspace, request, headers, runtime\n",
    "                )\n",
    "                return response.body\n",
    "            except Exception as e:\n",
    "                print(f\"❌ CMS查询错误: {e}\")\n",
    "                return None\n",
    "    \n",
    "    cms_tester = DirectCMSClient()\n",
    "    print(f\"✅ CMS client created directly\")\n",
    "\n",
    "print(f\"🔧 CMS客户端已初始化\")\n",
    "print(f\"🔧 workspace: {CMS_WORKSPACE}\")\n",
    "print(f\"🔧 Endpoint: {CMS_ENDPOINT}\")\n",
    "\n",
    "# Determine target service from runtime observations\n",
    "def infer_service_from_additional_evidence():\n",
    "    \"\"\"Try additional service inference methods when primary analysis fails\"\"\"\n",
    "    if 'span_patterns' not in globals():\n",
    "        return None\n",
    "    \n",
    "    service_candidates = {}\n",
    "    for span_name, count in span_patterns:\n",
    "        # More comprehensive service mapping\n",
    "        if 'CartService' in span_name:\n",
    "            service_candidates['cart'] = service_candidates.get('cart', 0) + count\n",
    "        elif 'ProductCatalogService' in span_name:\n",
    "            service_candidates['product-catalog'] = service_candidates.get('product-catalog', 0) + count  \n",
    "        elif 'PaymentService' in span_name:\n",
    "            service_candidates['payment'] = service_candidates.get('payment', 0) + count\n",
    "        elif 'CheckoutService' in span_name:\n",
    "            service_candidates['checkout'] = service_candidates.get('checkout', 0) + count\n",
    "        elif 'RecommendationService' in span_name:\n",
    "            service_candidates['recommendation'] = service_candidates.get('recommendation', 0) + count\n",
    "        elif 'CurrencyService' in span_name:\n",
    "            service_candidates['currency'] = service_candidates.get('currency', 0) + count\n",
    "        elif 'frontend' in span_name.lower():\n",
    "            service_candidates['frontend'] = service_candidates.get('frontend', 0) + count\n",
    "        elif 'flagservice' in span_name.lower():\n",
    "            service_candidates['ad'] = service_candidates.get('ad', 0) + count\n",
    "    \n",
    "    if service_candidates:\n",
    "        best_service = max(service_candidates.items(), key=lambda x: x[1])\n",
    "        return best_service[0]\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Determine target service using runtime data\n",
    "if 'TARGET_SERVICE_FROM_PATTERNS' in globals():\n",
    "    TARGET_SERVICE = TARGET_SERVICE_FROM_PATTERNS \n",
    "    print(f\"🎯 Using service from diff_patterns: {TARGET_SERVICE}\")\n",
    "else:\n",
    "    # Try additional inference methods\n",
    "    inferred_service = infer_service_from_additional_evidence()\n",
    "    if inferred_service:\n",
    "        TARGET_SERVICE = inferred_service\n",
    "        print(f\"🎯 Inferred service from span patterns: {TARGET_SERVICE}\")\n",
    "    else:\n",
    "        print(f\"🎯 Cannot determine target service from available data\")\n",
    "        if 'span_patterns' in globals():\n",
    "            print(f\"    Available span patterns: {globals()['span_patterns']}\")\n",
    "        TARGET_SERVICE = None\n",
    "\n",
    "if TARGET_SERVICE:\n",
    "    print(f\"🎯 正在分析目标service:{TARGET_SERVICE}的CPU指标\")\n",
    "else:\n",
    "    print(f\"⚠️ 无法确定目标service，将跳过service级别的分析\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 正在查询目标service的CPU使用率与限制...\n",
      "🔍 Query: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='ad'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_vs_limits', 'range', '1m')\n",
      "🔍 查询参数:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  时间范围: 2025-08-28 16:04:30 到 2025-08-28 16:19:30\n",
      "  查询语句: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='ad'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_vs_limits', 'range', '1m')\n",
      "\n",
      "📊 查询响应:\n",
      "  状态码: 200\n",
      "  返回header: ['__labels__', '__name__', '__ts__', '__value__', '__source__']\n",
      "  返回data行数: 1\n",
      "\n",
      "✅ 已获取 CPU使用率与限制 数据：共 1 条记录\n",
      "📋 Fields: ['__labels__', '__name__', '__ts__', '__value__', '__source__']\n",
      "📊 数据样例（前3条）:\n",
      "  Record 1: ['{}', 'null', '[1756368270000000000,1756368330000000000,1756368390000000000,1756368450000000000,1756368510000000000,1756368570000000000,1756368630000000000,1756368690000000000,1756368750000000000,1756368810000000000,1756368870000000000,1756368930000000000,1756368990000000000,1756369050000000000,1756369110000000000,1756369170000000000]', '[84.80424496849061,84.70326216377088,48.483419603056947,48.550235283462047,48.745470612906299,49.63776885048156,48.97956500230081,46.312801670803128,83.14145336417026,40.15562929365869,41.73813178811215,41.81709069273207,83.61013319402048,84.48045858716316,84.39578008025194,84.14853695560683]', '']\n"
     ]
    }
   ],
   "source": [
    "# 查询CPU使用率与限制\n",
    "print(\"📊 正在查询目标service的CPU使用率与限制...\")\n",
    "\n",
    "if TARGET_SERVICE:\n",
    "    cpu_usage_vs_limits_query = f\"\"\"\n",
    ".entity_set with(domain='k8s', name='k8s.deployment', query=`name='{TARGET_SERVICE}'` ) \n",
    "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_vs_limits', 'range', '1m')\n",
    "\"\"\"\n",
    "    print(f\"🔍 Query: {cpu_usage_vs_limits_query.strip()}\")\n",
    "else:\n",
    "    print(f\"⚠️ 跳过CPU分析 - 无目标service\")\n",
    "    cpu_usage_vs_limits_query = None\n",
    "\n",
    "# 执行查询\n",
    "if TARGET_SERVICE and cpu_usage_vs_limits_query:\n",
    "    try:\n",
    "        # 将时间字符串转换为CMS查询所需时间戳\n",
    "        from_time = int(time.mktime(datetime.strptime(NORMAL_START_TIME, \"%Y-%m-%d %H:%M:%S\").timetuple()))\n",
    "        to_time = int(time.mktime(datetime.strptime(ANOMALY_END_TIME, \"%Y-%m-%d %H:%M:%S\").timetuple()))\n",
    "        \n",
    "        cpu_usage_vs_limits_result = cms_tester._execute_spl_query(\n",
    "            cpu_usage_vs_limits_query.strip(),\n",
    "            from_time=from_time,\n",
    "            to_time=to_time\n",
    "        )\n",
    "        \n",
    "        if cpu_usage_vs_limits_result and cpu_usage_vs_limits_result.data:\n",
    "            print(f\"✅ 已获取 CPU使用率与限制 数据：共 {len(cpu_usage_vs_limits_result.data)} 条记录\")\n",
    "            \n",
    "            # 展示部分数据样例\n",
    "            if cpu_usage_vs_limits_result.header:\n",
    "                print(f\"📋 Fields: {cpu_usage_vs_limits_result.header}\")\n",
    "            \n",
    "            print(f\"📊 数据样例（前3条）:\")\n",
    "            for i, record in enumerate(cpu_usage_vs_limits_result.data[:3]):\n",
    "                print(f\"  Record {i+1}: {record}\")\n",
    "        else:\n",
    "            print(f\"⚠️  未找到 {TARGET_SERVICE} 的 CPU使用率与限制 数据\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 查询 CPU使用率与限制 时出错: {e}\")\n",
    "else:\n",
    "    print(\"⚠️ 跳过CPU分析 - 无有效目标service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 正在查询目标service的内存使用率与限制（用于对比）...\n",
      "🔍 Query: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='ad'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_vs_limits', 'range', '1m')\n",
      "🔍 查询参数:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  时间范围: 2025-08-28 16:04:30 到 2025-08-28 16:19:30\n",
      "  查询语句: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='ad'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_vs_limits', 'range', '1m')\n",
      "\n",
      "📊 查询响应:\n",
      "  状态码: 200\n",
      "  返回header: ['__labels__', '__name__', '__ts__', '__value__', '__source__']\n",
      "  返回data行数: 1\n",
      "\n",
      "✅ 已获取内存使用率与限制数据：共: 1 条记录\n",
      "📊 数据样例（前3条）:\n",
      "  Record 1: ['{}', 'null', '[1756368270000000000,1756368330000000000,1756368390000000000,1756368450000000000,1756368510000000000,1756368570000000000,1756368630000000000,1756368690000000000,1756368750000000000,1756368810000000000,1756368870000000000,1756368930000000000,1756368990000000000,1756369050000000000,1756369110000000000,1756369170000000000]', '[63.032986111111117,63.10199652777778,39.310763888888889,48.71918402777778,55.824869791666667,28.9296875,42.41493055555556,49.654296875,52.95269097222223,37.02821180555556,48.14605034722222,56.29774305555556,61.69357638888889,60.81727430555556,61.55078125,61.588541666666667]', '']\n"
     ]
    }
   ],
   "source": [
    "# 查询该service的内存使用率与限制，作为对比\n",
    "print(\"📊 正在查询目标service的内存使用率与限制（用于对比）...\")\n",
    "\n",
    "memory_usage_vs_limits_query = f\"\"\"\n",
    ".entity_set with(domain='k8s', name='k8s.deployment', query=`name='{TARGET_SERVICE}'` ) \n",
    "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_vs_limits', 'range', '1m')\n",
    "\"\"\"\n",
    "\n",
    "print(f\"🔍 Query: {memory_usage_vs_limits_query.strip()}\")\n",
    "\n",
    "try:\n",
    "    memory_usage_vs_limits_result = cms_tester._execute_spl_query(\n",
    "        memory_usage_vs_limits_query.strip(),\n",
    "        from_time=from_time,\n",
    "        to_time=to_time\n",
    "    )\n",
    "    \n",
    "    if memory_usage_vs_limits_result and memory_usage_vs_limits_result.data:\n",
    "        print(f\"✅ 已获取内存使用率与限制数据：共: {len(memory_usage_vs_limits_result.data)} 条记录\")\n",
    "        \n",
    "        # 展示数据样例\n",
    "        print(f\"📊 数据样例（前3条）:\")\n",
    "        for i, record in enumerate(memory_usage_vs_limits_result.data[:3]):\n",
    "            print(f\"  Record {i+1}: {record}\")\n",
    "    else:\n",
    "        print(f\"⚠️  未找到 {TARGET_SERVICE} 的内存使用率与限制数据\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ 查询内存使用率与限制时出错: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤4：使用 series_decompose_anomalies 进行异常检测\n",
    "\n",
    "利用SPL内置的异常检测功能，自动识别在延迟突增期间CPU使用率指标是否出现异常行为。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Step 4: CPU Usage Anomaly Detection...\n",
      "============================================================\n",
      "🔍 Query: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='ad'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "🔍 查询参数:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  时间范围: 2025-08-28 16:04:30 到 2025-08-28 16:19:30\n",
      "  查询语句: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='ad'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "📊 查询响应:\n",
      "  状态码: 200\n",
      "  返回header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  返回data行数: 1\n",
      "\n",
      "✅ 已获取异常检测结果：共 1 条记录\n",
      "📋 Fields: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  Record 1: ['{}', 'null', '[1756368270000000000,1756368330000000000,1756368390000000000,1756368450000000000,1756368510000000000,1756368570000000000,1756368630000000000,1756368690000000000,1756368750000000000,1756368810000000000,1756368870000000000,1756368930000000000,1756368990000000000,1756369050000000000,1756369110000000000,1756369170000000000]', '[0.005228085438315369,0.005193791837076788,0.01574098264742213,0.05080104782465078,0.08491927941541674,0.1113010170296222,0.15188487392446407,0.17759752364160309,0.12284808320152696,0.15847613546839474,0.20648986170445578,0.21362204922587045,0.1278870787174181,0.1345670313771442,0.11598737606642884,0.06708615128261136]', '[[0.1092253029346466,0.10929421335458756,0.10935002565383911,0.1092720702290535,0.10951691120862961,0.10960525274276734,0.10960283130407334,0.10964372754096985,0.10960360616445542,0.10974538326263428,0.10979123413562775,0.10991497337818146,0.10995426774024964,0.11001035571098328,0.11001100391149521,0.1099604144692421],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],null]', '[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]', '[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null]', 'null', '']\n",
      "🚨 在 ad 的CPU使用率中检测到异常！\n",
      "    📊 High anomaly score detected\n",
      "    📊 High anomaly score detected\n"
     ]
    }
   ],
   "source": [
    "print(\"🔍 Step 4: CPU Usage Anomaly Detection...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 使用异常检测进行查询\n",
    "cpu_anomaly_detection_query = f\"\"\"\n",
    ".entity_set with(domain='k8s', name='k8s.deployment', query=`name='{TARGET_SERVICE}'` ) \n",
    "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_total', 'range', '1m')\n",
    "| extend ret = series_decompose_anomalies(__value__, '{{\"confidence\": 0.035}}')\n",
    "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
    "\"\"\"\n",
    "\n",
    "print(f\"🔍 Query: {cpu_anomaly_detection_query.strip()}\")\n",
    "\n",
    "try:\n",
    "    # 增加时间范围以获得更好的异常检测上下文\n",
    "    extended_from_time = from_time \n",
    "    extended_to_time = to_time \n",
    "    \n",
    "    anomaly_result = cms_tester._execute_spl_query(\n",
    "        cpu_anomaly_detection_query.strip(),\n",
    "        from_time=extended_from_time,\n",
    "        to_time=extended_to_time\n",
    "    )\n",
    "    \n",
    "    if anomaly_result and anomaly_result.data:\n",
    "        print(f\"✅ 已获取异常检测结果：共 {len(anomaly_result.data)} 条记录\")\n",
    "        \n",
    "        if anomaly_result.header:\n",
    "            print(f\"📋 Fields: {anomaly_result.header}\")\n",
    "        \n",
    "        # 在结果中查找异常指示\n",
    "        anomaly_found = False\n",
    "        anomaly_details = []\n",
    "        \n",
    "        for i, record in enumerate(anomaly_result.data[:5]):\n",
    "            print(f\"  Record {i+1}: {record}\")\n",
    "            \n",
    "            # 检查是否有 ExceedUpperBound 或其他异常指示\n",
    "            if isinstance(record, (list, tuple)) and len(record) > 2:\n",
    "                # 查找 anomalies_type_series 字段（通常在倒数第二位或指定位置）\n",
    "                for item in record:\n",
    "                    if isinstance(item, str):\n",
    "                        # 检查 anomalies_type_series 中是否有 ExceedUpperBound\n",
    "                        if 'ExceedUpperBound' in item:\n",
    "                            anomaly_found = True\n",
    "                            anomaly_details.append('ExceedUpperBound detected')\n",
    "                        # 检查其他异常指示\n",
    "                        elif 'ExceedLowerBound' in item:\n",
    "                            anomaly_found = True\n",
    "                            anomaly_details.append('ExceedLowerBound detected')\n",
    "                        # 检查异常分数是否大于0\n",
    "                        elif any(x in item for x in ['1.0', '1,0'] if ',' in item or '.' in item):\n",
    "                            # This indicates anomaly score\n",
    "                            if '1.0' in item or '1,0' in item:\n",
    "                                anomaly_found = True\n",
    "                                anomaly_details.append('High anomaly score detected')\n",
    "        \n",
    "        if anomaly_found:\n",
    "            print(f\"🚨 在 {TARGET_SERVICE} 的CPU使用率中检测到异常！\")\n",
    "            for detail in anomaly_details:\n",
    "                print(f\"    📊 {detail}\")\n",
    "        else:\n",
    "            print(f\"✅ {TARGET_SERVICE} 的CPU使用率未检测到明显异常\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"⚠️  未找到 {TARGET_SERVICE} 的异常检测结果\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ 异常检测过程中出错: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤5：根因分析总结\n",
    "\n",
    "总结分析结果，并确定最可能的根因候选项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Step 5: Root Cause Analysis Summary\n",
      "============================================================\n",
      "📊 分析总结：\n",
      "   异常时间段：2025-08-28 16:14:30 到 2025-08-28 16:19:30\n",
      "   正常时间段：2025-08-28 16:04:30 到 2025-08-28 16:14:30\n",
      "   分析目标service：ad\n",
      "   发现高独占时间的span数量：1918\n",
      "\n",
      "🎯 根因发现：\n",
      "   ✅ 已获取 ad 的CPU使用数据\n",
      "   ✅ 已获取 ad 的内存使用数据\n",
      "   ✅ 异常检测分析已完成\n",
      "   📊 检测到的异常点总数：0\n",
      "   ℹ️  异常检测已完成，但未发现异常\n",
      "\n",
      "🏆 根因候选：\n",
      "   🎯 ad.cpu\n",
      "   📈 置信度：中\n",
      "   ❌ 证据：FALSE（未确认异常）\n",
      "   📝 支持证据：\n",
      "      - 发现 1918 个高独占时间span\n",
      "      - 模式分析表明涉及服务 ad\n",
      "      - 有CPU指标数据，但未检测到明显异常\n",
      "\n",
      "💡 建议：\n",
      "   - 进一步排查 ad 部署的资源使用\n",
      "   - 检查异常期间 ad 的日志\n",
      "   - 即使未检测到明显异常，也要核查是否存在细微性能问题\n",
      "\n",
      "============================================================\n",
      "🎯 最终答复：ad.cpu\n",
      "📈 置信度：中\n",
      "🔍 证据：FALSE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"🔍 Step 5: Root Cause Analysis Summary\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 分析结果\n",
    "print(f\"📊 分析总结：\")\n",
    "print(f\"   异常时间段：{ANOMALY_START_TIME} 到 {ANOMALY_END_TIME}\")\n",
    "print(f\"   正常时间段：{NORMAL_START_TIME} 到 {NORMAL_END_TIME}\")\n",
    "print(f\"   分析目标service：{TARGET_SERVICE}\")\n",
    "print(f\"   发现高独占时间的span数量：{len(top_95_percent_spans) if 'top_95_percent_spans' in locals() else 0}\")\n",
    "\n",
    "print(f\"\\n🎯 根因发现：\")\n",
    "\n",
    "# 检查是否有CPU问题的证据\n",
    "cpu_evidence = False\n",
    "memory_evidence = False\n",
    "anomaly_evidence = False\n",
    "\n",
    "if 'cpu_usage_vs_limits_result' in locals() and cpu_usage_vs_limits_result and cpu_usage_vs_limits_result.data:\n",
    "    cpu_evidence = True\n",
    "    print(f\"   ✅ 已获取 {TARGET_SERVICE} 的CPU使用数据\")\n",
    "else:\n",
    "    print(f\"   ❌ 未找到 {TARGET_SERVICE} 的CPU使用数据\")\n",
    "\n",
    "if 'memory_usage_vs_limits_result' in locals() and memory_usage_vs_limits_result and memory_usage_vs_limits_result.data:\n",
    "    memory_evidence = True\n",
    "    print(f\"   ✅ 已获取 {TARGET_SERVICE} 的内存使用数据\")\n",
    "else:\n",
    "    print(f\"   ❌ 未找到 {TARGET_SERVICE} 的内存使用数据\")\n",
    "\n",
    "# 检查实际异常点，至少需要3个异常点\n",
    "if 'anomaly_result' in locals() and anomaly_result and anomaly_result.data:\n",
    "    print(f\"   ✅ 异常检测分析已完成\")\n",
    "    \n",
    "    # 统计异常点数量，至少3个才能确认有异常\n",
    "    anomaly_point_count = 0\n",
    "    anomaly_types_found = []\n",
    "    \n",
    "    for record in anomaly_result.data:\n",
    "        if isinstance(record, (list, tuple)):\n",
    "            for item in record:\n",
    "                if isinstance(item, str):\n",
    "                    # 统计ExceedUpperBound出现次数\n",
    "                    exceed_upper_count = item.count('ExceedUpperBound')\n",
    "                    exceed_lower_count = item.count('ExceedLowerBound')\n",
    "                    \n",
    "                    anomaly_point_count += exceed_upper_count + exceed_lower_count\n",
    "                    \n",
    "                    if exceed_upper_count > 0:\n",
    "                        anomaly_types_found.extend(['ExceedUpperBound'] * exceed_upper_count)\n",
    "                    if exceed_lower_count > 0:\n",
    "                        anomaly_types_found.extend(['ExceedLowerBound'] * exceed_lower_count)\n",
    "    \n",
    "    print(f\"   📊 检测到的异常点总数：{anomaly_point_count}\")\n",
    "    \n",
    "    # 只有存在3个及以上异常点才确认异常\n",
    "    if anomaly_point_count >= 3:\n",
    "        anomaly_evidence = True\n",
    "        print(f\"   🚨 异常确认：发现 {anomaly_point_count} 个异常点（已达≥3阈值）\")\n",
    "        print(f\"   📝 异常类型：{', '.join(set(anomaly_types_found))}\")\n",
    "    elif anomaly_point_count > 0:\n",
    "        anomaly_evidence = False\n",
    "        print(f\"   ⚠️  异常证据不足：仅检测到 {anomaly_point_count} 个点（需要≥3）\")\n",
    "    else:\n",
    "        anomaly_evidence = False\n",
    "        print(f\"   ℹ️  异常检测已完成，但未发现异常\")\n",
    "else:\n",
    "    print(f\"   ❌ 异常检测分析失败\")\n",
    "\n",
    "print(f\"\\n🏆 根因候选：\")\n",
    "\n",
    "# 基于evidence的评估：只有实际检测到异常点才设置 evidence=True\n",
    "evidence = anomaly_evidence and len(top_95_percent_spans) > 0\n",
    "\n",
    "if evidence and cpu_evidence:\n",
    "    root_cause_candidate = f\"{TARGET_SERVICE}.cpu\"\n",
    "    confidence = \"高\"\n",
    "    \n",
    "    print(f\"   🎯 {root_cause_candidate}\")\n",
    "    print(f\"   📈 置信度：{confidence}\")\n",
    "    print(f\"   ✅ 证据：TRUE（已检测到异常）\")\n",
    "    print(f\"   📝 支持证据：\")\n",
    "    print(f\"      - 发现 {len(top_95_percent_spans)} 个高独占时间span\")\n",
    "    print(f\"      - 模式分析表明涉及服务 {TARGET_SERVICE}\")\n",
    "    print(f\"      - 有CPU指标数据可详细分析\")\n",
    "    print(f\"      - 自动异常检测确认了CPU使用异常\")\n",
    "        \n",
    "elif len(top_95_percent_spans) > 0 and cpu_evidence:\n",
    "    root_cause_candidate = f\"{TARGET_SERVICE}.cpu\"\n",
    "    confidence = \"中\"\n",
    "    \n",
    "    print(f\"   🎯 {root_cause_candidate}\")\n",
    "    print(f\"   📈 置信度：{confidence}\")\n",
    "    print(f\"   ❌ 证据：FALSE（未确认异常）\")\n",
    "    print(f\"   📝 支持证据：\")\n",
    "    print(f\"      - 发现 {len(top_95_percent_spans)} 个高独占时间span\")\n",
    "    print(f\"      - 模式分析表明涉及服务 {TARGET_SERVICE}\")\n",
    "    print(f\"      - 有CPU指标数据，但未检测到明显异常\")\n",
    "    \n",
    "else:\n",
    "    root_cause_candidate = \"unknown\"\n",
    "    confidence = \"低\"\n",
    "    \n",
    "    print(f\"   🎯 {root_cause_candidate}\")\n",
    "    print(f\"   📈 置信度：{confidence}\")\n",
    "    print(f\"   ❌ 证据：FALSE（证据不足）\")\n",
    "    print(f\"   📝 支持证据：\")\n",
    "    print(f\"      - 高独占时间span或指标数据有限\")\n",
    "    print(f\"      - 分析可能需要不同的参数或时间范围\")\n",
    "\n",
    "print(f\"\\n💡 建议：\")\n",
    "if evidence:\n",
    "    print(f\"   - 检查 {TARGET_SERVICE} 部署的CPU资源限制\")\n",
    "    print(f\"   - 检查异常期间是否有高CPU消耗操作\")\n",
    "    print(f\"   - 考虑扩容 {TARGET_SERVICE} 或优化CPU使用\")\n",
    "elif confidence == \"中\":\n",
    "    print(f\"   - 进一步排查 {TARGET_SERVICE} 部署的资源使用\")\n",
    "    print(f\"   - 检查异常期间 {TARGET_SERVICE} 的日志\")\n",
    "    print(f\"   - 即使未检测到明显异常，也要核查是否存在细微性能问题\")\n",
    "else:\n",
    "    print(f\"   - 调整分析参数（时间范围、持续时间阈值等）\")\n",
    "    print(f\"   - 核查指定时间段内数据是否齐全\")\n",
    "    print(f\"   - 可考虑扩展到其他服务的分析\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"🎯 最终答复：{root_cause_candidate}\")\n",
    "print(f\"📈 置信度：{confidence}\")\n",
    "print(f\"🔍 证据：{'TRUE' if evidence else 'FALSE'}\")\n",
    "print(f\"\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 附加分析工具\n",
    "\n",
    "以下代码提供了可按需运行的附加分析功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 附加分析：其他service的CPU和内存异常检测\n",
      "============================================================\n",
      "\n",
      "🔍 正在分析 frontend 服务的异常...\n",
      "🔍 查询参数:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  时间范围: 2025-08-28 16:04:30 到 2025-08-28 16:19:30\n",
      "  查询语句: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='frontend'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "📊 查询响应:\n",
      "  状态码: 200\n",
      "  返回header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  返回data行数: 1\n",
      "\n",
      "   ✅ frontend：已获取CPU指标数据\n",
      "🔍 查询参数:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  时间范围: 2025-08-28 16:04:30 到 2025-08-28 16:19:30\n",
      "  查询语句: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='frontend'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "📊 查询响应:\n",
      "  状态码: 200\n",
      "  返回header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  返回data行数: 1\n",
      "\n",
      "   ✅ frontend：已获取内存指标数据\n",
      "   ✅ frontend：未检测到异常\n",
      "\n",
      "🔍 正在分析 cart 服务的异常...\n",
      "🔍 查询参数:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  时间范围: 2025-08-28 16:04:30 到 2025-08-28 16:19:30\n",
      "  查询语句: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='cart'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "📊 查询响应:\n",
      "  状态码: 200\n",
      "  返回header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  返回data行数: 1\n",
      "\n",
      "   ✅ cart：已获取CPU指标数据\n",
      "🔍 查询参数:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  时间范围: 2025-08-28 16:04:30 到 2025-08-28 16:19:30\n",
      "  查询语句: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='cart'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "📊 查询响应:\n",
      "  状态码: 200\n",
      "  返回header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  返回data行数: 1\n",
      "\n",
      "   ✅ cart：已获取内存指标数据\n",
      "   ✅ cart：未检测到异常\n",
      "\n",
      "🔍 正在分析 checkout 服务的异常...\n",
      "🔍 查询参数:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  时间范围: 2025-08-28 16:04:30 到 2025-08-28 16:19:30\n",
      "  查询语句: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='checkout'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "📊 查询响应:\n",
      "  状态码: 200\n",
      "  返回header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  返回data行数: 1\n",
      "\n",
      "   ✅ checkout：已获取CPU指标数据\n",
      "🔍 查询参数:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  时间范围: 2025-08-28 16:04:30 到 2025-08-28 16:19:30\n",
      "  查询语句: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='checkout'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "📊 查询响应:\n",
      "  状态码: 200\n",
      "  返回header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  返回data行数: 1\n",
      "\n",
      "   ✅ checkout：已获取内存指标数据\n",
      "   ✅ checkout：未检测到异常\n",
      "\n",
      "🔍 正在分析 payment 服务的异常...\n",
      "🔍 查询参数:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  时间范围: 2025-08-28 16:04:30 到 2025-08-28 16:19:30\n",
      "  查询语句: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='payment'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "📊 查询响应:\n",
      "  状态码: 200\n",
      "  返回header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  返回data行数: 1\n",
      "\n",
      "   ✅ payment：已获取CPU指标数据\n",
      "🔍 查询参数:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  时间范围: 2025-08-28 16:04:30 到 2025-08-28 16:19:30\n",
      "  查询语句: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='payment'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "📊 查询响应:\n",
      "  状态码: 200\n",
      "  返回header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  返回data行数: 1\n",
      "\n",
      "   ✅ payment：已获取内存指标数据\n",
      "   ✅ payment：未检测到异常\n",
      "\n",
      "🔍 正在分析 shipping 服务的异常...\n",
      "🔍 查询参数:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  时间范围: 2025-08-28 16:04:30 到 2025-08-28 16:19:30\n",
      "  查询语句: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='shipping'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "📊 查询响应:\n",
      "  状态码: 200\n",
      "  返回header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  返回data行数: 1\n",
      "\n",
      "   ✅ shipping：已获取CPU指标数据\n",
      "🔍 查询参数:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  时间范围: 2025-08-28 16:04:30 到 2025-08-28 16:19:30\n",
      "  查询语句: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='shipping'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "📊 查询响应:\n",
      "  状态码: 200\n",
      "  返回header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  返回data行数: 1\n",
      "\n",
      "   ✅ shipping：已获取内存指标数据\n",
      "   ✅ shipping：未检测到异常\n",
      "\n",
      "🔍 正在分析 currency 服务的异常...\n",
      "🔍 查询参数:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  时间范围: 2025-08-28 16:04:30 到 2025-08-28 16:19:30\n",
      "  查询语句: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='currency'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "📊 查询响应:\n",
      "  状态码: 200\n",
      "  返回header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  返回data行数: 1\n",
      "\n",
      "   ✅ currency：已获取CPU指标数据\n",
      "🔍 查询参数:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  时间范围: 2025-08-28 16:04:30 到 2025-08-28 16:19:30\n",
      "  查询语句: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='currency'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "📊 查询响应:\n",
      "  状态码: 200\n",
      "  返回header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  返回data行数: 1\n",
      "\n",
      "   ✅ currency：已获取内存指标数据\n",
      "   ✅ currency：未检测到异常\n",
      "\n",
      "🔍 正在分析 ad 服务的异常...\n",
      "🔍 查询参数:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  时间范围: 2025-08-28 16:04:30 到 2025-08-28 16:19:30\n",
      "  查询语句: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='ad'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "📊 查询响应:\n",
      "  状态码: 200\n",
      "  返回header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  返回data行数: 1\n",
      "\n",
      "   ✅ ad：已获取CPU指标数据\n",
      "🔍 查询参数:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  时间范围: 2025-08-28 16:04:30 到 2025-08-28 16:19:30\n",
      "  查询语句: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='ad'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "📊 查询响应:\n",
      "  状态码: 200\n",
      "  返回header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  返回data行数: 1\n",
      "\n",
      "   ✅ ad：已获取内存指标数据\n",
      "   ✅ ad：未检测到异常\n",
      "\n",
      "🔍 正在分析 recommendation 服务的异常...\n",
      "🔍 查询参数:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  时间范围: 2025-08-28 16:04:30 到 2025-08-28 16:19:30\n",
      "  查询语句: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='recommendation'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "📊 查询响应:\n",
      "  状态码: 200\n",
      "  返回header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  返回data行数: 1\n",
      "\n",
      "   ✅ recommendation：已获取CPU指标数据\n",
      "🔍 查询参数:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  时间范围: 2025-08-28 16:04:30 到 2025-08-28 16:19:30\n",
      "  查询语句: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='recommendation'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "📊 查询响应:\n",
      "  状态码: 200\n",
      "  返回header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  返回data行数: 1\n",
      "\n",
      "   ✅ recommendation：已获取内存指标数据\n",
      "   ✅ recommendation：未检测到异常\n",
      "\n",
      "============================================================\n",
      "🎯 其他服务异常汇总：\n",
      "✅ 其他服务未检测到异常\n",
      "💡 主要异常似乎只影响 ad 服务\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Additional：其他service的高级异常检测\n",
    "print(\"🔍 附加分析：其他service的CPU和内存异常检测\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 需要检测的其他常见服务列表\n",
    "OTHER_SERVICES = [\"frontend\", \"cart\", \"checkout\", \"payment\", \"shipping\", \"currency\", \"ad\", \"recommendation\"]\n",
    "other_services_anomalies = {}\n",
    "\n",
    "for service in OTHER_SERVICES:\n",
    "    try:\n",
    "        print(f\"\\n🔍 正在分析 {service} 服务的异常...\")\n",
    "\n",
    "        # CPU异常检测\n",
    "        service_cpu_anomaly_query = f\"\"\"\n",
    ".entity_set with(domain='k8s', name='k8s.deployment', query=`name='{service}'` ) \n",
    "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_total', 'range', '1m')\n",
    "| extend ret = series_decompose_anomalies(__value__, '{{\"confidence\": 0.035}}')\n",
    "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
    "\"\"\"\n",
    "        \n",
    "        cpu_result = cms_tester._execute_spl_query(\n",
    "            service_cpu_anomaly_query.strip(),\n",
    "            from_time=from_time,\n",
    "            to_time=to_time\n",
    "        )\n",
    "        \n",
    "        cpu_anomaly_found = False\n",
    "        cpu_anomaly_details = []\n",
    "        cpu_anomaly_count = 0\n",
    "        \n",
    "        if cpu_result and cpu_result.data:\n",
    "            print(f\"   ✅ {service}：已获取CPU指标数据\")\n",
    "            \n",
    "            # 在CPU数据中统计异常点数 - 至少3个才算异常\n",
    "            for record in cpu_result.data:\n",
    "                if isinstance(record, (list, tuple)):\n",
    "                    for item in record:\n",
    "                        if isinstance(item, str):\n",
    "                            exceed_upper_count = item.count('ExceedUpperBound')\n",
    "                            exceed_lower_count = item.count('ExceedLowerBound')\n",
    "                            cpu_anomaly_count += exceed_upper_count + exceed_lower_count\n",
    "            \n",
    "            # 只有在异常点数达到3及以上时才认为有异常\n",
    "            if cpu_anomaly_count >= 3:\n",
    "                cpu_anomaly_found = True\n",
    "                cpu_anomaly_details.append(f'CPU异常点：{cpu_anomaly_count} 个')\n",
    "        \n",
    "        # 内存异常检测\n",
    "        service_memory_anomaly_query = f\"\"\"\n",
    ".entity_set with(domain='k8s', name='k8s.deployment', query=`name='{service}'` ) \n",
    "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_total', 'range', '1m')\n",
    "| extend ret = series_decompose_anomalies(__value__, '{{\"confidence\": 0.035}}')\n",
    "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
    "\"\"\"\n",
    "        \n",
    "        memory_result = cms_tester._execute_spl_query(\n",
    "            service_memory_anomaly_query.strip(),\n",
    "            from_time=from_time,\n",
    "            to_time=to_time\n",
    "        )\n",
    "        \n",
    "        memory_anomaly_found = False\n",
    "        memory_anomaly_details = []\n",
    "        memory_anomaly_count = 0\n",
    "        \n",
    "        if memory_result and memory_result.data:\n",
    "            print(f\"   ✅ {service}：已获取内存指标数据\")\n",
    "            \n",
    "            # 在内存数据中统计异常点数 - 至少3个才算异常\n",
    "            for record in memory_result.data:\n",
    "                if isinstance(record, (list, tuple)):\n",
    "                    for item in record:\n",
    "                        if isinstance(item, str):\n",
    "                            exceed_upper_count = item.count('ExceedUpperBound')\n",
    "                            exceed_lower_count = item.count('ExceedLowerBound')\n",
    "                            memory_anomaly_count += exceed_upper_count + exceed_lower_count\n",
    "            \n",
    "            # 只有在异常点数达到3及以上时才认为有异常\n",
    "            if memory_anomaly_count >= 3:\n",
    "                memory_anomaly_found = True\n",
    "                memory_anomaly_details.append(f'内存异常点：{memory_anomaly_count} 个')\n",
    "        \n",
    "        # 汇总当前服务的异常分析结果\n",
    "        service_anomalies = cpu_anomaly_details + memory_anomaly_details\n",
    "        \n",
    "        if service_anomalies:\n",
    "            print(f\"   🚨 {service}：检测到异常\")\n",
    "            for anomaly in service_anomalies:\n",
    "                print(f\"      📊 {anomaly}\")\n",
    "            other_services_anomalies[service] = service_anomalies\n",
    "        else:\n",
    "            if cpu_result and cpu_result.data or memory_result and memory_result.data:\n",
    "                print(f\"   ✅ {service}：未检测到异常\")\n",
    "            else:\n",
    "                print(f\"   ❌ {service}：无指标数据可用\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ {service}：出错 - {e}\")\n",
    "        \n",
    "    # 增加小延迟以避免API被请求过快\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# 其他服务异常检测结果总结\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"🎯 其他服务异常汇总：\")\n",
    "\n",
    "if other_services_anomalies:\n",
    "    print(f\"📊 检测到异常的服务：\")\n",
    "    for service, anomalies in other_services_anomalies.items():\n",
    "        print(f\"  🚨 {service}：\")\n",
    "        for anomaly in anomalies:\n",
    "            print(f\"    - {anomaly}\")\n",
    "    \n",
    "    print(f\"\\n💡 建议进一步调查：\")\n",
    "    for service in other_services_anomalies.keys():\n",
    "        print(f\"   - 检查 {service} 部署的资源约束情况\")\n",
    "        print(f\"   - 检查异常期间 {service} 的日志\")\n",
    "        \n",
    "else:\n",
    "    print(f\"✅ 其他服务未检测到异常\")\n",
    "    print(f\"💡 主要异常似乎只影响 {TARGET_SERVICE} 服务\")\n",
    "\n",
    "print(f\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用说明\n",
    "\n",
    "1. **配置参数** - 在第二个单元格中根据您的具体故障事件配置参数\n",
    "2. **按顺序运行所有单元格** - 分析步骤相互依赖，需按顺序执行\n",
    "3. **检查模式分析查询** - 在步骤2中查看模式分析查询，如需要可在SLS控制台手动执行\n",
    "4. **调整目标服务** - 在步骤3中根据模式分析结果调整 `TARGET_SERVICE`\n",
    "5. **查看最终总结** - 在步骤5中查看根因候选的最终总结\n",
    "\n",
    "### 预期工作流结果\n",
    "\n",
    "- **步骤1**: 识别具有异常高独占时间的span\n",
    "- **步骤2**: 揭示哪个服务（如 `recommendation`）存在最多问题span\n",
    "- **步骤3**: 显示已识别服务的CPU使用率指标\n",
    "- **步骤4**: 确认CPU使用率是否显示异常模式\n",
    "- **步骤5**: 得出根因结论（如 `recommendation.cpu`）\n",
    "\n",
    "### 故障排除\n",
    "\n",
    "- **未找到高独占时间span**: 调整 `DURATION_THRESHOLD` 或时间范围\n",
    "- **CMS查询失败**: 验证环境变量和工作空间访问权限\n",
    "- **模式分析显示没有明确的服务**: 检查是否多个服务受影响\n",
    "- **未检测到异常**: 问题可能与CPU无关；检查内存、网络或外部依赖"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
