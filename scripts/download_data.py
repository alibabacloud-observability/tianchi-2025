#!/usr/bin/env python
"""
æ•°æ®ä¸‹è½½è„šæœ¬
ç”¨äºæ‰¹é‡ä¸‹è½½Aæ¦œæ‰€æœ‰é—®é¢˜çš„è§‚æµ‹æ•°æ®åˆ°æœ¬åœ°ï¼Œæ”¯æŒæœ¬åœ°æ¨¡å¼åˆ†æ
"""

import asyncio
import json
import os
import sys
import time
import argparse
from datetime import datetime
from typing import List, Dict, Any, Optional
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from src.agents.log_agent import MinimalLogAgent
from src.agents.metric_agent import MinimalMetricAgent
from src.agents.trace_agent import MinimalTraceAgent
from src.utils.evidence_chain import EvidenceChain

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

class DataDownloader:
    """è§‚æµ‹æ•°æ®ä¸‹è½½å™¨"""
    
    def __init__(self, force_refresh: bool = False, debug: bool = False):
        self.force_refresh = force_refresh
        self.debug = debug
        self.logger = logging.getLogger(__name__)
        
        # æ•°æ®å­˜å‚¨ç›®å½•
        self.data_dir = Path(project_root) / "data"
        self.data_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–agents
        try:
            self.log_agent = MinimalLogAgent(debug=debug)
            self.metric_agent = MinimalMetricAgent(debug=debug)
            self.trace_agent = MinimalTraceAgent(debug=debug)
            self.logger.info("âœ… ä¸‰ä¸ªagentsåˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            self.logger.error(f"âŒ Agentsåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _parse_time_range(self, time_range: str) -> tuple:
        """è§£ææ—¶é—´èŒƒå›´å­—ç¬¦ä¸²"""
        try:
            start_str, end_str = time_range.split(' ~ ')
            start_time = datetime.strptime(start_str, '%Y-%m-%d %H:%M:%S')
            end_time = datetime.strptime(end_str, '%Y-%m-%d %H:%M:%S')
            return start_time, end_time
        except Exception as e:
            self.logger.error(f"âŒ æ—¶é—´èŒƒå›´è§£æå¤±è´¥: {e}")
            raise ValueError(f"Invalid time range format: {time_range}")
    
    def _calculate_baseline_period(self, failure_start: datetime) -> tuple:
        """è®¡ç®—åŸºçº¿æœŸæ—¶é—´èŒƒå›´ï¼ˆä¸parallel_data_coordinatorä¿æŒä¸€è‡´ï¼‰"""
        from datetime import timedelta
        
        # é…ç½®å‚æ•°ï¼ˆä¸aiops_engineä¿æŒä¸€è‡´ï¼‰
        baseline_hours_before = 0.15  # 9åˆ†é’Ÿ
        baseline_buffer_minutes = 1   # 1åˆ†é’Ÿç¼“å†²ï¼ˆç”¨æˆ·ä¿®æ”¹åçš„å€¼ï¼‰
        
        # æ•…éšœå‰1åˆ†é’Ÿä½œä¸ºåŸºçº¿ç»“æŸæ—¶é—´
        baseline_end = failure_start - timedelta(minutes=baseline_buffer_minutes)
        
        # ä»åŸºçº¿ç»“æŸæ—¶é—´å¾€å‰æ¨9åˆ†é’Ÿä½œä¸ºåŸºçº¿çª—å£
        baseline_start = baseline_end - timedelta(hours=baseline_hours_before)
        
        return baseline_start, baseline_end
    
    def _check_existing_data(self, problem_id: str) -> Dict[str, bool]:
        """æ£€æŸ¥é—®é¢˜çš„æœ¬åœ°æ•°æ®æ˜¯å¦å·²å­˜åœ¨"""
        problem_dir = self.data_dir / f"problem_{problem_id}"
        
        required_files = [
            'failure_logs.json',
            'failure_metrics.json', 
            'failure_traces.json',
            'baseline_logs.json',
            'baseline_metrics.json'
        ]
        
        existence_status = {}
        for file_name in required_files:
            file_path = problem_dir / file_name
            existence_status[file_name] = file_path.exists()
        
        return existence_status
    
    async def _download_single_problem_data(self, problem_data: Dict[str, Any]) -> bool:
        """ä¸‹è½½å•ä¸ªé—®é¢˜çš„æ‰€æœ‰è§‚æµ‹æ•°æ®"""
        
        problem_id = problem_data['problem_id']
        time_range = problem_data['time_range']
        
        self.logger.info(f"\nğŸ¯ å¼€å§‹ä¸‹è½½é—®é¢˜ {problem_id} çš„è§‚æµ‹æ•°æ®")
        self.logger.info(f"   æ—¶é—´èŒƒå›´: {time_range}")
        
        # åˆ›å»ºé—®é¢˜ç›®å½•
        problem_dir = self.data_dir / f"problem_{problem_id}"
        problem_dir.mkdir(exist_ok=True)
        
        # æ£€æŸ¥ç°æœ‰æ•°æ®
        existing_data = self._check_existing_data(problem_id)
        if not self.force_refresh and all(existing_data.values()):
            self.logger.info(f"   âœ… é—®é¢˜ {problem_id} çš„æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
            return True
        
        try:
            # è§£ææ—¶é—´èŒƒå›´
            start_time, end_time = self._parse_time_range(time_range)
            baseline_start, baseline_end = self._calculate_baseline_period(start_time)
            
            self.logger.info(f"   ğŸ“… æ•…éšœæ—¶é—´: {start_time} ~ {end_time}")
            self.logger.info(f"   ğŸ“… åŸºçº¿æ—¶é—´: {baseline_start} ~ {baseline_end}")
            
            # ä¸‹è½½ä»»åŠ¡å®šä¹‰
            download_tasks = [
                ('failure_logs', 'log', start_time, end_time),
                ('failure_metrics', 'metric', start_time, end_time), 
                ('failure_traces', 'trace', start_time, end_time),
                ('baseline_logs', 'log', baseline_start, baseline_end),
                ('baseline_metrics', 'metric', baseline_start, baseline_end),
            ]
            
            success_count = 0
            total_tasks = len(download_tasks)
            
            # ä¸²è¡Œä¸‹è½½å„ç±»æ•°æ®ï¼ˆé¿å…æœåŠ¡å‹åŠ›è¿‡å¤§ï¼‰
            for task_name, agent_type, task_start, task_end in download_tasks:
                file_path = problem_dir / f"{task_name}.json"
                
                # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ä¸”ä¸å¼ºåˆ¶åˆ·æ–°ï¼Œè·³è¿‡
                if not self.force_refresh and file_path.exists():
                    self.logger.info(f"   âœ… {task_name} å·²å­˜åœ¨ï¼Œè·³è¿‡")
                    success_count += 1
                    continue
                
                self.logger.info(f"   ğŸ”„ ä¸‹è½½ {task_name}...")
                
                try:
                    # æ ¹æ®agentç±»å‹ä¸‹è½½æ•°æ®
                    if agent_type == 'log':
                        data = await self._download_log_data(task_start, task_end)
                    elif agent_type == 'metric':
                        data = await self._download_metric_data(task_start, task_end)
                    elif agent_type == 'trace':
                        data = await self._download_trace_data(task_start, task_end)
                    else:
                        self.logger.error(f"   âŒ æœªçŸ¥çš„agentç±»å‹: {agent_type}")
                        continue
                    
                    # ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶
                    with open(file_path, 'w', encoding='utf-8') as f:
                        json.dump(data, f, ensure_ascii=False, indent=2, default=str)
                    
                    data_size = len(data) if isinstance(data, list) else len(str(data))
                    self.logger.info(f"   âœ… {task_name} ä¸‹è½½å®Œæˆ ({data_size} é¡¹)")
                    success_count += 1
                    
                    # æ·»åŠ å»¶è¿Ÿé¿å…æœåŠ¡å‹åŠ›
                    await asyncio.sleep(0.5)
                    
                except Exception as e:
                    self.logger.error(f"   âŒ {task_name} ä¸‹è½½å¤±è´¥: {e}")
                    # åˆ›å»ºç©ºæ–‡ä»¶æ ‡è®°å°è¯•è¿‡
                    with open(file_path, 'w', encoding='utf-8') as f:
                        json.dump({"error": str(e), "timestamp": datetime.now().isoformat()}, f)
            
            # åˆ›å»ºå…ƒæ•°æ®æ–‡ä»¶
            metadata = {
                "problem_id": problem_id,
                "time_range": time_range,
                "download_timestamp": datetime.now().isoformat(),
                "success_count": success_count,
                "total_tasks": total_tasks,
                "baseline_time_range": f"{baseline_start} ~ {baseline_end}",
                "force_refresh": self.force_refresh
            }
            
            metadata_path = problem_dir / "metadata.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            success_rate = success_count / total_tasks
            if success_rate >= 0.6:  # 60%ä»¥ä¸Šä»»åŠ¡æˆåŠŸè®¤ä¸ºé—®é¢˜æ•°æ®ä¸‹è½½æˆåŠŸ
                self.logger.info(f"   ğŸ‰ é—®é¢˜ {problem_id} ä¸‹è½½å®Œæˆ ({success_count}/{total_tasks} æˆåŠŸ)")
                return True
            else:
                self.logger.warning(f"   âš ï¸ é—®é¢˜ {problem_id} ä¸‹è½½éƒ¨åˆ†å¤±è´¥ ({success_count}/{total_tasks} æˆåŠŸ)")
                return False
                
        except Exception as e:
            self.logger.error(f"   âŒ é—®é¢˜ {problem_id} ä¸‹è½½å¤±è´¥: {e}")
            return False
    
    async def _download_log_data(self, start_time: datetime, end_time: datetime) -> List[Dict[str, Any]]:
        """ä¸‹è½½æ—¥å¿—æ•°æ®"""
        evidence_chain = EvidenceChain(start_time, end_time)
        result = self.log_agent.analyze(evidence_chain)
        
        # ä»evidence_chainä¸­æå–åŸå§‹æ—¥å¿—æ•°æ®
        log_data = []
        for evidence in evidence_chain.evidence:
            if evidence.evidence_type == 'log' and isinstance(evidence.data, list):
                log_data.extend(evidence.data)
        
        return log_data
    
    async def _download_metric_data(self, start_time: datetime, end_time: datetime) -> Dict[str, Any]:
        """ä¸‹è½½æŒ‡æ ‡æ•°æ®"""
        evidence_chain = EvidenceChain(start_time, end_time)
        result = self.metric_agent.analyze(evidence_chain)
        
        # ä»evidence_chainä¸­æå–åŸå§‹æŒ‡æ ‡æ•°æ®
        metric_data = {
            "k8s_metrics": {},
            "apm_metrics": {},
            "analysis_result": result
        }
        
        # ğŸ”§ ä¿®å¤ï¼šMetricAgentå°†æ‰€æœ‰æ•°æ®å­˜å‚¨åœ¨å•ä¸ªevidenceä¸­ï¼Œç»“æ„ä¸ºall_metrics
        for evidence in evidence_chain.evidence:
            if evidence.evidence_type == 'metric':
                if isinstance(evidence.data, dict):
                    # evidence.dataçš„ç»“æ„: {'k8s_golden_metrics': {...}, 'apm_service_metrics': {...}}
                    k8s_data = evidence.data.get('k8s_golden_metrics', {})
                    apm_data = evidence.data.get('apm_service_metrics', {})
                    
                    # æ£€æŸ¥K8sæ•°æ®ç»“æ„ï¼šæ–°ç»“æ„æ˜¯ {pod_name: {metric_name: {...}}}
                    if k8s_data and isinstance(list(k8s_data.values())[0], dict):
                        # æ–°çš„æŒ‰Podç»„ç»‡çš„ç»“æ„
                        metric_data["k8s_metrics"] = k8s_data
                        
                        # ç»Ÿè®¡ä¿¡æ¯
                        total_pods = len(k8s_data)
                        total_metrics = sum(len(pod_data) for pod_data in k8s_data.values())
                        self.logger.info(f"âœ… æå–åˆ° K8sæŒ‡æ ‡: {total_pods}ä¸ªPod, {total_metrics}ä¸ªæŒ‡æ ‡ç±»å‹")
                        
                    else:
                        # æ—§çš„æ··åˆç»“æ„ï¼ˆå‘åå…¼å®¹ï¼‰
                        metric_data["k8s_metrics"].update(k8s_data)
                        self.logger.info(f"âœ… æå–åˆ° K8sæŒ‡æ ‡: {len(k8s_data)} ä¸ªï¼ˆæ—§æ ¼å¼ï¼‰")
                    
                    self.logger.info(f"ğŸ¢ æ¶‰åŠæœåŠ¡: {sorted(apm_data.keys())}")                    
                    # APMæ•°æ®ç»“æ„ä¿æŒä¸å˜
                    metric_data["apm_metrics"].update(apm_data)
                    break  # MetricAgentåªæ·»åŠ ä¸€ä¸ªmetric evidence
        
        return metric_data
    
    async def _download_trace_data(self, start_time: datetime, end_time: datetime) -> List[Dict[str, Any]]:
        """ä¸‹è½½é“¾è·¯æ•°æ®"""
        evidence_chain = EvidenceChain(start_time, end_time)
        result = self.trace_agent.analyze(evidence_chain)
        
        # ä»evidence_chainä¸­æå–åŸå§‹é“¾è·¯æ•°æ®
        trace_data = []
        for evidence in evidence_chain.evidence:
            if evidence.evidence_type == 'trace' and isinstance(evidence.data, list):
                trace_data.extend(evidence.data)
        
        return trace_data

    async def download_all_problems(self, problems_file: str) -> None:
        """ä¸‹è½½æ‰€æœ‰é—®é¢˜çš„è§‚æµ‹æ•°æ®"""
        
        self.logger.info("ğŸš€ å¼€å§‹æ‰¹é‡ä¸‹è½½Aæ¦œè§‚æµ‹æ•°æ®")
        self.logger.info(f"   æ•°æ®å­˜å‚¨ç›®å½•: {self.data_dir}")
        self.logger.info(f"   å¼ºåˆ¶åˆ·æ–°: {self.force_refresh}")
        
        # è¯»å–é—®é¢˜åˆ—è¡¨
        try:
            with open(problems_file, 'r', encoding='utf-8') as f:
                problems = []
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if line:
                        try:
                            problems.append(json.loads(line))
                        except json.JSONDecodeError as e:
                            self.logger.error(f"âŒ ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
                            
            self.logger.info(f"   ğŸ“‹ åŠ è½½äº† {len(problems)} ä¸ªé—®é¢˜")
            
        except Exception as e:
            self.logger.error(f"âŒ è¯»å–é—®é¢˜æ–‡ä»¶å¤±è´¥: {e}")
            return
        
        # å¼€å§‹ä¸‹è½½
        start_time = time.time()
        successful_downloads = 0
        failed_problems = []
        
        for i, problem in enumerate(problems, 1):
            problem_id = problem.get('problem_id', f'unknown_{i}')
            
            self.logger.info(f"\nğŸ“ å¤„ç†é—®é¢˜ {i}/{len(problems)}: {problem_id}")
            
            try:
                success = await self._download_single_problem_data(problem)
                if success:
                    successful_downloads += 1
                else:
                    failed_problems.append(problem_id)
                    
            except Exception as e:
                self.logger.error(f"âŒ é—®é¢˜ {problem_id} ä¸‹è½½å¼‚å¸¸: {e}")
                failed_problems.append(problem_id)
        
        # ä¸‹è½½æ€»ç»“
        total_time = time.time() - start_time
        success_rate = successful_downloads / len(problems) if problems else 0
        
        self.logger.info(f"\nğŸ‰ æ‰¹é‡ä¸‹è½½å®Œæˆï¼")
        self.logger.info(f"   âœ… æˆåŠŸä¸‹è½½: {successful_downloads}/{len(problems)} ({success_rate:.1%})")
        self.logger.info(f"   â±ï¸  æ€»è€—æ—¶: {total_time:.1f}ç§’")
        self.logger.info(f"   ğŸ’¾ æ•°æ®å­˜å‚¨åœ¨: {self.data_dir}")
        
        if failed_problems:
            self.logger.warning(f"   âš ï¸ å¤±è´¥çš„é—®é¢˜: {failed_problems}")
        
        # åˆ›å»ºä¸‹è½½æŠ¥å‘Š
        report = {
            "download_timestamp": datetime.now().isoformat(),
            "total_problems": len(problems),
            "successful_downloads": successful_downloads,
            "failed_problems": failed_problems,
            "success_rate": success_rate,
            "total_time_seconds": total_time,
            "force_refresh": self.force_refresh,
            "data_directory": str(self.data_dir)
        }
        
        report_path = self.data_dir / f"download_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"   ğŸ“Š ä¸‹è½½æŠ¥å‘Š: {report_path}")

    async def download_single_problem(self, problems_file: str, problem_id: str) -> None:
        """ä¸‹è½½å•ä¸ªé—®é¢˜çš„è§‚æµ‹æ•°æ®"""
        
        self.logger.info(f"ğŸ¯ å¼€å§‹ä¸‹è½½å•ä¸ªé—®é¢˜: {problem_id}")
        
        # è¯»å–é—®é¢˜åˆ—è¡¨
        try:
            with open(problems_file, 'r', encoding='utf-8') as f:
                problems = []
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if line:
                        try:
                            problem_data = json.loads(line)
                            if problem_data.get('problem_id') == problem_id:
                                problems.append(problem_data)
                                break
                        except json.JSONDecodeError as e:
                            self.logger.error(f"âŒ ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
                            
            if not problems:
                self.logger.error(f"âŒ æœªæ‰¾åˆ°é—®é¢˜ID: {problem_id}")
                return
                
            self.logger.info(f"   âœ… æ‰¾åˆ°é—®é¢˜: {problem_id}")
            
        except Exception as e:
            self.logger.error(f"âŒ è¯»å–é—®é¢˜æ–‡ä»¶å¤±è´¥: {e}")
            return
        
        # ä¸‹è½½å•ä¸ªé—®é¢˜
        start_time = time.time()
        problem = problems[0]
        
        try:
            success = await self._download_single_problem_data(problem)
            total_time = time.time() - start_time
            
            if success:
                self.logger.info(f"ğŸ‰ é—®é¢˜ {problem_id} ä¸‹è½½å®Œæˆï¼")
                self.logger.info(f"   â±ï¸  è€—æ—¶: {total_time:.1f}ç§’")
                self.logger.info(f"   ğŸ’¾ æ•°æ®å­˜å‚¨åœ¨: {self.data_dir}/problem_{problem_id}")
            else:
                self.logger.error(f"âŒ é—®é¢˜ {problem_id} ä¸‹è½½å¤±è´¥")
                
        except Exception as e:
            self.logger.error(f"âŒ é—®é¢˜ {problem_id} ä¸‹è½½å¼‚å¸¸: {e}")

    async def list_available_problems(self, problems_file: str) -> None:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„é—®é¢˜ID"""
        
        self.logger.info("ğŸ“‹ åˆ—å‡ºæ‰€æœ‰å¯ç”¨é—®é¢˜:")
        
        try:
            with open(problems_file, 'r', encoding='utf-8') as f:
                problems = []
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if line:
                        try:
                            problem_data = json.loads(line)
                            problems.append(problem_data)
                        except json.JSONDecodeError as e:
                            self.logger.error(f"âŒ ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
                            
            self.logger.info(f"   ğŸ“Š æ€»å…± {len(problems)} ä¸ªé—®é¢˜")
            
            for i, problem in enumerate(problems, 1):
                problem_id = problem.get('problem_id', f'unknown_{i}')
                time_range = problem.get('time_range', 'Unknown')
                
                # æ£€æŸ¥æœ¬åœ°æ•°æ®çŠ¶æ€
                existing_data = self._check_existing_data(problem_id)
                status = "âœ… å·²ä¸‹è½½" if all(existing_data.values()) else "âŒ æœªä¸‹è½½"
                
                print(f"   {i:2d}. {problem_id} - {time_range} [{status}]")
                
        except Exception as e:
            self.logger.error(f"âŒ è¯»å–é—®é¢˜æ–‡ä»¶å¤±è´¥: {e}")

async def main():
    parser = argparse.ArgumentParser(description='ä¸‹è½½Aæ¦œæ‰€æœ‰é—®é¢˜çš„è§‚æµ‹æ•°æ®åˆ°æœ¬åœ°')
    parser.add_argument('--problems-file', default='dataset/Bæ¦œé¢˜ç›®.jsonl',
                       help='é—®é¢˜æ–‡ä»¶è·¯å¾„ (é»˜è®¤: dataset/Bæ¦œé¢˜ç›®.jsonl)')
    parser.add_argument('--force-refresh', action='store_true',
                       help='å¼ºåˆ¶é‡æ–°ä¸‹è½½å·²å­˜åœ¨çš„æ•°æ®')
    parser.add_argument('--debug', action='store_true',
                       help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
    parser.add_argument('--single-problem', type=str,
                       help='åªä¸‹è½½æŒ‡å®šé—®é¢˜IDçš„æ•°æ® (ä¾‹å¦‚: 004)')
    parser.add_argument('--list-problems', action='store_true',
                       help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„é—®é¢˜ID')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.problems_file):
        print(f"âŒ é—®é¢˜æ–‡ä»¶ä¸å­˜åœ¨: {args.problems_file}")
        sys.exit(1)
    
    downloader = DataDownloader(
        force_refresh=args.force_refresh,
        debug=args.debug
    )
    
    # å¤„ç†å•ä¸ªé—®é¢˜ä¸‹è½½
    if args.single_problem:
        await downloader.download_single_problem(args.problems_file, args.single_problem)
    elif args.list_problems:
        await downloader.list_available_problems(args.problems_file)
    else:
        await downloader.download_all_problems(args.problems_file)

if __name__ == "__main__":
    asyncio.run(main())
