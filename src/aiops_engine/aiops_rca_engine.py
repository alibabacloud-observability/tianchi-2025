#!/usr/bin/env python
"""
AIOpsæ ¹å› åˆ†æå¼•æ“ - æ–¹æ¡ˆAæ¸è¿›å¼ä¼˜åŒ–çš„å®Œæ•´é›†æˆ
ç»“åˆå¹¶è¡Œæ•°æ®è·å–ã€å¼‚å¸¸æ£€æµ‹ã€ä¸“å®¶è§„åˆ™çš„æ™ºèƒ½å†³ç­–å¼•æ“
"""

import asyncio
import logging
import time
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple

from .parallel_data_coordinator import ParallelDataCoordinator, DataBundle
from .anomaly_detection import AnomalyDetectionEngine, CorrelationAnalysisEngine, Anomaly, ServiceCorrelation
from .expert_rules import ExpertRulesEngine, RootCauseScore


class AIOpsRCAEngine:
    """AIOpsæ ¹å› åˆ†æå¼•æ“
    
    æ–¹æ¡ˆAçš„å®Œæ•´å®ç°ï¼šæ•°æ®é©±åŠ¨ + ç®—æ³•æ™ºèƒ½ + LLMè¾…åŠ©
    æ”¯æŒç¦»çº¿æ¨¡å¼å’Œåœ¨çº¿æ¨¡å¼
    """
    
    def __init__(self, debug: bool = False, offline_mode: bool = False, problem_id: str = None):
        self.debug = debug
        self.offline_mode = offline_mode
        self.problem_id = problem_id
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        try:
            # æ•°æ®åè°ƒå™¨éœ€è¦ä¼ é€’ç¦»çº¿æ¨¡å¼å‚æ•°
            self.data_coordinator = ParallelDataCoordinator(
                debug=debug, offline_mode=offline_mode, problem_id=problem_id
            )
            
            # å…¶ä»–ç»„ä»¶ä¸éœ€è¦ç¦»çº¿æ¨¡å¼å‚æ•°ï¼ˆå®ƒä»¬å¤„ç†çš„æ˜¯å·²æ”¶é›†çš„æ•°æ®ï¼‰
            self.anomaly_detector = AnomalyDetectionEngine(debug=debug)
            self.correlation_analyzer = CorrelationAnalysisEngine(debug=debug)
            self.expert_rules = ExpertRulesEngine(debug=debug)
            
            mode_desc = f"ç¦»çº¿æ¨¡å¼ (é—®é¢˜ID: {problem_id})" if offline_mode else "åœ¨çº¿æ¨¡å¼"
            self.logger.info(f"âœ… AIOps RCAå¼•æ“åˆå§‹åŒ–å®Œæˆ ({mode_desc})")
            
        except Exception as e:
            self.logger.error(f"âŒ AIOps RCAå¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
        
        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            'total_analyses': 0,
            'successful_analyses': 0,
            'avg_execution_time': 0.0,
            'avg_data_quality_score': 0.0,
            'avg_confidence_score': 0.0
        }
    
    async def analyze_root_cause(self, time_range: str, 
                                candidates: List[str]) -> List[RootCauseScore]:
        """æ‰§è¡Œå®Œæ•´çš„æ ¹å› åˆ†æ
        
        Args:
            time_range: æ—¶é—´èŒƒå›´ï¼Œå¦‚ "2025-08-28 15:08:03 ~ 2025-08-28 15:13:03"
            candidates: å€™é€‰æ ¹å› ï¼Œå¦‚ ["ad.Failure", "ad.LargeGc", "cart.Failure"]
            
        Returns:
            List[RootCauseScore]: æŒ‰ç½®ä¿¡åº¦æ’åºçš„æ ¹å› è¯„åˆ†ç»“æœ
        """
        
        start_time = time.time()
        analysis_id = f"RCA_{int(start_time)}"
        
        self.logger.info(f"ğŸš€ å¼€å§‹AIOpsæ ¹å› åˆ†æ [{analysis_id}]")
        self.logger.info(f"   ğŸ“… æ—¶é—´èŒƒå›´: {time_range}")
        self.logger.info(f"   ğŸ¯ å€™é€‰æ ¹å› : {candidates}")
        
        try:
            # 1. å¹¶è¡Œæ•°æ®æ”¶é›† (A1å±‚)
            self.logger.info("ğŸ”„ Phase 1: å¹¶è¡Œæ•°æ®æ”¶é›†")
            data_bundle = await self.data_coordinator.collect_comprehensive_data(time_range, candidates)
            
            data_summary = self.data_coordinator.get_data_summary(data_bundle)
            self.logger.info(f"   ğŸ“Š æ•°æ®è´¨é‡: {data_bundle.data_quality_score:.2f}/1.0")
            self.logger.info(f"   ğŸ¢ å‘ç°æœåŠ¡: {data_summary['services_discovered']}")
            
            # 2. å¼‚å¸¸æ£€æµ‹åˆ†æ (A2å±‚)
            self.logger.info("ğŸ”„ Phase 2: å¼‚å¸¸æ£€æµ‹åˆ†æ")
            anomalies = self.anomaly_detector.detect_all_anomalies(data_bundle)
            
            # 3. æœåŠ¡å…³è”åˆ†æ
            self.logger.info("ğŸ”„ Phase 3: æœåŠ¡å…³è”åˆ†æ")
            correlations = self.correlation_analyzer.analyze_service_correlations(anomalies, data_bundle)
            
            # 4. ä¸“å®¶è§„åˆ™è¯„åˆ† (A2å±‚)
            self.logger.info("ğŸ”„ Phase 4: ä¸“å®¶è§„åˆ™è¯„åˆ†")
            scored_results = self.expert_rules.evaluate_candidates(
                candidates, anomalies, correlations, data_bundle
            )
            
            # 5. ç»“æœä¼˜åŒ–å’ŒéªŒè¯
            optimized_results = self._optimize_results(scored_results, data_bundle)
            
            # 6. è®°å½•æ€§èƒ½ç»Ÿè®¡
            execution_time = time.time() - start_time
            self._update_performance_stats(execution_time, data_bundle.data_quality_score, optimized_results)
            
            self.logger.info(f"âœ… AIOpsæ ¹å› åˆ†æå®Œæˆ [{analysis_id}]")
            self.logger.info(f"   â±ï¸  æ€»è€—æ—¶: {execution_time:.2f}ç§’")
            self.logger.info(f"   ğŸ¯ æœ€ä½³å€™é€‰: {optimized_results[0].candidate if optimized_results else 'None'}")
            self.logger.info(f"   ğŸ“ˆ ç½®ä¿¡åº¦: {optimized_results[0].confidence:.3f} if optimized_results else 0")
            
            return optimized_results
            
        except Exception as e:
            execution_time = time.time() - start_time
            self.logger.error(f"âŒ AIOpsæ ¹å› åˆ†æå¤±è´¥ [{analysis_id}]: {e}")
            self.logger.error(f"   â±ï¸  æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
            
            # è¿”å›ç©ºç»“æœæˆ–åŸºäºéƒ¨åˆ†æ•°æ®çš„é™çº§ç»“æœ
            return self._create_fallback_results(candidates, str(e))
    
    def _optimize_results(self, results: List[RootCauseScore], 
                         data_bundle: DataBundle) -> List[RootCauseScore]:
        """ä¼˜åŒ–å’ŒéªŒè¯åˆ†æç»“æœ"""
        
        if not results:
            return results
        
        # 1. ç½®ä¿¡åº¦é˜ˆå€¼è¿‡æ»¤
        min_confidence = 0.3
        filtered_results = [r for r in results if r.confidence >= min_confidence]
        
        if not filtered_results:
            # å¦‚æœæ‰€æœ‰ç»“æœç½®ä¿¡åº¦éƒ½å¤ªä½ï¼Œä¿ç•™åŸå§‹ç»“æœä½†æ ‡è®°ä½ç½®ä¿¡åº¦
            self.logger.warning(f"âš ï¸ æ‰€æœ‰å€™é€‰æ ¹å› ç½®ä¿¡åº¦ä½äºé˜ˆå€¼ {min_confidence}")
            return results
        
        # 2. æ•°æ®è´¨é‡è°ƒæ•´
        data_quality_factor = max(data_bundle.data_quality_score, 0.5)  # æœ€ä½0.5çš„è´¨é‡å› å­
        
        for result in filtered_results:
            # æ ¹æ®æ•°æ®è´¨é‡è°ƒæ•´ç½®ä¿¡åº¦
            result.confidence *= data_quality_factor
            result.total_score *= data_quality_factor
        
        # 3. é‡æ–°æ’åº
        filtered_results.sort(key=lambda x: (x.confidence, x.total_score), reverse=True)
        
        # 4. æ·»åŠ å†³ç­–è§£é‡Š
        for i, result in enumerate(filtered_results[:3]):  # å‰3åæ·»åŠ é¢å¤–è§£é‡Š
            result.reasoning_chain.append(f"ç»æ•°æ®è´¨é‡è°ƒæ•´åæ’åç¬¬ {i+1}")
            if result.confidence > 0.8:
                result.reasoning_chain.append("é«˜ç½®ä¿¡åº¦æ¨è")
            elif result.confidence > 0.6:
                result.reasoning_chain.append("ä¸­ç­‰ç½®ä¿¡åº¦ï¼Œå»ºè®®è¿›ä¸€æ­¥éªŒè¯")
            else:
                result.reasoning_chain.append("ä½ç½®ä¿¡åº¦ï¼Œä»…ä¾›å‚è€ƒ")
        
        return filtered_results
    
    def _create_fallback_results(self, candidates: List[str], error_message: str) -> List[RootCauseScore]:
        """åˆ›å»ºé™çº§ç»“æœï¼ˆåˆ†æå¤±è´¥æ—¶ï¼‰"""
        
        fallback_results = []
        
        for candidate in candidates:
            # åŸºäºå€™é€‰åç§°çš„ç®€å•å¯å‘å¼è¯„åˆ†
            base_score = 0.1  # å¾ˆä½çš„åŸºç¡€åˆ†æ•°
            
            # æ ¹æ®å¸¸è§æ•…éšœæ¨¡å¼ç»™äºˆä¸€äº›åˆ†æ•°
            if "Failure" in candidate:
                base_score += 0.2
            if "LargeGc" in candidate:
                base_score += 0.15
            
            result = RootCauseScore(
                candidate=candidate,
                total_score=base_score,
                confidence=0.2,  # å¾ˆä½çš„ç½®ä¿¡åº¦
                anomaly_score=0.0,
                correlation_score=0.0,
                expert_rule_score=0.0,
                temporal_score=0.0,
                business_impact_score=base_score,
                supporting_anomalies=[],
                supporting_correlations=[],
                matched_rules=[],
                evidence_summary=[f"åˆ†æå¤±è´¥ï¼Œé™çº§ç»“æœ: {error_message}"],
                reasoning_chain=[
                    f"AIOpså¼•æ“åˆ†æå¤±è´¥: {error_message}",
                    "ä½¿ç”¨åŸºç¡€å¯å‘å¼è¯„åˆ†",
                    "å»ºè®®æ‰‹åŠ¨æ’æŸ¥æˆ–é‡è¯•åˆ†æ"
                ]
            )
            
            fallback_results.append(result)
        
        # ç®€å•æ’åº
        fallback_results.sort(key=lambda x: x.total_score, reverse=True)
        
        return fallback_results
    
    def _update_performance_stats(self, execution_time: float, 
                                 data_quality_score: float,
                                 results: List[RootCauseScore]):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        
        self.performance_stats['total_analyses'] += 1
        
        if results and results[0].confidence > 0.5:
            self.performance_stats['successful_analyses'] += 1
        
        # æ»‘åŠ¨å¹³å‡æ›´æ–°
        alpha = 0.1  # å¹³æ»‘å› å­
        self.performance_stats['avg_execution_time'] = (
            alpha * execution_time + (1 - alpha) * self.performance_stats['avg_execution_time']
        )
        
        self.performance_stats['avg_data_quality_score'] = (
            alpha * data_quality_score + (1 - alpha) * self.performance_stats['avg_data_quality_score']
        )
        
        if results:
            avg_confidence = sum(r.confidence for r in results) / len(results)
            self.performance_stats['avg_confidence_score'] = (
                alpha * avg_confidence + (1 - alpha) * self.performance_stats['avg_confidence_score']
            )
    
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        
        stats = self.performance_stats.copy()
        
        if stats['total_analyses'] > 0:
            stats['success_rate'] = stats['successful_analyses'] / stats['total_analyses']
        else:
            stats['success_rate'] = 0.0
        
        # è®¡ç®—æ€§èƒ½ç­‰çº§
        performance_grade = self._calculate_performance_grade(stats)
        stats['performance_grade'] = performance_grade
        
        # æ·»åŠ æ”¹è¿›å»ºè®®
        stats['improvement_suggestions'] = self._generate_improvement_suggestions(stats)
        
        return stats
    
    def _calculate_performance_grade(self, stats: Dict[str, Any]) -> str:
        """è®¡ç®—æ€§èƒ½ç­‰çº§"""
        
        # ç»¼åˆè¯„åˆ†
        score = 0
        
        # æˆåŠŸç‡è¯„åˆ† (40%)
        success_rate = stats.get('success_rate', 0)
        score += success_rate * 40
        
        # æ‰§è¡Œæ—¶é—´è¯„åˆ† (30%)
        avg_time = stats.get('avg_execution_time', 120)  # é»˜è®¤120ç§’
        if avg_time <= 30:
            score += 30
        elif avg_time <= 60:
            score += 25
        elif avg_time <= 90:
            score += 20
        else:
            score += 10
        
        # æ•°æ®è´¨é‡è¯„åˆ† (20%)
        data_quality = stats.get('avg_data_quality_score', 0.5)
        score += data_quality * 20
        
        # ç½®ä¿¡åº¦è¯„åˆ† (10%)
        confidence = stats.get('avg_confidence_score', 0.5)
        score += confidence * 10
        
        # ç­‰çº§åˆ’åˆ†
        if score >= 90:
            return "A+"
        elif score >= 80:
            return "A"
        elif score >= 70:
            return "B+"
        elif score >= 60:
            return "B"
        elif score >= 50:
            return "C"
        else:
            return "D"
    
    def _generate_improvement_suggestions(self, stats: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        
        suggestions = []
        
        success_rate = stats.get('success_rate', 0)
        avg_time = stats.get('avg_execution_time', 0)
        data_quality = stats.get('avg_data_quality_score', 0)
        confidence = stats.get('avg_confidence_score', 0)
        
        if success_rate < 0.8:
            suggestions.append("æé«˜åˆ†ææˆåŠŸç‡ï¼šæ£€æŸ¥æ•°æ®æºè¿æ¥ç¨³å®šæ€§")
        
        if avg_time > 60:
            suggestions.append("ä¼˜åŒ–æ‰§è¡Œæ—¶é—´ï¼šè€ƒè™‘å¢åŠ å¹¶è¡Œåº¦æˆ–ç¼“å­˜æœºåˆ¶")
        
        if data_quality < 0.7:
            suggestions.append("æ”¹å–„æ•°æ®è´¨é‡ï¼šå®Œå–„æ•°æ®æºé…ç½®å’Œæ¸…æ´—é€»è¾‘")
        
        if confidence < 0.6:
            suggestions.append("æå‡ç½®ä¿¡åº¦ï¼šä¸°å¯Œä¸“å®¶è§„åˆ™åº“å’Œå¼‚å¸¸æ£€æµ‹ç®—æ³•")
        
        if not suggestions:
            suggestions.append("æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œç»§ç»­ä¿æŒ")
        
        return suggestions
    
    async def batch_analyze(self, analysis_requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰¹é‡åˆ†æå¤šä¸ªé—®é¢˜"""
        
        self.logger.info(f"ğŸ”„ å¼€å§‹æ‰¹é‡åˆ†æ {len(analysis_requests)} ä¸ªé—®é¢˜")
        
        # åˆ›å»ºå¹¶è¡Œä»»åŠ¡
        tasks = []
        for i, request in enumerate(analysis_requests):
            task = asyncio.create_task(
                self._single_batch_analysis(i, request)
            )
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ•´ç†ç»“æœ
        batch_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                batch_results.append({
                    'problem_id': analysis_requests[i].get('problem_id', f'problem_{i}'),
                    'status': 'failed',
                    'error': str(result),
                    'results': []
                })
            else:
                batch_results.append(result)
        
        self.logger.info(f"âœ… æ‰¹é‡åˆ†æå®Œæˆ")
        
        return batch_results
    
    async def _single_batch_analysis(self, index: int, request: Dict[str, Any]) -> Dict[str, Any]:
        """å•ä¸ªæ‰¹é‡åˆ†æä»»åŠ¡"""
        
        try:
            time_range = request['time_range']
            candidates = request['candidates']
            problem_id = request.get('problem_id', f'problem_{index}')
            
            results = await self.analyze_root_cause(time_range, candidates)
            
            return {
                'problem_id': problem_id,
                'status': 'success',
                'results': [
                    {
                        'candidate': r.candidate,
                        'total_score': r.total_score,
                        'confidence': r.confidence,
                        'evidence_summary': r.evidence_summary,
                        'top_reasoning': r.reasoning_chain[:3]  # å‰3æ¡æ¨ç†
                    }
                    for r in results[:5]  # å‰5ä¸ªç»“æœ
                ]
            }
            
        except Exception as e:
            return {
                'problem_id': request.get('problem_id', f'problem_{index}'),
                'status': 'failed', 
                'error': str(e),
                'results': []
            }
