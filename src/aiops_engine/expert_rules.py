#!/usr/bin/env python
"""
A2: ä¸“å®¶è§„åˆ™å¼•æ“å’Œç»¼åˆè¯„åˆ†ç³»ç»Ÿ
åŸºäºè¿ç»´ä¸“å®¶ç»éªŒçš„æ•…éšœè¯Šæ–­è§„åˆ™åº“
"""

import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import json
import yaml

from .anomaly_detection import Anomaly, AnomalyType, SeverityLevel, ServiceCorrelation


@dataclass
class ExpertRule:
    """ä¸“å®¶è§„åˆ™å®šä¹‰"""
    name: str
    description: str
    target_candidates: List[str]  # é€‚ç”¨çš„å€™é€‰æ ¹å› ï¼Œå¦‚ ["ad.LargeGc", "ad.memory"]
    
    # è§¦å‘æ¡ä»¶
    required_anomaly_types: List[AnomalyType]
    required_services: List[str]  # å¯é€‰ï¼Œä¸ºç©ºè¡¨ç¤ºä»»æ„æœåŠ¡
    min_anomaly_count: int
    min_severity: SeverityLevel
    
    # è¯„åˆ†æƒé‡
    base_score: float  # åŸºç¡€åˆ†æ•° (0.0-1.0)
    confidence_multiplier: float  # ç½®ä¿¡åº¦ä¹˜æ•°
    
    # é™„åŠ æ¡ä»¶
    additional_conditions: Dict[str, Any]
    supporting_evidence: List[str]


@dataclass
class RootCauseScore:
    """æ ¹å› è¯„åˆ†ç»“æœ"""
    candidate: str
    total_score: float
    confidence: float
    
    # åˆ†ç»´åº¦è¯„åˆ†
    anomaly_score: float
    correlation_score: float
    expert_rule_score: float
    temporal_score: float
    business_impact_score: float
    
    # æ”¯æŒè¯æ®
    supporting_anomalies: List[Anomaly]
    supporting_correlations: List[ServiceCorrelation]
    matched_rules: List[str]
    evidence_summary: List[str]
    
    # æ¨ç†é“¾
    reasoning_chain: List[str]


class ExpertRulesEngine:
    """ä¸“å®¶è§„åˆ™å¼•æ“
    
    ç¼–ç è¿ç»´ä¸“å®¶çš„æ•…éšœè¯Šæ–­ç»éªŒå’Œæ¨¡å¼è¯†åˆ«è§„åˆ™
    """
    
    def __init__(self, debug: bool = False):
        self.debug = debug
        self.logger = logging.getLogger(__name__)
        
        # åŠ è½½ä¸“å®¶è§„åˆ™åº“
        self.rules = self._initialize_expert_rules()
        
        # æœåŠ¡ä¾èµ–æ‹“æ‰‘ï¼ˆç®€åŒ–ç‰ˆï¼‰
        self.service_topology = {
            'ad': {
                'downstream': ['recommendation', 'user-profile'],
                'upstream': ['frontend', 'load-balancer'],
                'criticality': 'high'
            },
            'cart': {
                'downstream': ['inventory', 'payment', 'checkout'],
                'upstream': ['frontend', 'recommendation'],
                'criticality': 'high'
            },
            'payment': {
                'downstream': ['risk-control', 'bank-gateway'],
                'upstream': ['cart', 'checkout'],
                'criticality': 'critical'
            },
            'checkout': {
                'downstream': ['inventory', 'payment', 'shipping'],
                'upstream': ['cart', 'frontend'],
                'criticality': 'critical'
            },
            'inventory': {
                'downstream': ['database', 'cache'],
                'upstream': ['cart', 'checkout'],
                'criticality': 'medium'
            },
            'recommendation': {
                'downstream': ['ml-engine', 'user-profile'],
                'upstream': ['frontend', 'ad'],
                'criticality': 'medium'
            }
        }
        
        self.logger.info(f"âœ… ä¸“å®¶è§„åˆ™å¼•æ“åˆå§‹åŒ–å®Œæˆ: {len(self.rules)} æ¡è§„åˆ™")
    
    def evaluate_candidates(self, candidates: List[str], anomalies: List[Anomaly], 
                           correlations: List[ServiceCorrelation], 
                           data_bundle) -> List[RootCauseScore]:
        """è¯„ä¼°æ‰€æœ‰å€™é€‰æ ¹å› å¹¶è¯„åˆ†æ’åº
        
        Args:
            candidates: å€™é€‰æ ¹å› åˆ—è¡¨ï¼Œå¦‚ ["ad.Failure", "ad.LargeGc", "cart.Failure"]
            anomalies: æ£€æµ‹åˆ°çš„å¼‚å¸¸åˆ—è¡¨
            correlations: æœåŠ¡å…³è”åˆ†æç»“æœ
            data_bundle: åŸå§‹æ•°æ®åŒ…
            
        Returns:
            List[RootCauseScore]: æŒ‰æ€»åˆ†æ’åºçš„è¯„åˆ†ç»“æœ
        """
        
        self.logger.info(f"ğŸ¯ å¼€å§‹è¯„ä¼° {len(candidates)} ä¸ªå€™é€‰æ ¹å› ")
        
        scored_candidates = []
        
        for candidate in candidates:
            score = self._evaluate_single_candidate(
                candidate, anomalies, correlations, data_bundle
            )
            scored_candidates.append(score)
        
        # æŒ‰æ€»åˆ†æ’åº
        scored_candidates.sort(key=lambda x: x.total_score, reverse=True)
        
        self._log_scoring_summary(scored_candidates)
        
        return scored_candidates
    
    def _evaluate_single_candidate(self, candidate: str, anomalies: List[Anomaly],
                                  correlations: List[ServiceCorrelation], 
                                  data_bundle) -> RootCauseScore:
        """è¯„ä¼°å•ä¸ªå€™é€‰æ ¹å› """
        
        service, failure_type = self._parse_candidate(candidate)
        
        # 1. å¼‚å¸¸åŒ¹é…è¯„åˆ†
        anomaly_score, supporting_anomalies = self._score_anomaly_match(
            candidate, service, failure_type, anomalies
        )
        
        # 2. å…³è”æ€§è¯„åˆ†
        correlation_score, supporting_correlations = self._score_correlations(
            service, correlations
        )
        
        # 3. ä¸“å®¶è§„åˆ™è¯„åˆ†
        expert_rule_score, matched_rules = self._score_expert_rules(
            candidate, service, failure_type, anomalies
        )
        
        # 4. æ—¶åºè¯„åˆ†
        temporal_score = self._score_temporal_patterns(
            service, anomalies
        )
        
        # 5. ä¸šåŠ¡å½±å“è¯„åˆ†
        business_impact_score = self._score_business_impact(
            service, failure_type
        )
        
        # 6. ç»¼åˆè¯„åˆ†
        total_score, confidence = self._calculate_comprehensive_score(
            anomaly_score, correlation_score, expert_rule_score, 
            temporal_score, business_impact_score, len(supporting_anomalies)
        )
        
        # 7. ç”Ÿæˆæ¨ç†é“¾
        reasoning_chain = self._generate_reasoning_chain(
            candidate, anomaly_score, correlation_score, expert_rule_score,
            supporting_anomalies, matched_rules
        )
        
        # 8. ç”Ÿæˆè¯æ®æ‘˜è¦
        evidence_summary = self._generate_evidence_summary(
            supporting_anomalies, supporting_correlations, matched_rules
        )
        
        return RootCauseScore(
            candidate=candidate,
            total_score=total_score,
            confidence=confidence,
            anomaly_score=anomaly_score,
            correlation_score=correlation_score,
            expert_rule_score=expert_rule_score,
            temporal_score=temporal_score,
            business_impact_score=business_impact_score,
            supporting_anomalies=supporting_anomalies,
            supporting_correlations=supporting_correlations,
            matched_rules=matched_rules,
            evidence_summary=evidence_summary,
            reasoning_chain=reasoning_chain
        )
    
    def _score_anomaly_match(self, candidate: str, service: str, failure_type: str, 
                            anomalies: List[Anomaly]) -> Tuple[float, List[Anomaly]]:
        """åŸºäºå¼‚å¸¸åŒ¹é…åº¦è¯„åˆ†"""
        
        score = 0.0
        supporting_anomalies = []
        
        # ç­›é€‰ç›¸å…³æœåŠ¡çš„å¼‚å¸¸
        service_anomalies = [a for a in anomalies if a.service == service or a.service == 'multi-service']
        
        for anomaly in service_anomalies:
            match_score = 0.0
            
            # æ ¹æ®æ•…éšœç±»å‹å’Œå¼‚å¸¸ç±»å‹çš„åŒ¹é…åº¦è¯„åˆ†
            if failure_type == "LargeGc":
                if anomaly.anomaly_type in [AnomalyType.MEMORY_LEAK, AnomalyType.GC_PRESSURE]:
                    match_score = 0.4
                elif anomaly.anomaly_type == AnomalyType.CPU_SPIKE:
                    match_score = 0.3  # CPUé«˜é€šå¸¸ä¼´éšGCå‹åŠ›
            
            elif failure_type == "memory":
                if anomaly.anomaly_type == AnomalyType.MEMORY_LEAK:
                    match_score = 0.5
                elif anomaly.anomaly_type == AnomalyType.GC_PRESSURE:
                    match_score = 0.3
            
            elif failure_type == "cpu":
                if anomaly.anomaly_type == AnomalyType.CPU_SPIKE:
                    match_score = 0.5
            
            elif failure_type == "networkLatency":
                # åªæœ‰çœŸæ­£çš„ç½‘ç»œå»¶è¿Ÿæ‰åŒ¹é…networkLatencyå€™é€‰æ ¹å› 
                if anomaly.anomaly_type == AnomalyType.NETWORK_LATENCY:
                    match_score = 0.5
            
            elif failure_type == "Failure":
                # é€šç”¨æ•…éšœå¯ä»¥åŒ¹é…å¤šç§å¼‚å¸¸ç±»å‹
                if anomaly.anomaly_type == AnomalyType.ERROR_BURST:
                    match_score = 0.4
                elif anomaly.anomaly_type == AnomalyType.SERVICE_LATENCY:
                    # æœåŠ¡å»¶è¿Ÿç—‡çŠ¶åŒ¹é…æœåŠ¡æ•…éšœå€™é€‰æ ¹å› 
                    match_score = 0.4  
                elif anomaly.severity in [SeverityLevel.HIGH, SeverityLevel.CRITICAL]:
                    match_score = 0.3
            
            # è€ƒè™‘å¼‚å¸¸ä¸¥é‡ç¨‹åº¦çš„åŠ æƒ
            severity_weight = {
                SeverityLevel.CRITICAL: 1.0,
                SeverityLevel.HIGH: 0.8,
                SeverityLevel.MEDIUM: 0.6,
                SeverityLevel.LOW: 0.3
            }
            
            match_score *= severity_weight[anomaly.severity]
            match_score *= anomaly.confidence  # ä¹˜ä»¥å¼‚å¸¸æ£€æµ‹ç½®ä¿¡åº¦
            
            if match_score > 0.1:  # åªè€ƒè™‘æœ‰æ„ä¹‰çš„åŒ¹é…
                score += match_score
                supporting_anomalies.append(anomaly)
        
        # å½’ä¸€åŒ–åˆ†æ•°
        score = min(score, 1.0)
        
        return score, supporting_anomalies
    
    def _score_correlations(self, service: str, 
                           correlations: List[ServiceCorrelation]) -> Tuple[float, List[ServiceCorrelation]]:
        """åŸºäºæœåŠ¡å…³è”æ€§è¯„åˆ†"""
        
        score = 0.0
        supporting_correlations = []
        
        for correlation in correlations:
            if service in [correlation.service_a, correlation.service_b]:
                # å…³è”å¼ºåº¦è¯„åˆ†
                correlation_strength = abs(correlation.correlation_coefficient)
                
                # å› æœå…³ç³»åŠ æƒ
                causal_weight = 1.2 if service in correlation.causal_direction else 1.0
                
                contribution = correlation_strength * correlation.confidence * causal_weight * 0.2
                score += contribution
                supporting_correlations.append(correlation)
        
        return min(score, 1.0), supporting_correlations
    
    def _score_expert_rules(self, candidate: str, service: str, failure_type: str,
                           anomalies: List[Anomaly]) -> Tuple[float, List[str]]:
        """åŸºäºä¸“å®¶è§„åˆ™è¯„åˆ†"""
        
        score = 0.0
        matched_rules = []
        
        for rule in self.rules:
            if candidate in rule.target_candidates:
                # æ£€æŸ¥è§„åˆ™æ˜¯å¦åŒ¹é…
                if self._check_rule_match(rule, service, anomalies):
                    rule_score = rule.base_score
                    
                    # æ ¹æ®åŒ¹é…çš„å¼‚å¸¸æ•°é‡è°ƒæ•´åˆ†æ•°
                    matching_anomalies = self._count_matching_anomalies(rule, anomalies)
                    if matching_anomalies >= rule.min_anomaly_count:
                        rule_score *= rule.confidence_multiplier
                    
                    score += rule_score
                    matched_rules.append(rule.name)
        
        return min(score, 1.0), matched_rules
    
    def _score_temporal_patterns(self, service: str, anomalies: List[Anomaly]) -> float:
        """åŸºäºæ—¶åºæ¨¡å¼è¯„åˆ†"""
        
        service_anomalies = [a for a in anomalies if a.service == service]
        
        if len(service_anomalies) <= 1:
            return 0.1  # å•ä¸ªå¼‚å¸¸çš„æ—¶åºåˆ†æ•°è¾ƒä½
        
        # å¼‚å¸¸æŒç»­æ€§è¯„åˆ†
        duration_score = 0.3 if len(service_anomalies) >= 3 else 0.1
        
        # å¼‚å¸¸ä¸¥é‡ç¨‹åº¦è¶‹åŠ¿è¯„åˆ†
        severity_trend_score = 0.2 if any(a.severity == SeverityLevel.CRITICAL for a in service_anomalies) else 0.1
        
        return min(duration_score + severity_trend_score, 1.0)
    
    def _score_business_impact(self, service: str, failure_type: str) -> float:
        """åŸºäºä¸šåŠ¡å½±å“è¯„åˆ†"""
        
        # è·å–æœåŠ¡å…³é”®ç¨‹åº¦
        service_info = self.service_topology.get(service, {'criticality': 'low'})
        criticality = service_info['criticality']
        
        # å…³é”®ç¨‹åº¦è¯„åˆ†
        criticality_scores = {
            'critical': 0.4,
            'high': 0.3,
            'medium': 0.2,
            'low': 0.1
        }
        
        # æ•…éšœç±»å‹å½±å“è¯„åˆ†
        failure_impact_scores = {
            'Failure': 0.3,      # å®Œå…¨æ•…éšœå½±å“æœ€å¤§
            'LargeGc': 0.25,     # GCé—®é¢˜å½±å“æ€§èƒ½
            'memory': 0.2,       # å†…å­˜é—®é¢˜å¯èƒ½å¯¼è‡´OOM
            'cpu': 0.15,         # CPUé—®é¢˜å½±å“å“åº”
            'networkLatency': 0.2 # ç½‘ç»œå»¶è¿Ÿå½±å“ç”¨æˆ·ä½“éªŒ
        }
        
        base_score = criticality_scores.get(criticality, 0.1)
        impact_score = failure_impact_scores.get(failure_type, 0.1)
        
        return base_score + impact_score
    
    def _calculate_comprehensive_score(self, anomaly_score: float, correlation_score: float,
                                     expert_rule_score: float, temporal_score: float,
                                     business_impact_score: float, anomaly_count: int) -> Tuple[float, float]:
        """è®¡ç®—ç»¼åˆè¯„åˆ†å’Œç½®ä¿¡åº¦"""
        
        # åŠ æƒç»¼åˆè¯„åˆ†
        weights = {
            'anomaly': 0.35,      # å¼‚å¸¸åŒ¹é…æƒé‡35%
            'expert_rules': 0.30, # ä¸“å®¶è§„åˆ™æƒé‡30%
            'correlation': 0.15,  # å…³è”åˆ†ææƒé‡15%
            'temporal': 0.10,     # æ—¶åºæ¨¡å¼æƒé‡10%
            'business': 0.10      # ä¸šåŠ¡å½±å“æƒé‡10%
        }
        
        total_score = (
            anomaly_score * weights['anomaly'] +
            expert_rule_score * weights['expert_rules'] +
            correlation_score * weights['correlation'] +
            temporal_score * weights['temporal'] +
            business_impact_score * weights['business']
        )
        
        # ç½®ä¿¡åº¦è®¡ç®—
        confidence_factors = [
            anomaly_score,
            expert_rule_score,
            min(anomaly_count / 5, 1.0),  # å¼‚å¸¸æ•°é‡å› å­
            1.0 if correlation_score > 0.3 else 0.5  # å…³è”æ€§å› å­
        ]
        
        confidence = sum(confidence_factors) / len(confidence_factors)
        confidence = min(confidence, 1.0)
        
        return total_score, confidence
    
    def _generate_reasoning_chain(self, candidate: str, anomaly_score: float, 
                                correlation_score: float, expert_rule_score: float,
                                supporting_anomalies: List[Anomaly], 
                                matched_rules: List[str]) -> List[str]:
        """ç”Ÿæˆæ¨ç†é“¾"""
        
        reasoning = []
        service, failure_type = self._parse_candidate(candidate)
        
        reasoning.append(f"è¯„ä¼°å€™é€‰æ ¹å› : {candidate}")
        
        # å¼‚å¸¸è¯æ®
        if supporting_anomalies:
            reasoning.append(f"å‘ç° {len(supporting_anomalies)} ä¸ªç›¸å…³å¼‚å¸¸:")
            for i, anomaly in enumerate(supporting_anomalies[:3], 1):
                reasoning.append(f"  {i}. {anomaly.evidence} ({anomaly.severity.value})")
        
        # ä¸“å®¶è§„åˆ™åŒ¹é…
        if matched_rules:
            reasoning.append(f"åŒ¹é…ä¸“å®¶è§„åˆ™: {', '.join(matched_rules)}")
        
        # è¯„åˆ†æ¨ç†
        if anomaly_score > 0.5:
            reasoning.append(f"å¼‚å¸¸åŒ¹é…åº¦é«˜ ({anomaly_score:.2f})")
        if expert_rule_score > 0.5:
            reasoning.append(f"ä¸“å®¶è§„åˆ™å¼ºæ”¯æŒ ({expert_rule_score:.2f})")
        if correlation_score > 0.3:
            reasoning.append(f"æœåŠ¡å…³è”æ€§æ˜æ˜¾ ({correlation_score:.2f})")
        
        return reasoning
    
    def _generate_evidence_summary(self, supporting_anomalies: List[Anomaly],
                                 supporting_correlations: List[ServiceCorrelation],
                                 matched_rules: List[str]) -> List[str]:
        """ç”Ÿæˆè¯æ®æ‘˜è¦"""
        
        evidence = []
        
        # å¼‚å¸¸è¯æ®
        if supporting_anomalies:
            critical_anomalies = [a for a in supporting_anomalies if a.severity == SeverityLevel.CRITICAL]
            high_anomalies = [a for a in supporting_anomalies if a.severity == SeverityLevel.HIGH]
            
            if critical_anomalies:
                evidence.append(f"å‘ç° {len(critical_anomalies)} ä¸ªä¸¥é‡å¼‚å¸¸")
            if high_anomalies:
                evidence.append(f"å‘ç° {len(high_anomalies)} ä¸ªé«˜çº§å¼‚å¸¸")
        
        # å…³è”è¯æ®
        if supporting_correlations:
            strong_correlations = [c for c in supporting_correlations if abs(c.correlation_coefficient) > 0.7]
            if strong_correlations:
                evidence.append(f"ä¸ {len(strong_correlations)} ä¸ªæœåŠ¡å­˜åœ¨å¼ºå…³è”")
        
        # è§„åˆ™è¯æ®
        if matched_rules:
            evidence.append(f"åŒ¹é… {len(matched_rules)} æ¡ä¸“å®¶è§„åˆ™")
        
        return evidence
    
    def _initialize_expert_rules(self) -> List[ExpertRule]:
        """åˆå§‹åŒ–ä¸“å®¶è§„åˆ™åº“"""
        
        rules = [
            # ad.LargeGc ä¸“å®¶è§„åˆ™
            ExpertRule(
                name="ad_large_gc_pattern",
                description="AdæœåŠ¡å¤§GCæ¨¡å¼è¯†åˆ«",
                target_candidates=["ad.LargeGc"],
                required_anomaly_types=[AnomalyType.MEMORY_LEAK, AnomalyType.GC_PRESSURE],
                required_services=["ad"],
                min_anomaly_count=2,
                min_severity=SeverityLevel.MEDIUM,
                base_score=0.4,
                confidence_multiplier=1.2,
                additional_conditions={
                    "memory_threshold": 0.85,
                    "gc_time_threshold": 1000  # ms
                },
                supporting_evidence=[
                    "Memory usage exceeds 85%",
                    "GC time over 1 second",
                    "CPU spike during GC events"
                ]
            ),
            
            # å†…å­˜æ³„æ¼æ¨¡å¼
            ExpertRule(
                name="memory_leak_pattern",
                description="å†…å­˜æ³„æ¼æ¨¡å¼è¯†åˆ«",
                target_candidates=["ad.memory", "cart.memory", "payment.memory"],
                required_anomaly_types=[AnomalyType.MEMORY_LEAK],
                required_services=[],  # ä»»æ„æœåŠ¡
                min_anomaly_count=1,
                min_severity=SeverityLevel.HIGH,
                base_score=0.35,
                confidence_multiplier=1.1,
                additional_conditions={},
                supporting_evidence=[
                    "Memory usage continuously increasing",
                    "No memory release after requests"
                ]
            ),
            
            # ç½‘ç»œè¶…æ—¶æ¨¡å¼
            ExpertRule(
                name="network_timeout_pattern", 
                description="ç½‘ç»œè¶…æ—¶çº§è”æ•…éšœæ¨¡å¼",
                target_candidates=["payment.networkLatency", "checkout.networkLatency", "inventory.networkLatency"],
                required_anomaly_types=[AnomalyType.NETWORK_LATENCY, AnomalyType.SERVICE_LATENCY],
                required_services=[],
                min_anomaly_count=1,
                min_severity=SeverityLevel.MEDIUM,
                base_score=0.3,
                confidence_multiplier=1.0,
                additional_conditions={},
                supporting_evidence=[
                    "Network latency significantly increased",
                    "Timeout errors in logs"
                ]
            ),
            
            # é”™è¯¯çˆ†å‘æ¨¡å¼
            ExpertRule(
                name="error_burst_pattern",
                description="é”™è¯¯çˆ†å‘å¯¼è‡´çš„æœåŠ¡æ•…éšœ",
                target_candidates=["ad.Failure", "cart.Failure", "payment.Failure", "checkout.Failure"],
                required_anomaly_types=[AnomalyType.ERROR_BURST],
                required_services=[],
                min_anomaly_count=1,
                min_severity=SeverityLevel.HIGH,
                base_score=0.35,
                confidence_multiplier=1.1,
                additional_conditions={},
                supporting_evidence=[
                    "Error rate significantly increased",
                    "Service availability degraded"
                ]
            ),
            
            # å¤åˆæ•…éšœæ¨¡å¼ - CPU + Memory
            ExpertRule(
                name="cpu_memory_compound_failure",
                description="CPUå’Œå†…å­˜å¤åˆèµ„æºè€—å°½",
                target_candidates=["ad.LargeGc", "cart.Failure", "payment.Failure"],
                required_anomaly_types=[AnomalyType.CPU_SPIKE, AnomalyType.MEMORY_LEAK],
                required_services=[],
                min_anomaly_count=2,
                min_severity=SeverityLevel.MEDIUM,
                base_score=0.4,
                confidence_multiplier=1.3,  # å¤åˆæ¨¡å¼ç½®ä¿¡åº¦æ›´é«˜
                additional_conditions={},
                supporting_evidence=[
                    "Both CPU and memory anomalies detected",
                    "Resource contention pattern"
                ]
            )
        ]
        
        return rules
    
    def _check_rule_match(self, rule: ExpertRule, service: str, anomalies: List[Anomaly]) -> bool:
        """æ£€æŸ¥è§„åˆ™æ˜¯å¦åŒ¹é…å½“å‰æƒ…å†µ"""
        
        # æ£€æŸ¥æœåŠ¡è¦æ±‚
        if rule.required_services and service not in rule.required_services:
            return False
        
        # æ£€æŸ¥å¼‚å¸¸ç±»å‹è¦æ±‚
        anomaly_types_found = [a.anomaly_type for a in anomalies if a.service == service]
        for required_type in rule.required_anomaly_types:
            if required_type not in anomaly_types_found:
                return False
        
        # æ£€æŸ¥å¼‚å¸¸æ•°é‡è¦æ±‚
        matching_anomalies = self._count_matching_anomalies(rule, anomalies)
        if matching_anomalies < rule.min_anomaly_count:
            return False
        
        # æ£€æŸ¥ä¸¥é‡ç¨‹åº¦è¦æ±‚
        severity_levels = [SeverityLevel.LOW, SeverityLevel.MEDIUM, SeverityLevel.HIGH, SeverityLevel.CRITICAL]
        
        # è·å–è¯¥æœåŠ¡çš„æ‰€æœ‰å¼‚å¸¸ä¸¥é‡ç¨‹åº¦ç´¢å¼•
        service_anomalies = [a for a in anomalies if a.service == service]
        if not service_anomalies:
            return False
        
        # æ‰¾åˆ°æœ€é«˜ä¸¥é‡ç¨‹åº¦çš„ç´¢å¼•ï¼ˆä½¿ç”¨ç´¢å¼•æ¯”è¾ƒè€Œä¸æ˜¯æšä¸¾å€¼æ¯”è¾ƒï¼‰
        max_severity_index = max(severity_levels.index(a.severity) for a in service_anomalies)
        min_required_index = severity_levels.index(rule.min_severity)
        
        if max_severity_index < min_required_index:
            return False
        
        return True
    
    def _count_matching_anomalies(self, rule: ExpertRule, anomalies: List[Anomaly]) -> int:
        """è®¡ç®—åŒ¹é…è§„åˆ™çš„å¼‚å¸¸æ•°é‡"""
        
        count = 0
        for anomaly in anomalies:
            if (not rule.required_services or anomaly.service in rule.required_services) and \
               anomaly.anomaly_type in rule.required_anomaly_types:
                count += 1
        
        return count
    
    def _parse_candidate(self, candidate: str) -> Tuple[str, str]:
        """è§£æå€™é€‰æ ¹å› å­—ç¬¦ä¸²"""
        
        if '.' in candidate:
            service, failure_type = candidate.split('.', 1)
            return service, failure_type
        else:
            return candidate, "Failure"
    
    def _log_scoring_summary(self, scored_candidates: List[RootCauseScore]):
        """è®°å½•è¯„åˆ†æ‘˜è¦"""
        
        self.logger.info("ğŸ† æ ¹å› è¯„åˆ†ç»“æœ:")
        
        for i, score in enumerate(scored_candidates[:5], 1):  # æ˜¾ç¤ºå‰5å
            self.logger.info(f"  {i}. {score.candidate}: {score.total_score:.3f} (ç½®ä¿¡åº¦: {score.confidence:.3f})")
            self.logger.info(f"     - å¼‚å¸¸åŒ¹é…: {score.anomaly_score:.3f}")
            self.logger.info(f"     - ä¸“å®¶è§„åˆ™: {score.expert_rule_score:.3f}")
            self.logger.info(f"     - æœåŠ¡å…³è”: {score.correlation_score:.3f}")
            if score.matched_rules:
                self.logger.info(f"     - åŒ¹é…è§„åˆ™: {', '.join(score.matched_rules)}")
