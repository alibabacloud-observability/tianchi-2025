#!/usr/bin/env python
"""
A1: å¹¶è¡Œæ•°æ®è·å–åè°ƒå™¨ - åŸºäºç°æœ‰ä¸‰ä¸ªagentså®ç°
ç»Ÿä¸€åè°ƒ log_agent, metric_agent, trace_agent å¹¶è¡Œå·¥ä½œ
"""

import asyncio
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import time

# å¯¼å…¥ç°æœ‰çš„ä¸‰ä¸ªä¸“ä¸šagents
from ..agents.log_agent import MinimalLogAgent
from ..agents.metric_agent import MinimalMetricAgent  
from ..agents.trace_agent import MinimalTraceAgent
from ..utils.evidence_chain import EvidenceChain


@dataclass
class DataBundle:
    """ç»Ÿä¸€çš„æ•°æ®åŒ…æ ¼å¼"""
    # åŸå§‹æ•°æ®
    logs: List[Dict[str, Any]]
    k8s_metrics: Dict[str, List[float]]
    apm_metrics: Dict[str, List[float]]  
    traces: List[Dict[str, Any]]
    
    # å…ƒä¿¡æ¯
    time_range: str
    start_time: datetime
    end_time: datetime
    baseline_start: datetime
    baseline_end: datetime
    
    # ç»Ÿè®¡ä¿¡æ¯
    collection_stats: Dict[str, Any]
    data_quality_score: float
    
    # åŸºçº¿æ•°æ®ï¼ˆç”¨äºå¼‚å¸¸æ£€æµ‹ï¼‰
    baseline_logs: List[Dict[str, Any]] = None
    baseline_k8s_metrics: Dict[str, List[float]] = None
    baseline_apm_metrics: Dict[str, List[float]] = None


@dataclass
class CollectionTask:
    """æ•°æ®æ”¶é›†ä»»åŠ¡"""
    name: str
    agent_type: str  # 'log', 'metric', 'trace'
    start_time: datetime
    end_time: datetime
    expected_duration_seconds: int
    status: str = 'pending'  # pending, running, completed, failed
    result: Any = None
    error_message: str = None
    execution_time: float = 0.0


class ParallelDataCoordinator:
    """A1å±‚ï¼šå¹¶è¡Œæ•°æ®è·å–åè°ƒå™¨
    
    åŸºäºç°æœ‰çš„ä¸‰ä¸ªä¸“ä¸šagentsï¼Œå®ç°å¹¶è¡Œæ•°æ®é‡‡é›†å’Œç»Ÿä¸€åè°ƒ
    æ”¯æŒç¦»çº¿æ¨¡å¼å’Œåœ¨çº¿æ¨¡å¼
    """
    
    def __init__(self, debug: bool = False, offline_mode: bool = False, problem_id: str = None):
        self.debug = debug
        self.offline_mode = offline_mode
        self.problem_id = problem_id
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–ä¸‰ä¸ªä¸“ä¸šagents - æ ¹æ®æ¨¡å¼ä¼ é€’å‚æ•°
        try:
            if offline_mode:
                self.log_agent = MinimalLogAgent(debug=debug, offline_mode=True, problem_id=problem_id)
                self.metric_agent = MinimalMetricAgent(debug=debug, offline_mode=True, problem_id=problem_id)
                self.trace_agent = MinimalTraceAgent(debug=debug, offline_mode=True, problem_id=problem_id)
                
                mode_desc = f"ç¦»çº¿æ¨¡å¼ (é—®é¢˜ID: {problem_id})"
            else:
                self.log_agent = MinimalLogAgent(debug=debug)
                self.metric_agent = MinimalMetricAgent(debug=debug)
                self.trace_agent = MinimalTraceAgent(debug=debug)
                
                mode_desc = "åœ¨çº¿æ¨¡å¼"
                
            self.logger.info(f"âœ… ä¸‰ä¸ªæ•°æ®é‡‡é›†agentsåˆå§‹åŒ–å®Œæˆ ({mode_desc})")
        except Exception as e:
            self.logger.error(f"âŒ Agentsåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
        
        # é…ç½®å‚æ•°
        self.config = {
            'baseline_hours_before': 0.15,      # åŸºçº¿æœŸï¼šæ•…éšœå‰2å°æ—¶
            'baseline_buffer_minutes': 1,   # åŸºçº¿ç¼“å†²ï¼šæ•…éšœå‰10åˆ†é’Ÿ
            'timeout_seconds': 120,          # å•ä¸ªagentè¶…æ—¶æ—¶é—´
            'parallel_timeout_seconds': 150, # å¹¶è¡Œæ€»è¶…æ—¶æ—¶é—´
            'retry_attempts': 2,             # é‡è¯•æ¬¡æ•°
        }
    
    async def collect_comprehensive_data(self, time_range: str, candidates: List[str]) -> DataBundle:
        """å¹¶è¡Œæ”¶é›†æ‰€æœ‰ç±»å‹çš„ç›‘æ§æ•°æ®
        
        Args:
            time_range: æ—¶é—´èŒƒå›´ï¼Œæ ¼å¼ "2025-08-28 15:08:03 ~ 2025-08-28 15:13:03"
            candidates: å€™é€‰æ ¹å› åˆ—è¡¨ï¼Œå¦‚ ["ad.Failure", "ad.LargeGc"]
            
        Returns:
            DataBundle: ç»Ÿä¸€æ ¼å¼çš„æ•°æ®åŒ…
        """
        
        start_total = time.time()
        self.logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œæ•°æ®æ”¶é›†")
        self.logger.info(f"   ğŸ“… æ—¶é—´èŒƒå›´: {time_range}")
        self.logger.info(f"   ğŸ¯ å€™é€‰æ ¹å› : {candidates}")
        
        # 1. è§£ææ—¶é—´èŒƒå›´
        start_time, end_time = self._parse_time_range(time_range)
        baseline_start, baseline_end = self._calculate_baseline_period(start_time)
        
        # 2. åˆ›å»ºå¹¶è¡Œæ”¶é›†ä»»åŠ¡
        tasks = [
            # æ•…éšœæœŸæ•°æ®æ”¶é›†
            CollectionTask('failure_logs', 'log', start_time, end_time, 30),
            CollectionTask('failure_metrics', 'metric', start_time, end_time, 45),
            CollectionTask('failure_traces', 'trace', start_time, end_time, 20),
            
            # åŸºçº¿æœŸæ•°æ®æ”¶é›†  
            CollectionTask('baseline_logs', 'log', baseline_start, baseline_end, 25),
            CollectionTask('baseline_metrics', 'metric', baseline_start, baseline_end, 40),
        ]
        
        # 3. å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        results = await self._execute_parallel_tasks(tasks)
        
        # 4. æ•´ç†å’ŒéªŒè¯æ•°æ®
        data_bundle = self._assemble_data_bundle(
            results, time_range, start_time, end_time, 
            baseline_start, baseline_end
        )
        
        # 5. è®¡ç®—æ•°æ®è´¨é‡è¯„åˆ†
        data_bundle.data_quality_score = self._calculate_data_quality_score(data_bundle)
        
        total_time = time.time() - start_total
        self.logger.info(f"âœ… å¹¶è¡Œæ•°æ®æ”¶é›†å®Œæˆ")
        self.logger.info(f"   â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’")
        self.logger.info(f"   ğŸ“Š æ•°æ®è´¨é‡è¯„åˆ†: {data_bundle.data_quality_score:.2f}/1.0")
        self.logger.info(f"   ğŸ“ˆ æ€§èƒ½æå‡: é¢„è®¡æ¯”ä¸²è¡Œå¿« {self._estimate_speedup(tasks, total_time):.1f}å€")
        
        return data_bundle
    
    async def _execute_parallel_tasks(self, tasks: List[CollectionTask]) -> Dict[str, CollectionTask]:
        """å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æ•°æ®æ”¶é›†ä»»åŠ¡"""
        
        self.logger.info(f"ğŸ”„ å¹¶è¡Œæ‰§è¡Œ {len(tasks)} ä¸ªæ•°æ®æ”¶é›†ä»»åŠ¡")
        
        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        async_tasks = []
        for task in tasks:
            async_task = asyncio.create_task(
                self._execute_single_task(task)
            )
            async_tasks.append(async_task)
        
        # å¹¶è¡Œç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        try:
            completed_tasks = await asyncio.wait_for(
                asyncio.gather(*async_tasks, return_exceptions=True),
                timeout=self.config['parallel_timeout_seconds']
            )
            
            # æ•´ç†ç»“æœ
            results = {}
            for i, result in enumerate(completed_tasks):
                task = tasks[i]
                if isinstance(result, Exception):
                    task.status = 'failed'
                    task.error_message = str(result)
                    self.logger.error(f"âŒ ä»»åŠ¡ {task.name} å¤±è´¥: {result}")
                else:
                    task.status = 'completed'
                    self.logger.info(f"âœ… ä»»åŠ¡ {task.name} å®Œæˆ: {task.execution_time:.1f}s")
                
                results[task.name] = task
            
            return results
            
        except asyncio.TimeoutError:
            self.logger.error(f"âŒ å¹¶è¡Œä»»åŠ¡è¶…æ—¶ ({self.config['parallel_timeout_seconds']}s)")
            # è¿”å›éƒ¨åˆ†å®Œæˆçš„ç»“æœ
            results = {}
            for task in tasks:
                if task.status not in ['completed', 'failed']:
                    task.status = 'timeout'
                results[task.name] = task
            return results
    
    async def _execute_single_task(self, task: CollectionTask) -> CollectionTask:
        """æ‰§è¡Œå•ä¸ªæ•°æ®æ”¶é›†ä»»åŠ¡"""
        
        start_time = time.time()
        task.status = 'running'
        
        try:
            # åˆ›å»ºä¸´æ—¶è¯æ®é“¾
            evidence_chain = EvidenceChain(task.start_time, task.end_time)
            
            # æ ¹æ®agentç±»å‹æ‰§è¡Œå¯¹åº”ä»»åŠ¡
            if task.agent_type == 'log':
                result = self.log_agent.analyze(evidence_chain)
                # ä»è¯æ®é“¾ä¸­æå–æ—¥å¿—æ•°æ®
                log_evidence = None
                for evidence in evidence_chain.evidence:
                    if evidence.evidence_type == 'log':
                        log_evidence = evidence.data
                        break
                task.result = {
                    'analysis': result,
                    'raw_logs': log_evidence or []
                }
                
            elif task.agent_type == 'metric':
                result = self.metric_agent.analyze(evidence_chain)
                # ä»è¯æ®é“¾ä¸­æå–æŒ‡æ ‡æ•°æ®
                metric_evidence = None
                for evidence in evidence_chain.evidence:
                    if evidence.evidence_type == 'metric':
                        metric_evidence = evidence.data
                        break
                task.result = {
                    'analysis': result,
                    'metrics_data': metric_evidence or {}
                }
                
            elif task.agent_type == 'trace':
                result = self.trace_agent.analyze(evidence_chain)
                # ä»è¯æ®é“¾ä¸­æå–é“¾è·¯æ•°æ®
                trace_evidence = None
                for evidence in evidence_chain.evidence:
                    if evidence.evidence_type == 'trace':
                        trace_evidence = evidence.data
                        break
                task.result = {
                    'analysis': result,
                    'trace_data': trace_evidence or []
                }
            
            task.execution_time = time.time() - start_time
            task.status = 'completed'
            
        except Exception as e:
            task.execution_time = time.time() - start_time
            task.status = 'failed'
            task.error_message = str(e)
            self.logger.error(f"âŒ ä»»åŠ¡ {task.name} æ‰§è¡Œå¤±è´¥: {e}")
        
        return task
    
    def _assemble_data_bundle(self, results: Dict[str, CollectionTask], 
                             time_range: str, start_time: datetime, end_time: datetime,
                             baseline_start: datetime, baseline_end: datetime) -> DataBundle:
        """ç»„è£…æ•°æ®åŒ…"""
        
        # æå–æ•…éšœæœŸæ•°æ®
        logs = []
        k8s_metrics = {}
        apm_metrics = {}
        traces = []
        
        # æå–åŸºçº¿æœŸæ•°æ®
        baseline_logs = []
        baseline_k8s_metrics = {}
        baseline_apm_metrics = {}
        
        # ç»Ÿè®¡ä¿¡æ¯
        collection_stats = {
            'tasks_completed': 0,
            'tasks_failed': 0,
            'tasks_timeout': 0,
            'total_execution_time': 0,
            'data_sources': {
                'logs': False,
                'k8s_metrics': False,
                'apm_metrics': False,
                'traces': False
            }
        }
        
        for task_name, task in results.items():
            collection_stats['total_execution_time'] += task.execution_time
            
            if task.status == 'completed':
                collection_stats['tasks_completed'] += 1
                
                if 'logs' in task_name:
                    if task.result and 'raw_logs' in task.result:
                        if 'failure' in task_name:
                            logs = task.result['raw_logs']
                            collection_stats['data_sources']['logs'] = True
                        elif 'baseline' in task_name:
                            baseline_logs = task.result['raw_logs']
                
                elif 'metrics' in task_name:
                    if task.result and 'metrics_data' in task.result:
                        metrics_data = task.result['metrics_data']
                        if 'failure' in task_name:
                            k8s_metrics = metrics_data.get('k8s_golden_metrics', {})
                            apm_metrics = metrics_data.get('apm_service_metrics', {})
                            collection_stats['data_sources']['k8s_metrics'] = bool(k8s_metrics)
                            collection_stats['data_sources']['apm_metrics'] = bool(apm_metrics)
                        elif 'baseline' in task_name:
                            baseline_k8s_metrics = metrics_data.get('k8s_golden_metrics', {})
                            baseline_apm_metrics = metrics_data.get('apm_service_metrics', {})
                
                elif 'traces' in task_name:
                    if task.result and 'trace_data' in task.result:
                        traces = task.result['trace_data']
                        collection_stats['data_sources']['traces'] = bool(traces)
            
            elif task.status == 'failed':
                collection_stats['tasks_failed'] += 1
            elif task.status == 'timeout':
                collection_stats['tasks_timeout'] += 1
        
        return DataBundle(
            logs=logs,
            k8s_metrics=k8s_metrics,
            apm_metrics=apm_metrics,
            traces=traces,
            time_range=time_range,
            start_time=start_time,
            end_time=end_time,
            baseline_start=baseline_start,
            baseline_end=baseline_end,
            baseline_logs=baseline_logs,
            baseline_k8s_metrics=baseline_k8s_metrics,
            baseline_apm_metrics=baseline_apm_metrics,
            collection_stats=collection_stats,
            data_quality_score=0.0  # Will be calculated later
        )
    
    def _calculate_data_quality_score(self, data_bundle: DataBundle) -> float:
        """è®¡ç®—æ•°æ®è´¨é‡è¯„åˆ† (0.0 - 1.0)"""
        
        score = 0.0
        max_score = 1.0
        
        # æ•°æ®å®Œæ•´æ€§è¯„åˆ† (40%)
        completeness_score = 0.0
        data_sources = data_bundle.collection_stats['data_sources']
        
        if data_sources['logs']:
            completeness_score += 0.1
        if data_sources['k8s_metrics']:
            completeness_score += 0.15
        if data_sources['apm_metrics']:
            completeness_score += 0.1
        if data_sources['traces']:
            completeness_score += 0.05
        
        # ä»»åŠ¡æˆåŠŸç‡è¯„åˆ† (30%)
        stats = data_bundle.collection_stats
        total_tasks = stats['tasks_completed'] + stats['tasks_failed'] + stats['tasks_timeout']
        success_rate = stats['tasks_completed'] / total_tasks if total_tasks > 0 else 0
        success_score = success_rate * 0.3
        
        # æ•°æ®é‡è¯„åˆ† (20%)
        volume_score = 0.0
        if len(data_bundle.logs) > 0:
            volume_score += 0.05
        if len(data_bundle.k8s_metrics) > 0:
            volume_score += 0.08
        if len(data_bundle.apm_metrics) > 0:
            volume_score += 0.05
        if len(data_bundle.traces) > 0:
            volume_score += 0.02
        
        # åŸºçº¿æ•°æ®å¯ç”¨æ€§è¯„åˆ† (10%)
        baseline_score = 0.0
        if data_bundle.baseline_logs:
            baseline_score += 0.03
        if data_bundle.baseline_k8s_metrics:
            baseline_score += 0.04
        if data_bundle.baseline_apm_metrics:
            baseline_score += 0.03
        
        total_score = completeness_score + success_score + volume_score + baseline_score
        return min(total_score, 1.0)
    
    def _parse_time_range(self, time_range: str) -> Tuple[datetime, datetime]:
        """è§£ææ—¶é—´èŒƒå›´å­—ç¬¦ä¸²"""
        try:
            start_str, end_str = time_range.split(' ~ ')
            start_time = datetime.strptime(start_str, '%Y-%m-%d %H:%M:%S')
            end_time = datetime.strptime(end_str, '%Y-%m-%d %H:%M:%S')
            return start_time, end_time
        except Exception as e:
            self.logger.error(f"âŒ æ—¶é—´èŒƒå›´è§£æå¤±è´¥: {e}")
            raise ValueError(f"Invalid time range format: {time_range}")
    
    def _calculate_baseline_period(self, failure_start: datetime) -> Tuple[datetime, datetime]:
        """è®¡ç®—åŸºçº¿æœŸæ—¶é—´èŒƒå›´
        
        é€»è¾‘ï¼šè·å–æ•…éšœå‰ä¸€æ®µæ—¶é—´çš„æ­£å¸¸åŸºçº¿æ•°æ®
        - baseline_end: æ•…éšœå‰buffer_minutesåˆ†é’Ÿï¼ˆé¿å…æ•…éšœå‰å…†å½±å“ï¼‰
        - baseline_start: å†å¾€å‰æ¨baseline_hours_beforeå°æ—¶çš„æ•°æ®ä½œä¸ºåŸºçº¿çª—å£
        """
        # æ•…éšœå‰10åˆ†é’Ÿä½œä¸ºåŸºçº¿ç»“æŸæ—¶é—´ï¼ˆé¿å…æ•…éšœå‰å…†ï¼‰
        baseline_end = failure_start - timedelta(minutes=self.config['baseline_buffer_minutes'])
        
        # ä»åŸºçº¿ç»“æŸæ—¶é—´å¾€å‰æ¨0.15å°æ—¶(9åˆ†é’Ÿ)ä½œä¸ºåŸºçº¿çª—å£
        baseline_start = baseline_end - timedelta(hours=self.config['baseline_hours_before'])
        
        return baseline_start, baseline_end
    
    def _estimate_speedup(self, tasks: List[CollectionTask], actual_total_time: float) -> float:
        """ä¼°ç®—ç›¸å¯¹äºä¸²è¡Œæ‰§è¡Œçš„åŠ é€Ÿæ¯”"""
        serial_time = sum(task.expected_duration_seconds for task in tasks)
        return serial_time / actual_total_time if actual_total_time > 0 else 1.0
    
    def get_data_summary(self, data_bundle: DataBundle) -> Dict[str, Any]:
        """è·å–æ•°æ®æ‘˜è¦"""
        
        # æå–æœåŠ¡åˆ—è¡¨
        services = set()
        
        # ä»æ—¥å¿—ä¸­æå–æœåŠ¡
        for log in data_bundle.logs:
            if isinstance(log, dict) and 'service_name' in log:
                services.add(log['service_name'])
        
        # ä»æŒ‡æ ‡ä¸­æå–æœåŠ¡
        for metric_name in data_bundle.apm_metrics.keys():
            if ':' in metric_name:
                service = metric_name.split(':')[0]
                services.add(service)
        
        # ä»é“¾è·¯ä¸­æå–æœåŠ¡
        for trace in data_bundle.traces:
            if isinstance(trace, dict) and 'service_name' in trace:
                services.add(trace['service_name'])
        
        return {
            'data_quality_score': data_bundle.data_quality_score,
            'services_discovered': sorted(list(services)),
            'data_counts': {
                'logs': len(data_bundle.logs),
                'k8s_metrics': len(data_bundle.k8s_metrics),
                'apm_metrics': len(data_bundle.apm_metrics),
                'traces': len(data_bundle.traces)
            },
            'baseline_available': {
                'logs': bool(data_bundle.baseline_logs),
                'k8s_metrics': bool(data_bundle.baseline_k8s_metrics),
                'apm_metrics': bool(data_bundle.baseline_apm_metrics)
            },
            'collection_performance': {
                'total_time': data_bundle.collection_stats['total_execution_time'],
                'tasks_completed': data_bundle.collection_stats['tasks_completed'],
                'tasks_failed': data_bundle.collection_stats['tasks_failed']
            }
        }
