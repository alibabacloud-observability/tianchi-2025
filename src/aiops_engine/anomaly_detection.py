#!/usr/bin/env python
"""
A2: å¼‚å¸¸æ£€æµ‹å’Œå…³è”åˆ†æå¼•æ“
åŸºäºç»Ÿè®¡å­¦æ–¹æ³•å’Œé¢†åŸŸçŸ¥è¯†çš„æ™ºèƒ½å¼‚å¸¸æ£€æµ‹
"""

import numpy as np
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import statistics


class AnomalyType(Enum):
    """å¼‚å¸¸ç±»å‹æšä¸¾ - é‡æ„ï¼šåŒºåˆ†æ ¹æœ¬åŸå› å’Œç—‡çŠ¶"""
    
    # === æ ¹æœ¬åŸå› ç±»å‹ï¼ˆèµ„æº/ç³»ç»Ÿå±‚é¢é—®é¢˜ï¼‰===
    CPU_SPIKE = "cpu_spike"                 # CPUé—®é¢˜å¯¼è‡´çš„æ€§èƒ½ä¸‹é™
    MEMORY_LEAK = "memory_leak"             # å†…å­˜é—®é¢˜ï¼ˆæ³„æ¼ã€ä¸è¶³ã€GCå‹åŠ›ç­‰ï¼‰
    NETWORK_LATENCY = "network_latency"     # çº¯ç²¹çš„ç½‘ç»œä¼ è¾“å»¶è¿Ÿ
    DISK_IO = "disk_io"                     # ç£ç›˜IOç“¶é¢ˆ
    GC_PRESSURE = "gc_pressure"             # åƒåœ¾å›æ”¶å‹åŠ›
    
    # === ç—‡çŠ¶ç±»å‹ï¼ˆè¡¨ç°å±‚é¢é—®é¢˜ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†ææ ¹å› ï¼‰===
    SERVICE_LATENCY = "service_latency"     # æœåŠ¡å“åº”å»¶è¿Ÿï¼ˆç—‡çŠ¶ï¼Œéœ€æŸ¥æ‰¾æ ¹å› ï¼‰
    ERROR_BURST = "error_burst"             # é”™è¯¯æ¿€å¢ï¼ˆç—‡çŠ¶ï¼Œéœ€æŸ¥æ‰¾æ ¹å› ï¼‰
    
    # === å…¶ä»– ===
    UNKNOWN = "unknown"


class SeverityLevel(Enum):
    """ä¸¥é‡ç¨‹åº¦æšä¸¾"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


@dataclass
class Anomaly:
    """å¼‚å¸¸æ£€æµ‹ç»“æœ"""
    service: str
    metric_name: str
    anomaly_type: AnomalyType
    severity: SeverityLevel
    confidence: float  # 0.0-1.0
    
    # æ•°å€¼ä¿¡æ¯
    current_value: float
    baseline_mean: float
    baseline_std: float
    z_score: float
    percentage_change: float
    
    # æ—¶é—´ä¿¡æ¯
    timestamp: datetime
    duration_minutes: int
    
    # è¯æ®ä¿¡æ¯
    evidence: str
    raw_data: Dict[str, Any]


@dataclass  
class ServiceCorrelation:
    """æœåŠ¡å…³è”æ€§åˆ†æç»“æœ"""
    service_a: str
    service_b: str
    correlation_coefficient: float
    lag_seconds: int
    causal_direction: str  # "A->B", "B->A", "bidirectional", "none"
    confidence: float
    supporting_evidence: List[str]


class AnomalyDetectionEngine:
    """A2å±‚ï¼šå¼‚å¸¸æ£€æµ‹å¼•æ“
    
    åŸºäºç»Ÿè®¡å­¦æ–¹æ³•æ£€æµ‹å„ç±»ç›‘æ§æ•°æ®ä¸­çš„å¼‚å¸¸
    """
    
    def __init__(self, debug: bool = False):
        self.debug = debug
        self.logger = logging.getLogger(__name__)
        
        # å¼‚å¸¸æ£€æµ‹é˜ˆå€¼é…ç½®
        self.thresholds = {
            # CPUç›¸å…³é˜ˆå€¼ - Phase 1ä¼˜åŒ–ï¼šé€‚åº¦é™ä½é˜ˆå€¼æé«˜æ•æ„Ÿåº¦
            'cpu_usage': {
                'z_score': 1.5,            # ä»2.5é™ä½åˆ°2.0ï¼Œæé«˜æ•æ„Ÿåº¦
                'percentage_change': 100,  # ä»150%é™ä½åˆ°100%ï¼Œæ•è·æ›´å¤šCPUå¼‚å¸¸  
                'absolute_high': 0.70      # ä»0.85é™ä½åˆ°0.75ï¼Œæ›´æ—©æ£€æµ‹é«˜CPU
            },
            
            # å†…å­˜ç›¸å…³é˜ˆå€¼
            'memory_usage': {
                'z_score': 1.5,
                'percentage_change': 100,  # 100%å˜åŒ–è§†ä¸ºå¼‚å¸¸
                'absolute_high': 0.70      # ç»å¯¹å€¼90%ä»¥ä¸Š
            },
            
            # ç½‘ç»œå»¶è¿Ÿé˜ˆå€¼
            'latency': {
                'z_score': 1.5,
                'percentage_change': 200,  # 200%å˜åŒ–è§†ä¸ºå¼‚å¸¸
                'absolute_high': 2.0       # ç»å¯¹å€¼2ç§’ä»¥ä¸Š
            },
            
            # é”™è¯¯ç‡é˜ˆå€¼
            'error_rate': {
                'z_score': 2.0,
                'percentage_change': 300,  # 300%å˜åŒ–è§†ä¸ºå¼‚å¸¸
                'absolute_high': 0.05      # ç»å¯¹å€¼5%ä»¥ä¸Š
            },
            
            # GCæ—¶é—´é˜ˆå€¼
            'gc_time': {
                'z_score': 2.5,
                'percentage_change': 200,  # 200%å˜åŒ–è§†ä¸ºå¼‚å¸¸
                'absolute_high': 1.0       # ç»å¯¹å€¼1ç§’ä»¥ä¸Š
            }
        }
        
        # æœåŠ¡æ˜ å°„ï¼ˆä»ä¸šåŠ¡åæ˜ å°„åˆ°æŠ€æœ¯æŒ‡æ ‡ï¼‰
        self.service_mappings = {
            'ad': ['ad', 'advertisement'],
            'cart': ['cart', 'shopping-cart'], 
            'payment': ['payment', 'pay'],
            'checkout': ['checkout'],
            'inventory': ['inventory', 'stock'],
            'recommendation': ['recommendation', 'recommend'],
            'frontend': ['frontend', 'frontend-web', 'frontend-proxy']
        }
    
    def detect_all_anomalies(self, data_bundle) -> List[Anomaly]:
        """æ£€æµ‹æ‰€æœ‰ç±»å‹çš„å¼‚å¸¸
        
        Args:
            data_bundle: æ¥è‡ªParallelDataCoordinatorçš„æ•°æ®åŒ…
            
        Returns:
            List[Anomaly]: æ£€æµ‹åˆ°çš„å¼‚å¸¸åˆ—è¡¨ï¼ŒæŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
        """
        
        self.logger.info("ğŸ” å¼€å§‹å…¨é¢å¼‚å¸¸æ£€æµ‹")
        
        anomalies = []
        # ç§»é™¤è°ƒè¯•æ–­ç‚¹
        
        # 1. K8sæŒ‡æ ‡å¼‚å¸¸æ£€æµ‹
        k8s_anomalies = self._detect_k8s_metrics_anomalies(
            data_bundle.k8s_metrics, 
            data_bundle.baseline_k8s_metrics,
            data_bundle.start_time
        )
        anomalies.extend(k8s_anomalies)
        
        # 2. APMæŒ‡æ ‡å¼‚å¸¸æ£€æµ‹  
        apm_anomalies = self._detect_apm_metrics_anomalies(
            data_bundle.apm_metrics,
            data_bundle.baseline_apm_metrics, 
            data_bundle.start_time
        )
        anomalies.extend(apm_anomalies)
        
        # 3. æ—¥å¿—å¼‚å¸¸æ£€æµ‹
        log_anomalies = self._detect_log_anomalies(
            data_bundle.logs,
            data_bundle.baseline_logs,
            data_bundle.start_time
        )
        anomalies.extend(log_anomalies)
        
        # 4. é“¾è·¯å¼‚å¸¸æ£€æµ‹
        trace_anomalies = self._detect_trace_anomalies(
            data_bundle.traces,
            data_bundle.start_time
        )
        anomalies.extend(trace_anomalies)
        
        # 5. ğŸ”— æ‰§è¡Œå› æœåˆ†æï¼šèµ„æºé—®é¢˜ä¼˜å…ˆä¸ºæ ¹å› 
        causal_analyzed_anomalies = self._perform_causal_analysis(anomalies)
        
        if self.debug:
            self.logger.info(f"ğŸ”— å› æœåˆ†æ: {len(anomalies)} â†’ {len(causal_analyzed_anomalies)} å¼‚å¸¸")
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦å’Œç½®ä¿¡åº¦æ’åº
        anomalies = self._rank_anomalies(causal_analyzed_anomalies)
        
        self.logger.info(f"âœ… å¼‚å¸¸æ£€æµ‹å®Œæˆ: å‘ç° {len(anomalies)} ä¸ªå¼‚å¸¸")
        self._log_anomaly_summary(anomalies)
        
        return anomalies
    
    
    
    def _detect_k8s_metrics_anomalies(self, current_metrics, baseline_metrics, timestamp: datetime) -> List[Anomaly]:
        """æ£€æµ‹K8så¼‚å¸¸ - æ–°æ ¼å¼ï¼šæŒ‰Podåˆ†åˆ«åˆ†æ"""
        
        anomalies = []
        for service, service_current_metrics in current_metrics.items():
            service_baseline_metrics = baseline_metrics.get(service, {})
            for pod, pod_current_metrics in service_current_metrics.items():            
                pod_baseline_metrics = service_baseline_metrics.get(pod, {})
                for metric_name, metric_data in pod_current_metrics.items():
                    if metric_name == 'entity_id':
                        continue
                    if not isinstance(metric_data, dict) or 'values' not in metric_data:
                        continue
                        
                    current_values = metric_data['values']
                    if not current_values:
                        continue
                    
                    # è·å–åŸºçº¿æ•°æ®
                    baseline_metric_data = pod_baseline_metrics.get(metric_name, {})
                    baseline_values = baseline_metric_data.get('values', []) if isinstance(baseline_metric_data, dict) else []
                    
                    if len(baseline_values) < 3:  # åŸºçº¿æ•°æ®ä¸è¶³
                        continue
                    # if service == "ad":
                    #     import pdb; pdb.set_trace()
                    
                    # æ‰§è¡Œå¼‚å¸¸æ£€æµ‹
                    anomaly = self._analyze_single_metric_anomaly(
                        pod, metric_name, service, current_values, baseline_values, timestamp
                    )
                    
                    if anomaly:
                        anomalies.append(anomaly)
                        if self.debug:
                            self.logger.info(f"ğŸ” æ£€æµ‹åˆ°K8så¼‚å¸¸: {service} - {pod} - {metric_name}")
        
        return anomalies
    
    
    def _analyze_single_metric_anomaly(self, pod_name: str, metric_name: str, service: str,
                                     current_values: List[float], baseline_values: List[float], 
                                     timestamp: datetime) -> Optional[Anomaly]:
        """åˆ†æå•ä¸ªæŒ‡æ ‡çš„å¼‚å¸¸æƒ…å†µ"""
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯

        current_mean = statistics.mean(current_values)
        baseline_mean = statistics.mean(baseline_values)
        
        try:
            baseline_std = statistics.stdev(baseline_values)
        except statistics.StatisticsError:
            baseline_std = 0
        
        if baseline_std == 0:  # é¿å…é™¤é›¶
            return None

        
        # Z-Scoreæ£€æµ‹
        z_score = abs((current_mean - baseline_mean) / baseline_std)
        percentage_change = ((current_mean - baseline_mean) / baseline_mean) * 100
        
        # æ ¹æ®æŒ‡æ ‡ç±»å‹åˆ¤æ–­å¼‚å¸¸
        anomaly_type, threshold_config = self._classify_k8s_anomaly_type(metric_name)
        
        if (z_score > threshold_config['z_score'] and 
            abs(percentage_change) > threshold_config['percentage_change']):
            
            # è®¡ç®—ä¸¥é‡ç¨‹åº¦
            severity = self._calculate_severity(z_score, abs(percentage_change), current_mean)
            # Phase 1ä¼˜åŒ–ï¼šç»Ÿä¸€ç½®ä¿¡åº¦è®¡ç®—ï¼Œç¡®ä¿CPUå¼‚å¸¸å…¬å¹³ç«äº‰
            confidence = min(z_score / 4, 1.0)  # ä¸APMå¼‚å¸¸ä¿æŒä¸€è‡´çš„ç½®ä¿¡åº¦è®¡ç®—
            
            return Anomaly(
                service=service,
                metric_name=f"{pod_name}:{metric_name}",  # åŒ…å«podä¿¡æ¯
                anomaly_type=anomaly_type,
                severity=severity,
                confidence=confidence,
                current_value=current_mean,
                baseline_mean=baseline_mean,
                baseline_std=baseline_std,
                z_score=z_score,
                percentage_change=percentage_change,
                timestamp=timestamp,
                duration_minutes=5,  # K8sæŒ‡æ ‡é€šå¸¸ä¸º5åˆ†é’Ÿçª—å£
                evidence=f"{service} pod {pod_name} {anomaly_type.value}: {current_mean:.2f} vs baseline {baseline_mean:.2f} ({percentage_change:+.1f}%)",
                raw_data={'current_values': current_values, 'baseline_values': baseline_values, 'pod_name': pod_name}
            )
        
        return None  # æœªæ£€æµ‹åˆ°å¼‚å¸¸
    
    def _is_error_metric(self, metric_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯é”™è¯¯ç±»æŒ‡æ ‡"""
        error_keywords = ['error', 'failure', 'exception', 'fault', 'é”™è¯¯', 'å¼‚å¸¸', 'æ•…éšœ']
        return any(keyword in metric_name.lower() for keyword in error_keywords)
    
    def _create_error_spike_anomaly(self, service: str, metric_name: str, 
                                  current_mean: float, baseline_mean: float,
                                  z_score: float, percentage_change: float, 
                                  timestamp: datetime, raw_data: List[float]) -> Anomaly:
        """åˆ›å»ºé”™è¯¯æ¿€å¢å¼‚å¸¸å¯¹è±¡"""
        
        # æ ¹æ®é”™è¯¯æ•°é‡ç¡®å®šä¸¥é‡ç¨‹åº¦
        if current_mean > 50:
            severity = SeverityLevel.CRITICAL
            confidence = 0.95
        elif current_mean > 10:
            severity = SeverityLevel.HIGH
            confidence = 0.85
        else:
            severity = SeverityLevel.MEDIUM
            confidence = 0.75
        
        # è®¡ç®—æŒç»­æ—¶é—´ï¼ˆåŸºäºæ•°æ®ç‚¹æ•°é‡ï¼Œå‡è®¾1ç§’1ä¸ªç‚¹ï¼‰
        duration_minutes = len(raw_data) // 60
        
        # ç”Ÿæˆè¯æ®æè¿°
        evidence = f"{service} error spike: from {baseline_mean:.1f} to {current_mean:.1f} errors"
        if percentage_change == float('inf'):
            evidence += " (baseline was healthy with no errors)"
        else:
            evidence += f" (+{percentage_change:.1f}% change)"
        
        return Anomaly(
            service=service,
            metric_name=metric_name,
            anomaly_type=AnomalyType.ERROR_BURST,
            severity=severity,
            confidence=confidence,
            current_value=current_mean,
            baseline_mean=baseline_mean,
            baseline_std=0.0,  # åŸºçº¿ä¸ºç©ºæ—¶æ ‡å‡†å·®ä¸º0
            z_score=z_score,
            percentage_change=percentage_change,
            timestamp=timestamp,
            duration_minutes=duration_minutes,
            evidence=evidence,
            raw_data={
                'current_values': raw_data,
                'baseline_values': [],
                'spike_detected': True,
                'baseline_was_healthy': baseline_mean == 0
            }
        )
    
    def _detect_apm_metrics_anomalies(self, current_metrics: Dict[str, Dict],
                                     baseline_metrics: Dict[str, Dict], 
                                     timestamp: datetime) -> List[Anomaly]:
        """æ£€æµ‹APMåº”ç”¨æŒ‡æ ‡å¼‚å¸¸ - é€‚é…æ–°æ•°æ®æ ¼å¼
        
        æ–°æ ¼å¼: {
            service_name: {
                metric_name: {
                    'values': [100, 120, ...],
                    'timestamps': [timestamp1, timestamp2, ...]  # ä¸ä½¿ç”¨ï¼Œä»…valuesç”¨äºå¼‚å¸¸æ£€æµ‹
                }
            }
        }
        """
        
        anomalies = []
        
        for service_name, service_current_metrics in current_metrics.items():
            service_baseline_metrics = baseline_metrics.get(service_name, {})
            
            for metric_name, metric_data in service_current_metrics.items():
                if not isinstance(metric_data, dict) or 'values' not in metric_data:
                    continue
                    
                current_values = metric_data['values']
                if not current_values:
                    continue
                    
                # è·å–åŸºçº¿æ•°æ®
                baseline_metric_data = service_baseline_metrics.get(metric_name, {})
                baseline_values = baseline_metric_data.get('values', []) if isinstance(baseline_metric_data, dict) else []
                
                current_mean = statistics.mean(current_values)
                
                # ğŸ”§ ä¿®å¤ï¼šå¤„ç†åŸºçº¿æ•°æ®ç¼ºå¤±çš„æƒ…å†µ
                if len(baseline_values) < 2:
                    # å¯¹äºerrorç±»æŒ‡æ ‡ï¼ŒåŸºçº¿ä¸ºç©ºå¯èƒ½è¡¨ç¤ºå¥åº·çŠ¶æ€ï¼ˆæ— é”™è¯¯ï¼‰
                    if self._is_error_metric(metric_name):
                        baseline_mean = 0.0
                        baseline_std = 0.0
                        
                        # å¦‚æœå½“å‰æœ‰é”™è¯¯è€ŒåŸºçº¿æ— é”™è¯¯ï¼Œè¿™æ˜¯æ˜æ˜¾å¼‚å¸¸
                        if current_mean > 0:
                            z_score = float('inf')  # æ— é™å¤§è¡¨ç¤ºæç«¯å¼‚å¸¸
                            percentage_change = float('inf') if baseline_mean == 0 else ((current_mean - baseline_mean) / baseline_mean) * 100
                            
                            # ç›´æ¥æ ‡è®°ä¸ºé«˜é£é™©å¼‚å¸¸
                            anomaly = self._create_error_spike_anomaly(
                                service_name, metric_name, current_mean, baseline_mean, 
                                z_score, percentage_change, timestamp, current_values
                            )
                            anomalies.append(anomaly)
                        
                    continue  # å…¶ä»–ç±»å‹æŒ‡æ ‡ä»ç„¶è·³è¿‡
                
                # ç»Ÿè®¡è®¡ç®—ï¼ˆæœ‰å……åˆ†åŸºçº¿æ•°æ®çš„æƒ…å†µï¼‰
                baseline_mean = statistics.mean(baseline_values) 
                baseline_std = statistics.stdev(baseline_values) if len(baseline_values) > 1 else 0
                
                if baseline_std == 0:
                    # åŸºçº¿æ ‡å‡†å·®ä¸º0ä½†æœ‰æ•°æ®ï¼Œå¯èƒ½æ˜¯ç¨³å®šæœŸ
                    if baseline_mean == 0 and current_mean > 0:
                        # ä»0åˆ°æœ‰å€¼çš„çªå˜ï¼Œç‰¹åˆ«æ˜¯errorç±»æŒ‡æ ‡
                        z_score = float('inf')
                        percentage_change = float('inf')
                    else:
                        percentage_change = ((current_mean - baseline_mean) / baseline_mean) * 100 if baseline_mean != 0 else 0
                        z_score = 0  # ç¨³å®šæœŸæ— æ³•è®¡ç®—z_score
                        continue
                else:
                    z_score = abs((current_mean - baseline_mean) / baseline_std)
                    percentage_change = ((current_mean - baseline_mean) / baseline_mean) * 100
                
                # APMæŒ‡æ ‡å¼‚å¸¸ç±»å‹è¯†åˆ«
                anomaly_type = self._classify_apm_anomaly_type(metric_name, percentage_change)
                threshold_config = self._get_threshold_config(anomaly_type, metric_name)
                
                if (z_score > threshold_config['z_score'] and 
                    abs(percentage_change) > threshold_config['percentage_change']):
                    
                    severity = self._calculate_severity(z_score, abs(percentage_change), current_mean)
                    confidence = min(z_score / 4, 1.0)
                    
                    anomaly = Anomaly(
                        service=service_name,
                        metric_name=metric_name,
                        anomaly_type=anomaly_type,
                        severity=severity, 
                        confidence=confidence,
                        current_value=current_mean,
                        baseline_mean=baseline_mean,
                        baseline_std=baseline_std,
                        z_score=z_score,
                        percentage_change=percentage_change,
                        timestamp=timestamp,
                        duration_minutes=5,
                        evidence=f"{service_name} {metric_name}: {current_mean:.3f} vs baseline {baseline_mean:.3f} ({percentage_change:+.1f}%)",
                        raw_data={
                            'current_values': current_values,
                            'baseline_values': baseline_values,
                            'metric_type': 'apm'
                        }
                    )
                    
                    anomalies.append(anomaly)
        
        return anomalies
    
    def _detect_log_anomalies(self, current_logs: List[Dict[str, Any]], 
                             baseline_logs: List[Dict[str, Any]],
                             timestamp: datetime) -> List[Anomaly]:
        """æ£€æµ‹æ—¥å¿—å¼‚å¸¸æ¨¡å¼"""
        
        anomalies = []
        
        # ç»Ÿè®¡å½“å‰æ—¥å¿—çš„é”™è¯¯æ¨¡å¼
        current_patterns = self._analyze_log_patterns(current_logs)
        baseline_patterns = self._analyze_log_patterns(baseline_logs) if baseline_logs else {}
        
        for service, patterns in current_patterns.items():
            baseline_service_patterns = baseline_patterns.get(service, {})
            
            # æ£€æŸ¥é”™è¯¯ç‡å¼‚å¸¸
            current_error_rate = patterns.get('error_rate', 0)
            baseline_error_rate = baseline_service_patterns.get('error_rate', 0)
            
            if current_error_rate > baseline_error_rate * 3 and current_error_rate > 0.1:
                anomaly = Anomaly(
                    service=service,
                    metric_name='log_error_rate',
                    anomaly_type=AnomalyType.ERROR_BURST,
                    severity=self._calculate_log_severity(current_error_rate),
                    confidence=0.8,
                    current_value=current_error_rate,
                    baseline_mean=baseline_error_rate,
                    baseline_std=0,
                    z_score=3.0,  # ç®€åŒ–å¤„ç†
                    percentage_change=((current_error_rate - baseline_error_rate) / baseline_error_rate * 100) if baseline_error_rate > 0 else 300,
                    timestamp=timestamp,
                    duration_minutes=5,
                    evidence=f"{service} log error rate: {current_error_rate:.1%} vs baseline {baseline_error_rate:.1%}",
                    raw_data={
                        'current_patterns': patterns,
                        'baseline_patterns': baseline_service_patterns
                    }
                )
                anomalies.append(anomaly)
        
        return anomalies
    
    def _detect_trace_anomalies(self, traces: List[Dict[str, Any]], 
                               timestamp: datetime) -> List[Anomaly]:
        """æ£€æµ‹é“¾è·¯è¿½è¸ªå¼‚å¸¸"""
        
        anomalies = []
        
        if not traces:
            return anomalies
        
        # æŒ‰æœåŠ¡åˆ†ç»„åˆ†æå»¶è¿Ÿ
        service_latencies = {}
        for trace in traces:
            service = trace.get('service_name', 'unknown')
            duration_ms = trace.get('duration_ms', 0)
            
            if service not in service_latencies:
                service_latencies[service] = []
            service_latencies[service].append(duration_ms)
        
        # æ£€æµ‹é«˜å»¶è¿Ÿå¼‚å¸¸
        for service, latencies in service_latencies.items():
            if len(latencies) < 3:
                continue
            
            # è®¡ç®—å»¶è¿Ÿç»Ÿè®¡
            mean_latency = statistics.mean(latencies)
            p95_latency = np.percentile(latencies, 95)
            p99_latency = np.percentile(latencies, 99)
            
            # é«˜å»¶è¿Ÿé˜ˆå€¼åˆ¤æ–­ï¼ˆç®€åŒ–ç‰ˆï¼‰
            if mean_latency > 1000 or p95_latency > 5000:  # 1ç§’å¹³å‡æˆ–5ç§’P95
                severity = SeverityLevel.HIGH if p99_latency > 10000 else SeverityLevel.MEDIUM
                
                anomaly = Anomaly(
                    service=service,
                    metric_name='trace_latency',
                    anomaly_type=AnomalyType.SERVICE_LATENCY,
                    severity=severity,
                    confidence=0.7,
                    current_value=mean_latency,
                    baseline_mean=500,  # å‡è®¾æ­£å¸¸åŸºçº¿500ms
                    baseline_std=100,
                    z_score=(mean_latency - 500) / 100,
                    percentage_change=((mean_latency - 500) / 500) * 100,
                    timestamp=timestamp,
                    duration_minutes=5,
                    evidence=f"{service} high latency: avg {mean_latency:.0f}ms, p95 {p95_latency:.0f}ms",
                    raw_data={
                        'latencies': latencies,
                        'stats': {
                            'mean': mean_latency,
                            'p95': p95_latency,
                            'p99': p99_latency
                        }
                    }
                )
                anomalies.append(anomaly)
        
        return anomalies
    
    def _classify_k8s_anomaly_type(self, metric_name: str) -> Tuple[AnomalyType, Dict]:
        """åˆ†ç±»K8sæŒ‡æ ‡å¼‚å¸¸ç±»å‹"""
        metric_lower = metric_name.lower()
        
        if 'cpu' in metric_lower:
            return AnomalyType.CPU_SPIKE, self.thresholds['cpu_usage']
        elif 'memory' in metric_lower:
            return AnomalyType.MEMORY_LEAK, self.thresholds['memory_usage']
        elif 'network' in metric_lower:
            return AnomalyType.NETWORK_LATENCY, self.thresholds['latency']
        else:
            return AnomalyType.UNKNOWN, self.thresholds['cpu_usage']  # é»˜è®¤
    
    def _classify_apm_anomaly_type(self, metric_name: str, percentage_change: float) -> AnomalyType:
        """åˆ†ç±»APMæŒ‡æ ‡å¼‚å¸¸ç±»å‹ - é‡æ„ï¼šæ­£ç¡®åŒºåˆ†æ ¹å› å’Œç—‡çŠ¶"""
        metric_lower = metric_name.lower()
        
        if 'error' in metric_lower:
            return AnomalyType.ERROR_BURST  # ç—‡çŠ¶ï¼šé”™è¯¯æ¿€å¢
        elif 'latency' in metric_lower or 'response_time' in metric_lower:
            return AnomalyType.SERVICE_LATENCY  # ç—‡çŠ¶ï¼šæœåŠ¡å“åº”å»¶è¿Ÿï¼ˆä¸æ˜¯ç½‘ç»œå»¶è¿Ÿï¼ï¼‰
        elif 'request' in metric_lower and percentage_change < 0:
            return AnomalyType.SERVICE_LATENCY  # è¯·æ±‚ä¸‹é™å¯èƒ½å¯¼è‡´æœåŠ¡å“åº”é—®é¢˜
        elif 'gc' in metric_lower:
            return AnomalyType.GC_PRESSURE  # æ ¹å› ï¼šGCå‹åŠ›
        else:
            return AnomalyType.UNKNOWN
    
    def _get_threshold_config(self, anomaly_type: AnomalyType, metric_name: str) -> Dict:
        """è·å–é˜ˆå€¼é…ç½® - æ›´æ–°ï¼šæ”¯æŒæ–°çš„å¼‚å¸¸ç±»å‹"""
        if anomaly_type == AnomalyType.CPU_SPIKE:
            return self.thresholds['cpu_usage']
        elif anomaly_type == AnomalyType.MEMORY_LEAK:
            return self.thresholds['memory_usage']
        elif anomaly_type == AnomalyType.NETWORK_LATENCY:
            return self.thresholds['latency']
        elif anomaly_type == AnomalyType.SERVICE_LATENCY:
            return self.thresholds['latency']  # æœåŠ¡å»¶è¿Ÿä½¿ç”¨å»¶è¿Ÿé˜ˆå€¼
        elif anomaly_type == AnomalyType.ERROR_BURST:
            return self.thresholds['error_rate']
        elif anomaly_type == AnomalyType.GC_PRESSURE:
            return self.thresholds['gc_time']
        else:
            return self.thresholds['cpu_usage']  # é»˜è®¤
    
    def _is_root_cause_type(self, anomaly_type: AnomalyType) -> bool:
        """åˆ¤æ–­å¼‚å¸¸ç±»å‹æ˜¯å¦ä¸ºæ ¹æœ¬åŸå› ç±»å‹"""
        root_cause_types = {
            AnomalyType.CPU_SPIKE,
            AnomalyType.MEMORY_LEAK,
            AnomalyType.NETWORK_LATENCY,
            AnomalyType.DISK_IO,
            AnomalyType.GC_PRESSURE
        }
        return anomaly_type in root_cause_types
    
    def _is_symptom_type(self, anomaly_type: AnomalyType) -> bool:
        """åˆ¤æ–­å¼‚å¸¸ç±»å‹æ˜¯å¦ä¸ºç—‡çŠ¶ç±»å‹"""
        symptom_types = {
            AnomalyType.SERVICE_LATENCY,
            AnomalyType.ERROR_BURST
        }
        return anomaly_type in symptom_types
    
    def _perform_causal_analysis(self, anomalies: List[Anomaly]) -> List[Anomaly]:
        """æ‰§è¡Œå› æœåˆ†æï¼šèµ„æºé—®é¢˜ä¼˜å…ˆä¸ºæ ¹å› ï¼Œç—‡çŠ¶æ¬¡ä¹‹
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        1. å¦‚æœåŒæ—¶å­˜åœ¨èµ„æºå¼‚å¸¸å’Œç—‡çŠ¶å¼‚å¸¸ï¼Œä¼˜å…ˆé€‰æ‹©èµ„æºå¼‚å¸¸ä¸ºæ ¹å› 
        2. å¦‚æœç—‡çŠ¶å¼‚å¸¸æ— å¯¹åº”èµ„æºå¼‚å¸¸ï¼Œç—‡çŠ¶æœ¬èº«ä¸ºæ ¹å› 
        3. èµ„æºå¼‚å¸¸å§‹ç»ˆä¿ç•™ä¸ºæ ¹å› 
        """
        
        if not anomalies:
            return anomalies
            
        # æŒ‰æœåŠ¡åˆ†ç»„åˆ†æ
        service_anomalies = {}
        for anomaly in anomalies:
            service = anomaly.service
            if service not in service_anomalies:
                service_anomalies[service] = []
            service_anomalies[service].append(anomaly)
        
        final_anomalies = []
        
        for service, service_anomaly_list in service_anomalies.items():
            # åˆ†ç¦»æ ¹å› å¼‚å¸¸å’Œç—‡çŠ¶å¼‚å¸¸
            root_cause_anomalies = [a for a in service_anomaly_list if self._is_root_cause_type(a.anomaly_type)]
            symptom_anomalies = [a for a in service_anomaly_list if self._is_symptom_type(a.anomaly_type)]
            other_anomalies = [a for a in service_anomaly_list if not self._is_root_cause_type(a.anomaly_type) and not self._is_symptom_type(a.anomaly_type)]
            
            # 1. èµ„æºå¼‚å¸¸å§‹ç»ˆä¿ç•™ï¼ˆæ ¹å› ä¼˜å…ˆï¼‰
            final_anomalies.extend(root_cause_anomalies)
            
            # 2. å¤„ç†ç—‡çŠ¶å¼‚å¸¸
            if root_cause_anomalies:
                # æœ‰æ ¹å› å¼‚å¸¸æ—¶ï¼Œç—‡çŠ¶å¼‚å¸¸è¢«è§£é‡Šä¸ºç”±æ ¹å› å¯¼è‡´ï¼Œä¸å•ç‹¬æŠ¥å‘Š
                if self.debug:
                    root_causes = [a.anomaly_type.value for a in root_cause_anomalies]
                    symptoms = [a.anomaly_type.value for a in symptom_anomalies]
                    self.logger.info(f"ğŸ”— æœåŠ¡ {service} å› æœåˆ†æ: æ ¹å›  {root_causes} è§£é‡Šç—‡çŠ¶ {symptoms}")
            else:
                # æ— æ ¹å› å¼‚å¸¸æ—¶ï¼Œç—‡çŠ¶æœ¬èº«ä¸ºæ ¹å› 
                final_anomalies.extend(symptom_anomalies)
                if self.debug and symptom_anomalies:
                    self.logger.info(f"ğŸ” æœåŠ¡ {service} ç—‡çŠ¶å¼‚å¸¸æ— å¯¹åº”æ ¹å› ï¼Œç—‡çŠ¶æœ¬èº«ä¸ºæ ¹å› ")
            
            # 3. å…¶ä»–æœªåˆ†ç±»å¼‚å¸¸ä¿ç•™
            final_anomalies.extend(other_anomalies)
        
        return final_anomalies
    
    def _calculate_severity(self, z_score: float, percentage_change: float, current_value: float) -> SeverityLevel:
        """è®¡ç®—å¼‚å¸¸ä¸¥é‡ç¨‹åº¦"""
        
        # ç»¼åˆè¯„åˆ†
        severity_score = 0
        
        # Z-scoreè´¡çŒ®
        if z_score > 5:
            severity_score += 3
        elif z_score > 3:
            severity_score += 2
        elif z_score > 2:
            severity_score += 1
        
        # ç™¾åˆ†æ¯”å˜åŒ–è´¡çŒ®
        if percentage_change > 500:
            severity_score += 3
        elif percentage_change > 200:
            severity_score += 2
        elif percentage_change > 100:
            severity_score += 1
        
        # ç»å¯¹å€¼è´¡çŒ®ï¼ˆé’ˆå¯¹ç‰¹å®šæŒ‡æ ‡ï¼‰
        if current_value > 0.9:  # å¦‚CPU/å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡90%
            severity_score += 2
        elif current_value > 0.8:
            severity_score += 1
        
        # æ˜ å°„åˆ°ä¸¥é‡ç¨‹åº¦çº§åˆ«
        if severity_score >= 6:
            return SeverityLevel.CRITICAL
        elif severity_score >= 4:
            return SeverityLevel.HIGH
        elif severity_score >= 2:
            return SeverityLevel.MEDIUM
        else:
            return SeverityLevel.LOW
    
    def _calculate_log_severity(self, error_rate: float) -> SeverityLevel:
        """è®¡ç®—æ—¥å¿—å¼‚å¸¸ä¸¥é‡ç¨‹åº¦"""
        if error_rate > 0.5:
            return SeverityLevel.CRITICAL
        elif error_rate > 0.3:
            return SeverityLevel.HIGH
        elif error_rate > 0.1:
            return SeverityLevel.MEDIUM
        else:
            return SeverityLevel.LOW
    
    def _analyze_log_patterns(self, logs: List[Dict[str, Any]]) -> Dict[str, Dict]:
        """åˆ†ææ—¥å¿—æ¨¡å¼"""
        patterns = {}
        
        for log in logs:
            if not isinstance(log, dict):
                continue
                
            service = log.get('service_name', 'unknown')
            
            if service not in patterns:
                patterns[service] = {
                    'total_logs': 0,
                    'error_logs': 0,
                    'error_rate': 0.0
                }
            
            patterns[service]['total_logs'] += 1
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºé”™è¯¯æ—¥å¿—
            log_text = str(log.get('raw_log_text', '')).lower()
            if any(keyword in log_text for keyword in ['error', 'exception', 'fail', 'timeout']):
                patterns[service]['error_logs'] += 1
        
        # è®¡ç®—é”™è¯¯ç‡
        for service, pattern in patterns.items():
            if pattern['total_logs'] > 0:
                pattern['error_rate'] = pattern['error_logs'] / pattern['total_logs']
        
        return patterns
    
    def _extract_service_from_k8s_metric(self, metric_name: str) -> str:
        """ä»K8sæŒ‡æ ‡åç§°ä¸­æå–æœåŠ¡å"""
        # åˆ†æmetric_nameæ ¼å¼ï¼Œæå–ä¸šåŠ¡åº”ç”¨ä¿¡æ¯
        # ä¾‹: "cpu_usage_percent[ad+cart+paymentpodsÃ—12]" -> "multi-service"
        
        if '[' in metric_name:
            service_info = metric_name.split('[')[1].split(']')[0]
            if '+' in service_info:
                return "multi-service"  # å¤šæœåŠ¡æŒ‡æ ‡
            else:
                # æå–å•ä¸€æœåŠ¡å
                for app in service_info.split('pods')[0].split('+'):
                    return app.strip()
        
        return "k8s-cluster"
    
    def _rank_anomalies(self, anomalies: List[Anomaly]) -> List[Anomaly]:
        """å¯¹å¼‚å¸¸è¿›è¡Œæ’åº"""
        
        # å®šä¹‰ä¸¥é‡ç¨‹åº¦æƒé‡
        severity_weights = {
            SeverityLevel.CRITICAL: 4,
            SeverityLevel.HIGH: 3, 
            SeverityLevel.MEDIUM: 2,
            SeverityLevel.LOW: 1
        }
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦å’Œç½®ä¿¡åº¦æ’åº - é‡æ„ï¼šæ ¹å› ä¼˜å…ˆäºç—‡çŠ¶
        def sort_key(anomaly):
            # åŸºç¡€æ’åºæƒé‡
            base_score = (severity_weights[anomaly.severity], anomaly.confidence, anomaly.z_score)
            
            # æ ¹æœ¬åŸå› ç±»å‹è·å¾—ä¼˜å…ˆæƒé‡ï¼ˆèµ„æºé—®é¢˜ä¼˜å…ˆä¸ºæ ¹å› ï¼‰
            if self._is_root_cause_type(anomaly.anomaly_type):
                root_cause_bonus = 0.1  # æ ¹å› è·å¾—æ˜¾è‘—ä¼˜å…ˆæƒ
            else:
                root_cause_bonus = 0.0
            
            return (base_score[0], base_score[1] + root_cause_bonus, base_score[2])
        
        return sorted(anomalies, key=sort_key, reverse=True)
    
    def _log_anomaly_summary(self, anomalies: List[Anomaly]):
        """è®°å½•å¼‚å¸¸æ‘˜è¦"""
        
        if not anomalies:
            return
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
        severity_counts = {}
        type_counts = {}
        service_counts = {}
        
        for anomaly in anomalies:
            severity_counts[anomaly.severity] = severity_counts.get(anomaly.severity, 0) + 1
            type_counts[anomaly.anomaly_type] = type_counts.get(anomaly.anomaly_type, 0) + 1
            service_counts[anomaly.service] = service_counts.get(anomaly.service, 0) + 1
        
        self.logger.info("ğŸ“Š å¼‚å¸¸æ£€æµ‹æ‘˜è¦:")
        self.logger.info(f"   ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ: {dict(severity_counts)}")
        self.logger.info(f"   å¼‚å¸¸ç±»å‹åˆ†å¸ƒ: {dict(type_counts)}")
        self.logger.info(f"   å—å½±å“æœåŠ¡: {dict(service_counts)}")
        
        # æ˜¾ç¤ºå‰5ä¸ªæœ€ä¸¥é‡çš„å¼‚å¸¸
        # import pdb; pdb.set_trace()
        self.logger.info("ğŸ”¥ æœ€ä¸¥é‡çš„å¼‚å¸¸:")
        for i, anomaly in enumerate(anomalies[:30], 1):
            self.logger.info(f"   {i}. {anomaly.service} - {anomaly.anomaly_type.value} ({anomaly.severity.value}) - {anomaly.evidence}")


class CorrelationAnalysisEngine:
    """å…³è”åˆ†æå¼•æ“"""
    
    def __init__(self, debug: bool = False):
        self.debug = debug
        self.logger = logging.getLogger(__name__)
    
    def analyze_service_correlations(self, anomalies: List[Anomaly], 
                                   data_bundle) -> List[ServiceCorrelation]:
        """åˆ†ææœåŠ¡é—´çš„å…³è”æ€§"""
        
        self.logger.info("ğŸ”— å¼€å§‹æœåŠ¡å…³è”åˆ†æ")
        
        correlations = []
        
        # 1. åŸºäºå¼‚å¸¸ä¼ æ’­çš„å…³è”åˆ†æ
        propagation_correlations = self._analyze_anomaly_propagation(anomalies)
        correlations.extend(propagation_correlations)
        
        # 2. åŸºäºæ—¶åºæ•°æ®çš„å…³è”åˆ†æ
        metric_correlations = self._analyze_metric_correlations(data_bundle)
        correlations.extend(metric_correlations)
        
        self.logger.info(f"âœ… å…³è”åˆ†æå®Œæˆ: å‘ç° {len(correlations)} ä¸ªæœåŠ¡å…³è”")
        
        return correlations
    
    def _analyze_anomaly_propagation(self, anomalies: List[Anomaly]) -> List[ServiceCorrelation]:
        """åŸºäºå¼‚å¸¸ä¼ æ’­åˆ†ææœåŠ¡å…³è”"""
        # ç®€åŒ–å®ç°ï¼šåŸºäºå¼‚å¸¸æ—¶é—´åºåˆ—åˆ†ææœåŠ¡é—´çš„å½±å“å…³ç³»
        correlations = []
        
        # æŒ‰æœåŠ¡åˆ†ç»„å¼‚å¸¸
        service_anomalies = {}
        for anomaly in anomalies:
            if anomaly.service not in service_anomalies:
                service_anomalies[anomaly.service] = []
            service_anomalies[anomaly.service].append(anomaly)
        
        # å¯»æ‰¾æ—¶é—´ç›¸å…³çš„å¼‚å¸¸æ¨¡å¼
        services = list(service_anomalies.keys())
        for i, service_a in enumerate(services):
            for service_b in services[i+1:]:
                # ç®€åŒ–çš„å…³è”åº¦è®¡ç®—
                correlation_score = 0.5  # å ä½ç¬¦
                
                if correlation_score > 0.6:
                    correlation = ServiceCorrelation(
                        service_a=service_a,
                        service_b=service_b,
                        correlation_coefficient=correlation_score,
                        lag_seconds=0,
                        causal_direction="unknown",
                        confidence=0.6,
                        supporting_evidence=[f"Both services show anomalies in similar timeframe"]
                    )
                    correlations.append(correlation)
        
        return correlations
    
    def _analyze_metric_correlations(self, data_bundle) -> List[ServiceCorrelation]:
        """åŸºäºæŒ‡æ ‡æ•°æ®åˆ†æå…³è”æ€§"""
        # ç®€åŒ–å®ç°ï¼Œè¿”å›ç©ºåˆ—è¡¨
        # åœ¨å®Œæ•´ç‰ˆæœ¬ä¸­ï¼Œè¿™é‡Œä¼šåˆ†ææ—¶é—´åºåˆ—çš„ç›¸å…³æ€§
        return []
