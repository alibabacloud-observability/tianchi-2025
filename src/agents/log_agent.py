"""
Real Log Agent for RCA Data Collection - FIXED VERSION
ä¿®å¤äº†æ—¥å¿—æ— æ³•ä¸‹è½½çš„é—®é¢˜ï¼Œä½¿ç”¨éªŒè¯æœ‰æ•ˆçš„apm.log.agent_infoé…ç½®
"""
import logging
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from tools.paas_data_tools import umodel_get_logs
from ..utils.evidence_chain import EvidenceChain


class MinimalLogAgent:
    """ä¿®å¤åçš„æœ€å°æ—¥å¿—ä»£ç† - ä¸“æ³¨äºæœ‰æ•ˆçš„é…ç½®ï¼Œæ”¯æŒç¦»çº¿æ¨¡å¼"""
    
    def __init__(self, debug: bool = False, offline_mode: bool = False, problem_id: str = None):
        self.debug = debug
        self.offline_mode = offline_mode
        self.problem_id = problem_id
        self.logger = logging.getLogger(__name__)
        
        if offline_mode:
            # ç¦»çº¿æ¨¡å¼ï¼šä½¿ç”¨æœ¬åœ°æ•°æ®åŠ è½½å™¨
            from ..utils.local_data_loader import get_local_data_loader
            self.local_loader = get_local_data_loader(debug=debug)
            self.logger.info(f"âœ… MinimalLogAgentåˆå§‹åŒ–å®Œæˆ (ç¦»çº¿æ¨¡å¼, é—®é¢˜ID: {problem_id})")
        else:
            
            # ä½¿ç”¨æ’æŸ¥éªŒè¯çš„æœ‰æ•ˆé…ç½® (ç§»é™¤å…¶ä»–æ— æ•ˆçš„log_sets)
            self.effective_log_config = {
                'domain': 'apm',
                'entity_set_name': 'apm.service',
                'log_set_name': 'apm.log.agent_info',  # å”¯ä¸€æœ‰æ•ˆçš„log_set
                'log_set_domain': 'apm'
            }
            
            self.logger.info(f"âœ… MinimalLogAgentåˆå§‹åŒ–å®Œæˆ (åœ¨çº¿æ¨¡å¼)")
            if self.debug:
                self.logger.info(f"ğŸ”§ ä½¿ç”¨æœ‰æ•ˆé…ç½®: {self.effective_log_config}")
    
    def _fetch_mcp_log_data(self, start_time: datetime, end_time: datetime) -> List[Dict[str, Any]]:
        """ä½¿ç”¨æ’æŸ¥éªŒè¯çš„æœ‰æ•ˆé…ç½®è·å–æ—¥å¿—æ•°æ®"""
        try:
            start_timestamp = int(start_time.timestamp())
            end_timestamp = int(end_time.timestamp())
            
            self.logger.info(f"ğŸ” è·å–æ—¥å¿—æ•°æ® - ä½¿ç”¨ä¿®å¤åçš„é…ç½®")
            self.logger.info(f"   æ—¶é—´èŒƒå›´: {start_time} ~ {end_time}")
            
            # ç›´æ¥è°ƒç”¨LangChainå·¥å…·
            result = umodel_get_logs.invoke({
                "domain": self.effective_log_config['domain'],
                "entity_set_name": self.effective_log_config['entity_set_name'],
                "log_set_name": self.effective_log_config['log_set_name'],
                "log_set_domain": self.effective_log_config['log_set_domain'],
                "from_time": start_timestamp,
                "to_time": end_timestamp
            })
            
            logs = result.data if result and not result.error else []
            
            if logs:
                self.logger.info(f"âœ… æˆåŠŸè·å– {len(logs)} æ¡æ—¥å¿—")
                
                # ç»Ÿè®¡æœåŠ¡åˆ†å¸ƒï¼ˆåŸºäºå®é™…å­—æ®µç»“æ„ï¼‰
                services = set()
                log_types = set()
                for log in logs:
                    if isinstance(log, dict):
                        if 'service_name' in log and log['service_name']:
                            services.add(log['service_name'])
                        if 'log_type' in log and log['log_type']:
                            log_types.add(log['log_type'])
                
                if services:
                    self.logger.info(f"ğŸ“Š æ¶‰åŠæœåŠ¡ ({len(services)}ä¸ª): {sorted(services)}")
                else:
                    self.logger.warning(f"âš ï¸ æ— æ³•ä»æ—¥å¿—ä¸­æå–æœåŠ¡ä¿¡æ¯")
                
                if log_types:
                    self.logger.info(f"ğŸ“Š æ—¥å¿—ç±»å‹: {sorted(log_types)}")
                
                return logs
            else:
                self.logger.warning("âš ï¸ æ—¥å¿—è·å–ç»“æœä¸ºç©º")
                return []
            
        except Exception as e:
            self.logger.error(f"âŒ è·å–æ—¥å¿—æ•°æ®å¤±è´¥: {e}")
            return []
    
    def _parse_log_response(self, result: Dict) -> List[Dict[str, Any]]:
        """è§£æMCPæ—¥å¿—å“åº” - ä¼˜åŒ–ç‰ˆæœ¬"""
        logs = []
        
        if 'content' not in result:
            self.logger.warning("âš ï¸ MCPå“åº”ä¸­æ— contentå­—æ®µ")
            return logs
        
        content = result['content']
        if not isinstance(content, list):
            self.logger.warning(f"âš ï¸ contentæ ¼å¼å¼‚å¸¸: {type(content)}")
            return logs
        
        self.logger.debug(f"ğŸ” è§£æ {len(content)} ä¸ªcontenté¡¹")
        
        for i, item in enumerate(content):
            if isinstance(item, dict) and item.get('type') == 'text':
                try:
                    text_content = item.get('text', '')
                    if not text_content.strip():
                        continue
                        
                    # è§£æJSONå†…å®¹
                    text_data = json.loads(text_content)
                    
                    if isinstance(text_data, dict) and 'data' in text_data:
                        log_data = text_data['data']
                        
                        if isinstance(log_data, list):
                            # å¤„ç†æ—¥å¿—æ•°ç»„
                            for log_entry in log_data:
                                if isinstance(log_entry, dict):
                                    enhanced_log = self._enhance_log_entry(log_entry)
                                    logs.append(enhanced_log)
                        elif isinstance(log_data, dict):
                            # å¤„ç†å•ä¸ªæ—¥å¿—å¯¹è±¡
                            enhanced_log = self._enhance_log_entry(log_data)
                            logs.append(enhanced_log)
                            
                        self.logger.debug(f"âœ… è§£æcontenté¡¹ [{i}]: è·å¾— {len(log_data) if isinstance(log_data, list) else 1} æ¡æ—¥å¿—")
                        
                except json.JSONDecodeError as e:
                    self.logger.debug(f"âš ï¸ JSONè§£æå¤±è´¥ [{i}]: {e}")
                    # ä¿ç•™æ— æ³•è§£æçš„åŸå§‹æ–‡æœ¬
                    raw_text = item.get('text', '').strip()
                    if raw_text:
                        logs.append({
                            'raw_log_text': raw_text,
                            'parse_status': 'failed',
                            'content_item_index': i
                        })
            else:
                self.logger.debug(f"âš ï¸ è·³è¿‡éæ–‡æœ¬é¡¹ [{i}]: {item.get('type', 'unknown')}")
        
        return logs
    
    def _enhance_log_entry(self, log_entry: Dict) -> Dict[str, Any]:
        """å¢å¼ºæ—¥å¿—æ¡ç›® - æ ‡å‡†åŒ–å­—æ®µå’Œæ·»åŠ å…ƒä¿¡æ¯"""
        enhanced = log_entry.copy()
        
        # ç¡®ä¿å…³é”®å­—æ®µå­˜åœ¨
        enhanced['service_name'] = enhanced.get('service_name', 'unknown')
        enhanced['log_type'] = enhanced.get('log_type', 'agent_info')
        
        # æ¸…ç†å’Œæ ‡å‡†åŒ–å­—æ®µ
        for field in ['language', 'version', 'source']:
            if field in enhanced:
                value = enhanced[field]
                if value == 'null' or value is None:
                    enhanced[field] = 'unknown'
        
        # æ·»åŠ è§£æå…ƒä¿¡æ¯
        enhanced['_parsed_timestamp'] = datetime.now().isoformat()
        enhanced['_source_config'] = 'apm.log.agent_info'
        enhanced['_parsing_version'] = 'fixed_v1.0'
        
        return enhanced
    
    def _fetch_log_data(self, start_time: datetime, end_time: datetime) -> List[Dict[str, Any]]:
        """æ ¹æ®æ¨¡å¼è·å–æ—¥å¿—æ•°æ®"""
        if self.offline_mode:
            # ç¦»çº¿æ¨¡å¼ï¼šä»æœ¬åœ°æ–‡ä»¶åŠ è½½æ•°æ®
            if not self.problem_id:
                self.logger.error("âŒ ç¦»çº¿æ¨¡å¼éœ€è¦æŒ‡å®šproblem_id")
                return []
            
            # åˆ¤æ–­æ˜¯æ•…éšœæœŸè¿˜æ˜¯åŸºçº¿æœŸæ•°æ®
            # è¿™é‡Œç®€å•åˆ¤æ–­ï¼šå¦‚æœæ—¶é—´æ®µè¾ƒçŸ­ï¼ˆ<=5åˆ†é’Ÿï¼‰è®¤ä¸ºæ˜¯æ•…éšœæœŸï¼Œå¦åˆ™æ˜¯åŸºçº¿æœŸ
            duration_minutes = (end_time - start_time).total_seconds() / 60
            data_type = "failure" if duration_minutes <= 10 else "baseline"
            
            self.logger.info(f"ğŸ“‚ ä»æœ¬åœ°åŠ è½½ {self.problem_id} {data_type} æ—¥å¿—æ•°æ®")
            return self.local_loader.load_logs(self.problem_id, data_type)
        else:
            # åœ¨çº¿æ¨¡å¼ï¼šä»MCPæœåŠ¡è·å–æ•°æ®  
            return self._fetch_mcp_log_data(start_time, end_time)
    
    def analyze(self, evidence_chain: EvidenceChain) -> Dict[str, Any]:
        """åˆ†ææ—¥å¿—æ•°æ® - æ”¯æŒç¦»çº¿æ¨¡å¼"""
        
        mode_desc = "ç¦»çº¿æ¨¡å¼" if self.offline_mode else "åœ¨çº¿æ¨¡å¼"
        self.logger.info(f"ğŸ” å¼€å§‹æ—¥å¿—åˆ†æ ({mode_desc})")
        
        # è·å–æ—¥å¿—æ•°æ® - æ ¹æ®æ¨¡å¼é€‰æ‹©æ•°æ®æº
        log_data = self._fetch_log_data(evidence_chain.start_time, evidence_chain.end_time)
        
        # æ‰§è¡Œæ—¥å¿—åˆ†æ
        analysis_result = self._analyze_logs(log_data, evidence_chain.start_time, evidence_chain.end_time)
        
        # æ·»åŠ åˆ°è¯æ®é“¾
        confidence = 0.9 if len(log_data) > 0 else 0.1
        source_desc = f"{mode_desc}_log_analysis"
        evidence_chain.add_evidence(
            'log', 
            source_desc, 
            log_data,  # ä¿å­˜åŸå§‹æ—¥å¿—æ•°æ®åˆ°è¯æ®é“¾
            confidence=confidence
        )
        
        self.logger.info(f"âœ… æ—¥å¿—åˆ†æå®Œæˆ ({mode_desc})")
        self.logger.info(f"   æ—¥å¿—æ€»æ•°: {len(log_data)}")
        self.logger.info(f"   æœåŠ¡æ•°é‡: {len(analysis_result.get('services', []))}")
        self.logger.info(f"   ç½®ä¿¡åº¦: {confidence}")
        
        return analysis_result
    
    def _analyze_logs(self, logs: List[Dict], start_time: datetime, end_time: datetime) -> Dict[str, Any]:
        """åˆ†ææ—¥å¿—æ•°æ® - æå–å…³é”®ä¿¡æ¯"""
        
        # åˆå§‹åŒ–ç»Ÿè®¡
        services = set()
        log_types = set()
        languages = set()
        versions = set()
        
        service_distribution = {}
        parse_stats = {'successful': 0, 'failed': 0}
        
        for log in logs:
            if isinstance(log, dict):
                # ç»Ÿè®¡è§£æçŠ¶æ€
                if log.get('parse_status') == 'failed':
                    parse_stats['failed'] += 1
                else:
                    parse_stats['successful'] += 1
                
                # æå–æœåŠ¡ä¿¡æ¯
                service = log.get('service_name', 'unknown')
                if service != 'unknown':
                    services.add(service)
                    service_distribution[service] = service_distribution.get(service, 0) + 1
                
                # æå–å…¶ä»–å­—æ®µ
                log_types.add(log.get('log_type', 'unknown'))
                languages.add(log.get('language', 'unknown'))
                versions.add(log.get('version', 'unknown'))
        
        # æ„å»ºåˆ†æç»“æœ
        analysis = {
            'summary': {
                'total_logs': len(logs),
                'unique_services': len(services),
                'time_range': f"{start_time.isoformat()} ~ {end_time.isoformat()}",
                'collection_method': 'fixed_apm.log.agent_info',
                'parsing_success_rate': (parse_stats['successful'] / len(logs) * 100) if logs else 0
            },
            'services': sorted(services),
            'service_distribution': service_distribution,
            'log_types': sorted(log_types),
            'languages': sorted(languages),
            'versions': sorted(versions),
            'parse_statistics': parse_stats,
            'sample_logs': logs[:5] if logs else [],  # ä¿ç•™å‰5æ¡ä½œä¸ºæ ·æœ¬
            'raw_data_available': len(logs) > 0
        }
        
        return analysis
