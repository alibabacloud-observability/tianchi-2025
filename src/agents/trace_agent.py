"""
Real Trace Agent for RCA Data Collection - Using Real MCP Data
"""
import logging
from datetime import datetime
from typing import List, Dict, Any
from tools.paas_data_tools import umodel_search_traces, umodel_get_traces
from ..utils.evidence_chain import EvidenceChain


class MinimalTraceAgent:
    """Minimal trace agent for data collection - æ”¯æŒç¦»çº¿æ¨¡å¼"""
    
    def __init__(self, debug: bool = False, offline_mode: bool = False, problem_id: str = None):
        self.debug = debug
        self.offline_mode = offline_mode
        self.problem_id = problem_id
        self.logger = logging.getLogger(__name__)
        
        if offline_mode:
            # ç¦»çº¿æ¨¡å¼ï¼šä½¿ç”¨æœ¬åœ°æ•°æ®åŠ è½½å™¨
            from ..utils.local_data_loader import get_local_data_loader
            self.local_loader = get_local_data_loader(debug=debug)
            self.logger.info(f"âœ… MinimalTraceAgentåˆå§‹åŒ–å®Œæˆ (ç¦»çº¿æ¨¡å¼, é—®é¢˜ID: {problem_id})")
        else:
            
            self.logger.info(f"âœ… MinimalTraceAgentåˆå§‹åŒ–å®Œæˆ (åœ¨çº¿æ¨¡å¼)")
        
    def _fetch_mcp_trace_data(self, start_time: datetime, end_time: datetime) -> List[Dict[str, Any]]:
        """Fetch real trace data from MCP server."""
        try:
            start_timestamp = int(start_time.timestamp())
            end_timestamp = int(end_time.timestamp())
            
            # Use real MCP trace search and get methods
            spans = []
            
            # First search for trace IDs
            search_result = umodel_search_traces.invoke({
                "domain": "apm",
                "entity_set_name": "apm.service",
                "trace_set_domain": "apm", 
                "trace_set_name": "apm.trace.common",
                "from_time": start_timestamp,
                "to_time": end_timestamp,
                "limit": 50
            })
            
            # Parse LangChain response format
            trace_ids = []
            if search_result and not search_result.error and search_result.data:
                content = search_result.data
                
                # Extract trace IDs from content - correct parsing logic
                for item in content:
                    if isinstance(item, dict):
                        trace_id = item.get('traceId')
                        if trace_id:
                            trace_ids.append(trace_id)
                
                # Get detailed spans for each trace ID
                for trace_id in trace_ids:  # Limit to 20 traces
                    trace_result = umodel_get_traces.invoke({
                        "domain": "apm",
                        "entity_set_name": "apm.service",
                        "trace_ids": [trace_id],  # Now properly a list
                        "trace_set_domain": "apm",
                        "trace_set_name": "apm.trace.common", 
                        "from_time": start_timestamp,  # æ·»åŠ æ—¶é—´å‚æ•° - å…³é”®ä¿®å¤ï¼
                        "to_time": end_timestamp       # æ·»åŠ æ—¶é—´å‚æ•° - å…³é”®ä¿®å¤ï¼
                    })
                    
                    # Parse trace spans from response
                    if trace_result and not trace_result.error and trace_result.data:
                        content = trace_result.data
                        
                        # Extract spans from content
                        for span_item in content:
                            spans.extend([span_item])

            formatted_spans = []
            for span in spans:
                try:
                    # Parse span data with correct field mapping
                    formatted_span = {
                        'trace_id': span.get('traceId', ''),        # ä¿®å¤ï¼šæ­£ç¡®å­—æ®µå
                        'span_id': span.get('spanId', ''),          # ä¿®å¤ï¼šæ­£ç¡®å­—æ®µå
                        'service_name': span.get('serviceName', ''), # ä¿®å¤ï¼šæ­£ç¡®å­—æ®µå
                        'operation_name': span.get('spanName', ''),  # ä¿®å¤ï¼šæ­£ç¡®å­—æ®µå
                        'start_time': span.get('startTime', 0),      # ä¿®å¤ï¼šæ­£ç¡®å­—æ®µå
                        'duration_ms': float(span.get('duration_ms', 0)),  # ä¿®å¤ï¼šå·²ç»æ˜¯ms
                        'status_code': span.get('statusCode', 0),    # ä¿®å¤ï¼šæ­£ç¡®å­—æ®µå
                        'tags': span.get('attributes', {}),         # ä¿®å¤ï¼šä½¿ç”¨attributesä½œä¸ºtags
                        'raw_span': span
                    }
                    formatted_spans.append(formatted_span)
                    
                except Exception as e:
                    self.logger.debug(f"Failed to parse span: {e}")
                    continue
            
            self.logger.info(f"âœ… Fetched {len(formatted_spans)} trace spans")
            return formatted_spans
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to fetch trace data: {e}")
            return []
    
    def analyze_high_latency_spans(self, traces: List[Dict[str, Any]], 
                                  percentile_threshold: float = 95.0) -> List[Dict[str, Any]]:
        """Analyze high latency spans."""
        if not traces:
            return []
        
        # Calculate latency percentile
        durations = [trace['duration_ms'] for trace in traces if trace['duration_ms'] > 0]
        if not durations:
            return []
        
        durations.sort()
        p_index = int(len(durations) * percentile_threshold / 100)
        threshold = durations[min(p_index, len(durations) - 1)]
        
        # Find high latency spans
        high_latency_spans = [
            trace for trace in traces 
            if trace['duration_ms'] >= threshold
        ]
        
        return high_latency_spans[:10]  # Return top 10
    
    def _fetch_trace_data(self, start_time: datetime, end_time: datetime) -> List[Dict[str, Any]]:
        """æ ¹æ®æ¨¡å¼è·å–é“¾è·¯æ•°æ®"""
        if self.offline_mode:
            # ç¦»çº¿æ¨¡å¼ï¼šä»æœ¬åœ°æ–‡ä»¶åŠ è½½æ•°æ®
            if not self.problem_id:
                self.logger.error("âŒ ç¦»çº¿æ¨¡å¼éœ€è¦æŒ‡å®šproblem_id")
                return []
            
            # åˆ¤æ–­æ˜¯æ•…éšœæœŸè¿˜æ˜¯åŸºçº¿æœŸæ•°æ®
            duration_minutes = (end_time - start_time).total_seconds() / 60
            data_type = "failure" if duration_minutes <= 10 else "baseline"
            
            self.logger.info(f"ğŸ”— ä»æœ¬åœ°åŠ è½½ {self.problem_id} {data_type} é“¾è·¯æ•°æ®")
            return self.local_loader.load_traces(self.problem_id, data_type)
        else:
            # åœ¨çº¿æ¨¡å¼ï¼šä»MCPæœåŠ¡è·å–æ•°æ®
            return self._fetch_mcp_trace_data(start_time, end_time)
    
    def analyze(self, evidence_chain: EvidenceChain) -> Dict[str, Any]:
        """åˆ†æé“¾è·¯æ•°æ® - æ”¯æŒç¦»çº¿æ¨¡å¼"""
        
        mode_desc = "ç¦»çº¿æ¨¡å¼" if self.offline_mode else "åœ¨çº¿æ¨¡å¼"
        self.logger.info(f"ğŸ”— å¼€å§‹é“¾è·¯åˆ†æ ({mode_desc})")
        
        # è·å–é“¾è·¯æ•°æ® - æ ¹æ®æ¨¡å¼é€‰æ‹©æ•°æ®æº
        trace_data = self._fetch_trace_data(evidence_chain.start_time, evidence_chain.end_time)
        
        # åˆ†æé«˜å»¶è¿Ÿspans
        high_latency = self.analyze_high_latency_spans(trace_data)
        
        # æ·»åŠ è¯æ®
        source_desc = f"{mode_desc}_trace_analysis"
        evidence_chain.add_evidence('trace', source_desc, trace_data, confidence=0.6)
        
        self.logger.info(f"âœ… é“¾è·¯åˆ†æå®Œæˆ ({mode_desc})")
        self.logger.info(f"   Spanæ•°é‡: {len(trace_data)}")
        self.logger.info(f"   æœåŠ¡æ•°é‡: {len(set(trace.get('service_name', 'unknown') for trace in trace_data))}")
        self.logger.info(f"   é«˜å»¶è¿Ÿæ•°é‡: {len(high_latency)}")
        
        return {
            'span_count': len(trace_data),
            'services': list(set(trace.get('service_name', 'unknown') for trace in trace_data)),
            'high_latency_count': len(high_latency),
            'avg_duration_ms': sum(trace['duration_ms'] for trace in trace_data) / len(trace_data) if trace_data else 0
        }
